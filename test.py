from utils.validation_utils import get_document_structure, find_prompt_usage_in_code
from utils.prompt_utils import validate_prompt_parameters, get_prompt_expected_parameters, AGENT_TOOLS, AGENT_GROUPS, AGENT_GROUPS_TEXT, update_prompt
from utils.completion_examples_utils import INSIGHT_STORE_EXAMPLE
from utils.get_weekly_stored import get_weekly_storage_stats, format_storage_stats_table
# print(get_document_structure("to_be_implemented_questions"))
# print(validate_prompt_parameters("insights_task_context", """Here are the other OKRs stored, choose one as a base for the insight (use its okr_name when calling store_insight):
# {{okrs}}
# Here are the other insights stored, DO NOT REPEAT THEM. WE WANT UNIQUE INSIGHTS
# {{insights}}
# Only choose 1 OKR to use as a base. Prioritize choosing an OKR where the insight count is low and has a greater impact on the business.

# Make sure at least [x]%% sessions are being tracked on the site to consider the insight.


# Example 1: Test for total sessions on the form. If > 100, focus on form.
# Example 2: Then if %% mobile visits is > 70, focus on mobile.
# Example 3: Choose OKR Enhance_User_Engagement_and_Conversion_Efficiency because there are 0 insights.
# Example 4: Choose OKR Click_Interaction_Analysis because it can directly impact user behavior on the website.
# Example 5: Choose OKR Enhance User Engagement and Streamline Navigation if there are navigational elements on a site that can be easily fixed with design updates.


# IMPORTANT: The task is not complete unless the Insight is explicitly stored by the insights_analyst agent. Be careful of circular reasoning. The data statement must be proven using data and not derived from previous assumptions. The severity should be as high as possible given the data available. Always use the most recent OKR.

# The data must be high quality in order to find an insight that can be stored. If the data available does not have a reasonable quality in terms of total sessions on the site, lack of heatmap data, or the values are all 0s or close to 0, please ask the research analyst to find another OKR.
# """))

#print(get_prompt_expected_parameters('okr_store_agent_system_message'))
# print(AGENT_GROUPS_TEXT)

print(update_prompt("okr_task_context", """```json
{
  "context": "This is the context for the OKR task. The goal is to create a new OKR using environment functions. All data must be traceable, auditable, and non-hallucinated. No estimations should be made.\n\nHere are the other OKRs stored for this website. Use them as context for creating this OKR, but do not create a duplicate. Reference the names of other OKRs if they are relevant:\nThe `okrs` variable is a JSON array of the following form (but may have more or fewer entries). Each OKR has a `name`, `description`, `code`, `reach_code`, `queries`, and `trajectory` field.\n```json\n{okrs}\n```\n\n**Workflow and Agent Responsibilities:**\n\nThis prompt is used in a group chat with the following agents: `okr_python_analyst`, `insights_behavioral_analyst`, and `okr_store_agent`. The workflow MUST proceed as follows, and the task is not complete until the OKR is explicitly stored by the `okr_store_agent`:\n\n1.  **Data Validation (Mandatory First Step by `okr_python_analyst`):**\n    -   The `okr_python_analyst` must first define `start_date`, `end_date`, and `stream_key` in every code block. Then, use the `run_sitewiz_query` tool to query for non-zero counts of sessions, funnel events, and heatmap events over the past 7 days. Print the results of each query.\n    -   If *any* of these counts are zero or the query fails (error in description), output \"NO DATA\" and terminate. Do not proceed to further analysis if this initial validation fails. Instead, consider a different approach or data source after communicating with the `okr_research_agent`.\\n\n\n2.  **Data Exploration (by `okr_python_analyst`):**\n    -   If data validation is successful, the `okr_python_analyst` will explore the available data. The focus is on *discovering* potential metrics, NOT immediately defining an OKR. The `okr_python_analyst` MUST output raw query results before writing `calculate_reach` or `calculate_metrics` functions. Start with simple queries, such as querying limited results (limit 10) from `funnels.base_url` or `heatmaps.url`. Prioritize metrics which:\n        *   Show significant change over time (i.e., variability).\n        *   Impact a large number of sessions (i.e., high reach).\n        *   Align with the business objectives (stated in the provided context).\n\n3.  **Qualitative Analysis (by `insights_behavioral_analyst` - Optional, but highly encouraged):**\n    -   Once the `okr_python_analyst` finds a promising direction (at least a URL to search and some idea of a data trend) from the data exploration, the `insights_behavioral_analyst` should analyze available heatmap data and session recordings (if any) to find potential reasons *why* the data may look like it does to provide extra context. It *must* wait for a URL from the `okr_python_analyst` before starting its analysis.\n\n4.  **OKR Definition and Code Generation (by `okr_python_analyst`):**\n    -   Only AFTER the data exploration, validation, and qualitative analysis (if applicable), the `okr_python_analyst` should proceed to define the `calculate_metrics` and `calculate_reach` functions. These functions MUST return non-zero, non-uniform values for at least 3 days when executed. Output the result of these functions to find if data exists.\n    *   Provide *all* necessary data for storage:\n        *   Python code for calculating the metric (`okr_code`). The function MUST be named `calculate_metrics`. The code must be thoroughly validated and print non-zero, non-uniform values for at least 3 days.\n        *   Python code for calculating the metric's reach (`okr_reach_code`). The function MUST be named `calculate_reach`. The code must be thoroughly validated and print non-zero values for at least 3 days.\n        *   The executed SQL queries as strings.\n        *   A suggested `okr_name`.\n        *   A suggested `okr_description`.\n        *   A suggested `trajectory` of the workflow used (detailing the steps taken to discover the OKR).\n\n5.  **OKR Creation and Storage:**\n    *   Only when *all* data, functions, and analysis are prepared by the `okr_python_analyst` and validated, and confirmed through a specific message by the okr_research_agent to proceed, call the `okr_store_group` using the following message without any markdown formatting or backticks:  \"okr_store_group, please format and store the following OKR data. The code has been tested and returns valid output.\" - this message must be exact.\n\nThe `okr_python_analyst` should first verify the output of the code block is correct before sending to the `okr_store_group`. It is not complete unless the OKR is explicitly stored by the `okr_store_agent`. \n\nOptimal planning ensures that the agents that fetch the necessary data from the environment go first with a plan (e.g., `okr_python_analyst`, `insights_behavioral_analyst`, etc.). No agents should attempt to store data or hallucinate data from the environment. Only when all the information from the environment is available and interpreted correctly should the output be stored by the specific `okr_store_group`. The `okr_store_agent` *only* stores when all information is complete, and is triggered by a specific message from `okr_python_analyst`: \"okr_store_group, please format and store the following OKR data. The code has been tested and returns valid output.\"\nAll data for the store function must come from the python analyst, so the python analyst must go first. If the python analyst fails to find data to store, it should output a message with what is not working so it is clear to move to a different task.\n\nMake sure that data validation using `run_sitewiz_query` is the first step in all workflows. If no data is found, the task should terminate immediately. The `okr_python_analyst` must find and explore data *before* the other agents perform qualitative analysis or store anything.\n\n"
}"""))

# stats = get_weekly_storage_stats()
# print(format_storage_stats_table(stats))
