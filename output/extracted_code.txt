

# File: backend/agents/data_analyst_group/agents/SocietyOfMindAgent.py

from typing import Any, AsyncGenerator, List, Mapping, Sequence, Tuple, Callable, Optional

from autogen_core import CancellationToken

from autogen_core.models import ChatCompletionClient, LLMMessage, SystemMessage, UserMessage

from autogen_agentchat.base import Response

from autogen_agentchat.state import SocietyOfMindAgentState

from autogen_agentchat.base import TaskResult, Team

from autogen_agentchat.messages import AgentEvent, BaseChatMessage, BaseAgentEvent, ChatMessage, TextMessage

from autogen_agentchat.agents import BaseChatAgent

from utils.functions import convert_message_to_dict, fetch_results, process_messages_success

import json

from decimal import Decimal

def default(self, obj):
    if isinstance(obj, Decimal):
        return str(obj)
    return super(DecimalEncoder, self).default(obj)

def __init__(self, name: str, team: Team, model_client: ChatCompletionClient, *, description: str='An agent that uses an inner team of agents to generate responses.', instruction: str=DEFAULT_INSTRUCTION, response_prompt: str=DEFAULT_RESPONSE_PROMPT, verify_value: Optional[str]=None, verify_function: Optional[str]=None, start_message: str=DEFAULT_START_MESSAGE) -> None:
    super().__init__(name=name, description=description)
    self._team = team
    self._model_client = model_client
    self._instruction = instruction
    self._response_prompt = response_prompt
    self._verify_value = verify_value
    self._verify_function = verify_function
    self._start_message = start_message

@property
def produced_message_types(self) -> Sequence[type[ChatMessage]]:
    return (TextMessage,)



# File: backend/agents/data_analyst_group/agents/ZepConversationalAgent.py

from autogen_agentchat.agents import BaseChatAgent

from typing import Dict, Union

from zep_cloud.client import AsyncZep, Zep

from dotenv import load_dotenv

from utils.functions import get_api_key

import os

import json

def get_relevant_memory(stream_key, query):

    def truncate(text: str, max_length: int=252) -> str:
        """
        Truncate text to a maximum length.
        """
        if len(text) > max_length:
            return text[:max_length] + '...'
        return text
    memory = zep.graph.search(group_id=stream_key, query=truncate(query), reranker='mmr', mmr_lambda=0.5)
    memory_str = memory.json()
    memory_json = json.loads(memory_str)
    filter_keys = ['fact', 'name', 'labels', 'summary']
    edges = memory_json.get('edges', [])
    memory_edges = [{key: value for key, value in item.items() if key in filter_keys} for item in edges]
    nodes = memory_json.get('nodes', [])
    memory_nodes = [{key: value for key, value in item.items() if key in filter_keys} for item in nodes]
    memory_text = json.dumps(memory_edges + memory_nodes, indent=2)
    return memory_text

def truncate(text: str, max_length: int=252) -> str:
    """
        Truncate text to a maximum length.
        """
    if len(text) > max_length:
        return text[:max_length] + '...'
    return text

def __init__(self, name: str, system_message: str, llm_config: dict, stream_key: str, is_termination_msg: callable, description: str, question: str):
    super().__init__(name=name, llm_config=llm_config, description=description, system_message=system_message)
    self.stream_key = stream_key
    self.question = question
    self.original_system_message = system_message
    self.register_hook('process_all_messages_before_reply', self.persist_user_messages)

def persist_user_messages(self, messages):
    """
        User sends a message to the agent. Add the message to Zep and
        update the system message with relevant facts from Zep.
        """
    if len(messages) > 0:
        zep.graph.add(group_id=self.stream_key, data=json.dumps(messages), type='json')
    prev_message = messages[-1]
    query = f'Find facts to help answer the question: {self.question}\n\n    Here is the previous message for context:\n    {prev_message}'
    memory_text = get_relevant_memory(self.stream_key, query)
    self.update_system_message(self.original_system_message + f'\n\nRelevant facts about prior conversations:\n{memory_text}')
    return messages



# File: backend/agents/data_analyst_group/agents/behavioral_analyst.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from utils.functions import is_termination_msg

def create_behavioral_analyst(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = "\nYou are an expert Behavioral Analyst who analyzes images and videos to identify patterns and anomalies.\n\n**Responsibilities:**\n1. Run 'get_top_pages' to understand which urls have the most traffic. You must run this first to understand the pages on the site so you can run 'get_heatmap' on an existing page.\n2. Run 'get_heatmap' or 'get_session_recording' tools (if available).\n - Do not process any interaction data, only analyze given images or videos.\n - Only run get_heatmap for urls in top pages because then you know they exist and have traffic\n3. Analyze heatmap image / session videos to identify interesting elements or interactions. When doing your analysis, you must include the exact heatmap and session recording IDs so the other analysts know what IDs are used.\n\nNote if there are no clicks for the page, it's likely that page doesn't exist or gets low traffic, so focus on urls with traffic instead.\n\nNote: You can only get element xpaths by running 'get_heatmap'.\n"
    agent = AssistantAgent(name='behavioral_analyst', model_client=get_llm_config(0.5), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='Behavioral Analyst who gets heatmap and analyzes them to find patterns and insights.')
    return agent



# File: backend/agents/data_analyst_group/agents/code_critic.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import List, Any

from utils.functions import is_termination_msg

from utils.extra_functions import readable_functions

from autogen_agentchat.agents import AssistantAgent

def create_code_critic(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    """
    Create a Code Critic with a dynamic system message based on provided functions.
    """
    function_details = readable_functions(functions)
    system_message = f'\nYou are an expert Code Critic who verifies that the code accurately answers the question. Here are schemas and functions potentially used:\n\n{function_details}\n\n\n**Workflow**:\n- Think step by step what question is being asked, what is run by the code, and whether the output correctly answers the question.\n- Were the results of the code actually printed for verification?\n- Be careful of pitfalls in calculations like these (but not limited to them):\n    - Potential divide by 0?\n    - Incorrectly calculating percentages?\n    - Calculating metrics like CTR incorrectly because the element to be clicked / page is not specified?\n    - Is the correct information from the databases used to calculate the metrics?\n    - Is the code using unique sessions correctly, is it not using unique sessions when it is supposed to?\n    - Are approximations made that are not accurate?\n- Provide feedback on the code quality and the results.\n\nYou must specify who should respond next. Here are the 2 options: \n(a) Team: The code is correct and correctly answers the question. Ask the team to proceed with the next step while providing all the results so far without omitting any. You must end this message with TERMINATE.\n(b) Python Analyst: If the code is inaccurate or does not answer the question, ask the Python Analyst to correct the code.\n\nHere is a sample output message for each option. Your message must be similar to 1 these examples:\n(a) - In this case the code and results must be provided in the response. Do not go with this case if there is no code or results.\n```Here are the results for the question:\n\n[Full Results]\n\nThe following code was run to retrieve the data:\n\n```python\n...\n```\n\nI have verified the code and results. Team, please proceed with the next step. TERMINATE```\n\n(b)\n```I reviewed the code and results and found that the code does not accurately answer the question because [reason]. \nPython Analyst, please correct the code.```\n'
    return AssistantAgent(name='code_critic', model_client=get_llm_config(0.5), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='Code Critic who verifies that the code accurately answers the question.')



# File: backend/agents/data_analyst_group/agents/code_execution_agent.py

from autogen_agentchat.agents import CodeExecutorAgent

from config.config import Config

from utils.functions import is_termination_msg

import re

from dataclasses import dataclass

from typing import List

from autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler

from autogen_core.code_executor import CodeBlock, CodeExecutor

from autogen_core.models import AssistantMessage, ChatCompletionClient, LLMMessage, SystemMessage, UserMessage

def extract_markdown_code_blocks(markdown_text: str) -> List[CodeBlock]:
    pattern = re.compile('```(?:\\s*([\\w\\+\\-]+))?\\n([\\s\\S]*?)```')
    matches = pattern.findall(markdown_text)
    code_blocks: List[CodeBlock] = []
    for match in matches:
        language = match[0].strip() if match[0] else ''
        code_content = match[1]
        code_blocks.append(CodeBlock(code=code_content, language=language))
    return code_blocks

def create_code_execution_agent(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions='', executor=None):
    system_message = '\nYou are a Code Execution Agent whose only job is to execute Python code. Once code is executed, results should be analyzed by the Python Analyst.\n\nIf no code is provided by the python analyst, ask the python analyst to provide the code. If the code is not executable, ask the python analyst to correct it.\n'
    return CodeExecutorAgent(name='code_execution_agent', code_executor=executor, system_message=system_message, description='Code Execution Agent responsible for executing Python code provided by the Python Analyst. Results must be given back to the Python Analyst for analysis.')

def __init__(self, model_client: ChatCompletionClient) -> None:
    super().__init__('An assistant agent.')
    self._model_client = model_client
    self._chat_history: List[LLMMessage] = [SystemMessage(content='Write Python script in markdown block, and it will be executed.\nAlways save figures to file in the current directory. Do not use plt.show(). All code required to complete this task must be contained within a single response.')]

def __init__(self, code_executor: CodeExecutor) -> None:
    super().__init__('An executor agent.')
    self._code_executor = code_executor



# File: backend/agents/data_analyst_group/agents/guardrails_agent.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import Dict, Any

from utils.functions import is_termination_msg

def create_guardrails_agent(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f'\nYou are an expert Guardrails Agent focused on evaluating suggestions against business context and constraints. \nYour workflow is to check the following with an explanation and adjust the suggestion accordingly:\n\n1. **Identify Business Relevance**\n   - 1A) Browse the website to ensure suggestion is not already implemented\n   - 1B) Identify current interactions on the site that relate to the suggestion\n   - 1C) Identify all the locations (urls and section) where the suggestion should be implemented\n   - Tools: Web Agent Browsing\n\n2. **Evaluate Business Alignment**\n   - 2A) Assess if the suggestion aligns with stated business goals and KPIs\n   - 2B) Ensure language of suggestion is aligns with current website messaging\n   - 2C) Ensure alignment with brand values and voice\n   - 2D) Consider impact on core metrics and business objectives\n   - Tools: Use given business context and website scrape\n\n3. **Clear Implementation Guidelines**\n   - 3A) Specify exactly how the website should look after implementing the suggestion\n   - 3B) Ensure implementation aligns with UI/UX guidelines and Psychological principles\n   - 3C) Consider accessibility and usability implications\n   - 3D) Check consistency with existing design patterns\n   - 3E) Evaluate potential user friction points\n   - Tools: UX Research tool, Web Scrape, and Web Agent screenshots\n\n4. **Risk Analysis**\n   - 4A) Identify potential negative impacts\n   - 4B) Consider security / compliance requirements\n\n5. **Suggestion Formatting**\n   - 5A) Suggestion is cleanly read, there are no IDs in the text\n\nFor the current business context:\n{business_context}\n\nRegarding the question:\n{question}\n\nStep by step, adjust the suggestion and check each bullet point with your reasoning. \nAdditionally, create design requirement document specifying the exact design of the suggestion and all the locations it should be implemented.\nIf this cannot be done, TERMINATE, and return control to the suggestions analyst to create a new suggestion.\n\nRemember:\n- Be objective and data-driven in your analysis\n- Consider both short-term and long-term implications\n- Be specific about concerns and requirements\n'
    return AssistantAgent(name='guardrails_agent', model_client=get_llm_config(1, 'main'), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='Business Analyst who evaluates suggestions against business context and constraints')



# File: backend/agents/data_analyst_group/agents/insights_analyst.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import Dict, Any

import json

from utils.functions import is_termination_msg

from prompts.insights_prompts import insight_criteria, insight_example, insight_notes

def create_insights_analyst(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f"""\nYou are an expert Insights Analyst who only does the following:\n1. Looks at code and data analysis results from the Python Analyst.\n2. If code for derivations is available, find actionable insights from the python code provided that are specific and can be tested through A/B experiments.\n3. If code for derivations is not available, return a message asking the python analyst to provide code.\n3. Identify the code executed that led to the aggregate metrics insights including the imports and functions run (eg 'run_sitewiz_query'). If the code is missing, request it from the Python Analyst.\n4. Ask the insights user proxy to store the insights in the database using the 'store_insight' tool call with the correct format and substitutions. Only the Insights User Proxy has access to this tool, you must give them the right input\n\nYour role is not to create any code, instead, you repurpose code from the python analyst.\n\nIMPORTANT:\n- In the data statement, you must use values that evaluate to percentages using the calc function (eg {{calc({{a}}/{{b}} * 100)}}) in your insights to replace all numbers. These values must come from the data provided by the analysts.\n- The insight cannot be on tracking configuration, assume the tracking configuration is correct.\n\n**Example Insights (Data Statements):**\n1. "Users who click on the {{element}} on page {{page_name}} are {{calc({{a}}/{{b}} * 100 - 100)}}% more likely to make a purchase." a,b should be in derivation. element, page_name should be in variables.\n2. "The most common path for users who make a purchase is {{page1}} -> {{page2}} -> {{page3}} with a conversion rate of {{calc({{x1}}/{{x2}} * 100)}}%, and {{calc({{y1}}/{{y2}} * 100)}}% who visit page {{page1}} click on the {{element}} to page {{page2}}." x1, x2, y1, y2 should be in derivation. page1, page2, page3, element should be in variables.\n3. "Users from demographic {{segment}} who click on the {{element}} on page {{page_name}} are {{calc(x1/x2 * 100 - 100)}}% more likely to convert into a {{target}} than other users from demographic {{segment}}." x1, x2 should be in derivation. segment, element, page_name, target should be in variables.\n\nInsights should meet all of the following criteria:\n\nInsights must be in this format:\n{insight_criteria}\n\nNotes on finding an insight:\n{insight_notes}\n\n#### Insight n: xxx\n- **Data Statement**: <Statement that must include a calculation of percent insight>. This is interesting because <explanation>.\n- **Problem Statement**: <Problem or opportunity> because... <assumption about the value it can create>.\n- **A/B Test**: We would test this by changing [test description] on the url [url] to see if we can increase our [target] by [x]%.\n- **Business Objective**: <OKR that insight relates to>\n- **Prioritization**: \n    - reach, frequency, severity scores\n- **Derivation**: Output the full Python code directly from the python analyst including the SQL query used to make the insight. If you cannot find the code, ask the python analyst to provide code. YOU CANNOT STORE INSIGHTS WITHOUT WORKING CODE FOR EACH DERIVATION.\n\nIMPORTANT:\n- Do not fabricate data or insights. If data is insufficient, request additional metrics.\n- Ensure insights are specific, actionable, and tied to business objectives.\n- Follow the prescribed output format for insights.\n- You cannot use numbers in the data statement, only variable names that are defined in the derivation.\n- The derivation must be a string of valid python code that prints the number, otherwise the insight will be rejected. It must contain all the imports necessary or the code will fail.\n- Calculations must be made off of aggregate data, not a small selected sample because we want to make sure the percentage calculations are accurate for the whole audince.\n\nVERY IMPORTANT:\n- Think step by step to make sure the code in every derivation is a string containing python code that is executable and has all the imports and variables defined. If there is even one mistake, the insight is not stored.\n- Think step by step to ensure the code in every derivation came directly from the python analyst. You cannot write code yourself. If there isn't code for the derivation, ask the python analyst to get it.\n\nFor each valid insight where we know how each number was derived, ask the insights_user_proxy to store it in the database using the 'store_insight' tool call using this format. \nIf we don't know how the numbers are derived or the way they are derived is not clear, do not ask to store the insight and provide feedback on how to improve it. They should not be estimates or guesses, they should be precise calculations based on the data provided.\n\neg, this is what should be passed on to the insights_user_proxy:\n\n{insight_example}\n\nIMPORTANT\n1. The variable, derivation, and reference substitutions should only be for the data_statement field (and all calculations in the data statement must use variables from the derivation field). The other fields should be filled in with the actual values as needed. Reference substitutions can be in variables, derivation, or data_statement.\n2. However, all text should be human readable, eg for the data statement, you must use put session ids, xpaths, etc. in variables where the human would read the 'readable' field with a tooltip of the actual value.\n3. The derivation field must contain SQL code that was used to derive the variable, and you must verify that the python analyst outputted this code and got the result accurately. You cannot make up this code.\n4. If you do create an input for insights_user_proxy (for store_insight), you must end your message with 'Pass this to the insights code analyst to verify the code, then the insights user proxy to store the insight.'"""
    return AssistantAgent(name='insights_analyst', model_client=get_llm_config(0.5, 'reasoning'), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='Insights Analyst who creates insights but does not store them. The insights user proxy will store them.')



# File: backend/agents/data_analyst_group/agents/insights_analyst_code.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import Dict, Any

import json

from utils.functions import is_termination_msg

from prompts.insights_prompts import insight_criteria, insight_example, insight_notes

def create_insights_analyst_code(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f"Your role is to make sure every code block in the derivation field in the derivation array for each insight provided is fully complete and executable with no errors.\n\nThat means for every store_insight that the insights analyst gives, you must:\n1. In every derivation field in the derivation array, find the exact code given by the python analyst to get that result\n2. Replace the derivation field in the derivation array with the full code given by the python analyst. DO NOT OMIT ANY VARIABLE OR IMPORTS NECESSARY.\n3. Do not hardcode any numbers in the derivation field. All numbers should be computed from scratch from the database queries in the python code \n4. Do not assume any variables were already computed, the code is needed from scratch to compute all the variables and needs to be in the string. Each string is independent of the other  \n5. Ensure quotes in code are promerly formatted, especially in sql queries, escaping quotes as needed\n\nTo help, this is a negative example because not all the variables necessary are defined and values are hardcoded:\n\nimport pandas as pd\nfrom functions import run_sitewiz_query\n\n# Derived from grouped_device['total_revenue'] for device_form=0\n# grouped_device.loc[grouped_device['device_form']==0, 'total_revenue'] = 3033.0\ndesktop_rev = 3033.0\n\nIf there are no store_insight calls from the insight_analyst, just output 'No store insight calls found. TERMINATE'. \nIf there is no code provided by the python analyst to support the insight, output 'Code needed by python analyst. TERMINATE'.\n\nOtherwise, return what the store_insight call should be.\n\nDo not create any new code, all code should come from the python analyst.\n\n\nHere is an example insight for reference: {insight_example}\n\n\nRemember these notes: {insight_notes}"
    return AssistantAgent(name='insights_analyst_code', model_client=get_llm_config(1, 'deepseek'), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='Insight Coding Analyst who adds code to insights analyst. The insights user proxy will store them.')



# File: backend/agents/data_analyst_group/agents/insights_user_proxy.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from utils.functions import is_termination_msg

from prompts.insights_prompts import insight_criteria, insight_example

def create_insights_user_proxy(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f"You are an insights user proxy whose role is to call 'store_insight' using the context of the previous messages.\n\neg:\n\n{insight_example}\n\nAll the types must match and the keys must be the same. Estimate the value if they are not provided. Don't leave any field empty, estimate them if needed.\n\nThe variable, derivation, and reference substitutions should only be for the data_statement field. The other fields should be filled in with the actual values as needed. Reference substitutions can be in variables, derivation, or data_statement.\nHowever, all text should be human readable, eg for the data statement, you must use put session ids, xpaths, etc. in variables where the human would read the 'readable' field with a tooltip of the actual value.\nAll calculations should use variables in the derivation field, they should not explicitly use numbers so we can verify that each number is derived from the data provided.\n\nNote that `store_insight` will return a result explaining if it is stored and why. You must call store_insight with the given context. \nOutput the exact result of `store_insight`\n- If it not stored due to failing criteria, End this message with 'TERMINATE'.\n- If it is not stored due to an execution error, just continue.\n- If it is stored, End this message with 'TERMINATE'.\n"
    agent = AssistantAgent(name='insights_user_proxy', model_client=get_llm_config(1), description='Insights User Proxy who reviews and approves insights.', system_message=system_message, tools=tools, reflect_on_tool_use=True, handoffs=handoffs)
    return agent



# File: backend/agents/data_analyst_group/agents/okr_creator_agent.py

from autogen_agentchat.agents import AssistantAgent

from prompts.okr_prompts import all_okr_prompts

def create_okr_creator_agent(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f'You are an OKR Creator Agent responsible for creating and formulating Objectives and Key Results (OKRs). Your primary responsibilities are:\n\n1. Analyze Business Context:\n   - Understand the business goals and requirements\n   - Identify key metrics and success indicators\n   - Formulate appropriate OKRs based on analysis\n\n2. Create OKR Code:\n   - Design OKR tracking code that follows best practices\n   - Include clear descriptions and implementation details\n   - Ensure code meets all validation requirements\n   - Work with OKR Store Agent to save the created OKRs\n\n{all_okr_prompts(stream_key)}\n\n\nKey Requirements:\n- Create well-structured and measurable OKRs\n- Ensure OKRs align with business objectives\n- Include clear implementation details\n- Follow proper code formatting and documentation standards\n\n{additional_instructions}'
    return AssistantAgent(name='okr_creator_agent', model_client=get_llm_config(0.5, 'main'), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='OKR Creator Agent who analyzes business context and creates appropriate OKRs with implementation details.')



# File: backend/agents/data_analyst_group/agents/okr_research_agent.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import Dict, Any

import json

from utils.functions import is_termination_msg

from prompts.okr_prompts import all_okr_prompts

def create_okr_research_agent(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f'You are an OKR Research Agent who specializes in identifying and analyzing Objectives and Key Results (OKRs) for website optimization. Your role is to:\n\n1. Research and identify potential OKRs by analyzing:\n   - User behavior patterns (clicks, scrolls, hovers)\n   - Page navigation flows\n   - User engagement metrics\n   - Understand what the pages of the website look like\n\nUse this business context to identify OKRs most relevant to the business (but also include any OKRs that would help)\n{business_context}\n\n2. Prioritize OKRs that are:\n   - Measurable through website analytics\n   - Actionable through website changes\n   - Aligned with business objectives\n   - Realistic and achievable\n   - Time-bound and trackable\n\n{all_okr_prompts(stream_key)}\n\nYour goal is to help identify the most impactful OKRs that can be tracked and improved through website optimization.\n\n{additional_instructions}'
    return AssistantAgent(name='okr_research_agent', model_client=get_llm_config(0.5, 'main'), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='OKR Research Agent who identifies and analyzes potential Objectives and Key Results for website optimization, focusing on trackable and actionable metrics.')



# File: backend/agents/data_analyst_group/agents/okr_store_agent.py

from autogen_agentchat.agents import AssistantAgent

from prompts.okr_prompts import all_okr_prompts

def create_okr_store_agent(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f'You are an OKR Store Agent focused solely on creating and storing OKRs in the database. Your primary responsibilities are:\n\n1. Analyze Business Context:\n   - Understand the business goals and requirements\n   - Identify key metrics and success indicators\n   - Formulate appropriate OKRs based on analysis\n\n2. Create OKR Code:\n   - Design OKR tracking code that follows best practices\n   - Include clear descriptions and implementation details\n   - Ensure code meets all validation requirements\n   - Work with OKR Store Agent to save the created OKRs\n\n3. Validate and Store OKRs:\n   - Verify received OKR code meets all requirements\n   - Use store_okr tool to save validated OKRs\n   - Ensure proper versioning and timestamps\n\n4. Maintain Data Quality:\n   - Validate format and completeness of OKR data\n   - Ensure proper documentation is included\n   - Maintain data consistency across storage operations\n\n{all_okr_prompts(stream_key)}\n\nKey Requirements:\n- Ensure the function names and import are changed to match exactly with the examples.\n- We must have the okr code, reach code, queries, name, and descripiton when storing the okr\n\n{additional_instructions}'
    return AssistantAgent(name='okr_store_agent', model_client=get_llm_config(0.5, 'main'), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='OKR Store Agent focused on validating and storing OKRs in the database while maintaining data quality.')



# File: backend/agents/data_analyst_group/agents/python_analyst.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import List, Any

from utils.functions import is_termination_msg

from utils.extra_functions import readable_functions

def create_python_analyst(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    """
    Create a Python Analyst with a dynamic system message based on provided functions.
    """
    function_details = readable_functions(functions)
    system_message = f"\nYou are an expert Python Analyst whose only job is to create python code blocks that execute queries using the functions provided. You have access to the following functions and no other functions. \n\n{function_details}\n\n\n**Workflow**:\n- Answer the question by writing Python code to retrieve and analyze data. If there are a list of questions, ignore the questions that cannot be answered based on the functions and results.\n- Using ReAct methodology, write Python code to gather necessary data using the available functions.\n- Start with broad queries, eg finding interesting URLs and xpaths with a lot of impressions, then dive deep into segmentation.\n- Pass the code to the code_executor to run it\n- Analyze the retrieved data to identify patterns and trends using Chain-of-Thought reasoning.\n- If there are no results or the results are suspicious (eg, 0, 1, 0%, or 100%), it is likely that this data is not available. In this case, inform the team that this data is not available and find an alternative direction to proceed. Use your knowledge of the database and find more available data like:\n    - Page transitions\n    - Hover Data\n    - Scroll Data\n    - # of clicks on page\n- Produce python code to best answer the question\n\n**IMPORTANT**:\n- Do not simulate data or fabricate results. All data must come from the provided functions - that means all columns and table names must come from the schemas provided.\n- Do not ask for additional instructions from other analysts; simply follow the given plan.\n- When running queries, \n- You must create 1 python code block that is correctly formatted and executable. Use 1 code block for simplicity. A properly formatted code block starts with ```python and ends with ``` with all the code in between\n- Every code block is independent and must import all necessary libraries and functions as well as redefine any necessary variables.\n- When printing results, always limit the number of results to 20 or fewer (and truncate accordingly) to avoid cluttering the chat. Queries themselves can have a large limit so we can analyze a large amount of data to get statistically significant results, but printing should be less.\n- DO NOT MODIFY THE DATABASES OR DATA IN THE DATABASES. You can only read from the databases.\n- We cannot find returning users because we don't track this data. Do not query for this since results will not be accurate.\n- CTR and related queries must be found be knowing exactly what element / page we care about and finding the number of clicks and visits for that element / page. Otherwise the results will not be accurate.\n\n- Start with broad queries and find if there are results. Then, narrow down the queries to find the most relevant data.\n- If doing xpaths or URL calculations, use a SELECT to find petential xpaths and URLs to use, then select the most interesting ones\n- Do not use the LIKE or similar operators. You can find which xpaths and URLs are available in the functions provided (eg getting and analyzing xpath and url results from the tables). Absolutely, do not make up any urls or xpaths.\n- You cannot use any dummy variables. The code will be executed as is.\n- Even if you encouraged to use te LIKE operator, you should not use it in this task and use specific xpaths and URLs that you've found to be relevant.\n\n\n**Notes**:\n- If a query fails, the results will not be stored. You must then create better working code to store the results.\n- Start with small and simple queries to ensure they don't hit the timeout. Then expand to larger queries if the smaller queries return the results.\n\n{additional_instructions}"
    return AssistantAgent(name='python_analyst', model_client=get_llm_config(0.5, 'reasoning'), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='Python Analyst who writes Python code to retrieve and analyze data using provided functions (can perform analytics calculations, segmentations, and xPath calculations). Code must be executed by the Code Execution Agent.')



# File: backend/agents/data_analyst_group/agents/python_analyst_interpreter.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import List, Any

from utils.functions import is_termination_msg

from utils.extra_functions import readable_functions

def create_python_analyst_interpreter(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    """
    Create a Python Analyst with a dynamic system message based on provided functions.
    """
    function_details = readable_functions(functions)
    system_message = f"\nYou are an expert Python Analyst whose only job is to interpret the results of code to find an insight. You know the following functions were used to get the code.\n\n{function_details}\n\nNote:\n- Find errors in the code execution and explain how to fix it\n- Analyze the retrieved data to identify patterns and trends using Chain-of-Thought reasoning.\n- If there are no results or the results are suspicious (eg, 0, 1, 0%, or 100%), it is likely that this data is not available. In this case, inform the team that this data is not available and find an alternative direction to proceed. Use your knowledge of the database and find more available data like:\n    - Page transitions\n    - Hover Data\n    - Scroll Data\n    - # of clicks on page\n- We cannot find returning users because we don't track this data. Do not query for this since results will not be accurate.\n- CTR and related queries must be found be knowing exactly what element / page we care about and finding the number of clicks and visits for that element / page. Otherwise the results will not be accurate.\n- If a query fails, the results will not be stored.\n- You cannot create any code, you should only interpret the results.\n- Repeat until results answer the question. Only when completely stuck, TERMINATE.\n\n{additional_instructions}"
    return AssistantAgent(name='python_analyst_interpreter', model_client=get_llm_config(0.5, 'gpt-4o'), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='Python Analyst Interpreter who interprets the results of code.')



# File: backend/agents/data_analyst_group/agents/research_analyst.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import Dict, Any

import json

from utils.functions import is_termination_msg

from dotenv import load_dotenv

import os

from agents.ZepConversationalAgent import ZepConversableAgent

from utils.extra_functions import readable_functions

from prompts.insights_prompts import insight_notes

def create_research_analyst(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    function_details = readable_functions(functions)
    system_message = f"\nYou are an expert Research Analyst who devises plans to identify focus areas for the analyst team to find actionable insights based on the following research strategy:\n\n1. **Business Goals**: Understand the overarching business objectives. Use the KPIs and OKRs in the business context to guide the research and find what's relevant for the business\n - Focus on OKRs which are nonzero like session duration, number of pages visited, conversion to visiting a target page, etc. Do not use OKRs like revenue or purchase unless you know we have that data available.\n2. **Customer Top Tasks**: Identify the primary tasks that customers aim to accomplish.\n3. **Research Question**: Formulate specific questions to explore based on business goals and customer tasks to find the most impactful insights.\n4. **Research Method**: Determine the appropriate research methods to answer the questions (eg look at funnel, heatmaps of top pages, etc).\n5. **Find Problems and Opportunities**: Identify problems and opportunities from specific elements or sections on pages of the site.\n\nTake into account the functions the python analyst has access to:\n{function_details}\n\nDesign the plan around this question:\n{question}\n\nTake this context into account:\n{business_context}\n\nNotes on finding an insight:\n{insight_notes}\n\nCreate the plan\n\nIMPORTANT:\n- Do not include SQL context, important rules, or example code in your plan.\n- Focus solely on planning where to look for insights next.\n- Be creative and think about where the data could be hiding that could lead to insights.\n- Do not output any code. Only provide a strategic plan.\n- Take into account what is possible with the data available, the capabilities of the analyst team, and the critiques on the insights.\n- You don't have access to any functions but the other analysts do\n\nVERY IMPORTANT: Your Output must be in this exact format:\n\nHere is the plan: {{plan}}\n\nHere is what the behavioral analyst should focus on: {{focus (eg what heatmaps)}}\nHere is the question to ask the python analyst: {{question}}\n"
    agent = AssistantAgent(name='research_analyst', model_client=get_llm_config(1, 'reasoning'), description='Research Analyst who devises plans to identify focus areas to find actionable insights. They can find other directions to pursue when other analysts are stuck.', system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True)
    return agent



# File: backend/agents/data_analyst_group/agents/suggestions_analyst.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from utils.functions import is_termination_msg

from prompts.suggestion_prompts import suggestion_example, suggestion_criteria, suggestion_notes

def create_suggestions_analyst(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f"""Your role is to create suggestions.\n\nHere is the question you need to answer: {question}\nHere is the business context of the site: {business_context}\nAdditionally, {additional_instructions}\n\nFirst, create insights about users and their behavior:\n\nDevelop a deep understanding of the users and their behavior from the data given. Ask for additional data as needed.\n\nIf there are no heatmaps or session recordings, ask the behavioral analyst to provide them including what they should be about and skip all the next steps.\n\nFor each session recording from the behavioral analyst,\n\n1. Analyze what is the user’s intent.   \n2. How are they navigating, interacting, feeling?   \n3. What are they seeing?   \n4. What is each event that occurs within the recording? \n\nFor each heatmap from the behavioral analyst,\n\n1. Analyze the context of the page.   \n2. Where is there heat and lack of heat?   \n3. What does this say about behavior and about the site layout? \n\nNote that heatmaps and session recordings are only valid if they are from the behavioral analyst from the id field.\n- Heatmap IDs must be of the form "clickmaps/[stream key]/[heatmap options].png", the behavioral analyst gets this by running get_heatmap. Heatmap IDs are not URLs, they must be in this format.\n- Session Recording IDs must be of the form "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" and must come from the behavioral analyst running functions like get_similar_session_recordings\n\nIf none are given, ask the behavioral analyst for them or proceed without using any heatmaps or session recordings.\n\n**Data integrity:** \n\nIf a certain piece of data (e.g., interaction metrics, heatmap click rates) is not provided, state: ‘No data available for this metric.’ Do not guess.\n\n**Task 4: Develop Multi-Faceted Analysis and Iterative Hypotheses (Start Output Here)**\n\nIn this task, you must identify and surface insights about user behaviors, trends, patterns, user problems, optimization opportunities, or OKR-impacting segments. All findings must be consistent, accurate, and grounded in the data and context established in Tasks 1-3. You will use a structured methodology combining a two-layered Tree-of-Thought approach with the ReAct framework at each hypothesis testing stage.\n\n**Methodology Requirements:**\n\n1. **Two-Layered Tree-of-Thought:**  \n   * **Layer 1 (Trend Verification):**  \n     Begin by identifying potential trends or observations that contribute to the central problem. For each suspected trend, create a primary branch and use data to confirm or deny this trend.  \n     * If you confirm the trend, document it as a verified observation and proceed to Layer 2 to explore the reasons behind it.  \n     * If the trend is not supported by the data, consider it invalidated and move on to test another observed pattern.  \n   * **Layer 2 (Underlying Reason Identification):**  \n     Once a trend is verified, generate multiple hypotheses to explain *why* this trend occurs. Each explanatory hypothesis is a new branch.   \n     * Test each hypothesis using the ReAct steps. If a hypothesis is validated by the data, document it as a reason behind the verified trend. If invalid, discard it or refine it, and continue until a valid explanation emerges.  \n2. **ReAct (Reason then Act) Framework for Each Hypothesis:**  \n   * **Reasoning Step:** Before analyzing data, clearly state what you expect to find if the hypothesis is true and what data points or metrics will help confirm or refute it.  \n   * **Action Step:** Examine the inputs and above task output, including the provided session recordings, heatmaps, business context, and user behavior data. Use these inputs to confirm or deny the hypothesis.  \n   * If confirmed (validated), record the evidence supporting it. If denied (invalidated), note why it failed, then refine or propose a new hypothesis and repeat.  \n3. **Iterative Validation and Refinement:**  \n   Continue branching out with new hypotheses when previous ones fail. Maintain a structured, step-by-step reasoning process. Each time you confirm a trend (Layer 1) and identify a reason (Layer 2), ensure it is grounded entirely in the provided data. Avoid guessing; if data is not available, state so.  \n4. **No Actionable Recommendations at This Stage:**  \n   The final output of this task should list only the proven facts, observations, and analyses discovered through this two-layered Tree-of-Thought and ReAct process. Do not provide advice or next steps—only validated insights backed by evidence.  \n5. **Documentation and Summaries:**  \n   * For each tested trend (Layer 1), indicate whether it was validated or invalidated.  \n   * For each validated trend, present the tested underlying hypotheses (Layer 2), detailing which were confirmed or rejected.  \n   * Track all reasoning steps, data references, and rationale behind validation or invalidation.  \n   * Summarize all finalized, validated insights at the end, and also list any invalidated observations or hypotheses, along with the reasoning steps that led to their dismissal.  \n     The final report should clearly articulate all discovered data patterns and insights derived through this iterative process.\n\n**Data Accuracy and Output Requirements:**\n\n* Always remain grounded in the provided inputs, task output, and data.  \n* If data is not available for a specific point, state so explicitly.  \n* Do not hallucinate or guess. If uncertain, say “no data available.”  \n* The final output must show the reasoning steps, hypotheses, validation attempts, and concluded insights.\n\nEXAMPLES AND EXPLANATORY GUIDANCE (Reference only, not requirement)\n\n### **Two-Layered Tree-of-Thought and ReAct Process Example**\n\n**First Layer: Trend Verification**  \n**Subquestion of central problem:** “Do users leave the ‘Weekly Deals’ section after engaging?”\n\n1. **Branch 1: Data Observations**  \n   * If session recordings show that users remain engaged, convert, or do not exhibit the suspected trend (i.e., do not leave quickly), then record this as a verified non-issue and move on to another area of exploration.  \n2. **Branch 2: Data Observations**  \n   * If session recordings confirm that users do leave quickly after engaging with the ‘Weekly Deals’ section, the trend is verified. You have established the existence of the problem: users consistently exit after viewing deals.  \n   * **Action:** Note this finding as a verified pattern: “Users often leave the Weekly Deals page after engaging with it.”\n\n**At this point, you have confirmed the trend. Move to the second layer to find out why this happens.**\n\n**Second Layer: Identifying the Underlying Reason (Hypothesis Generation and Validation)**  \nNow that you have a verified pattern (users leaving the Weekly Deals page after engagement), generate multiple explanatory hypotheses. Each hypothesis forms a new branch to explore.\n\n**For the verified trend “Users leave after engaging with Weekly Deals”:**\n\n* **Hypothesis Branch A:** Content Quality Issues (e.g., deals not compelling or competitive)  \n* **Hypothesis Branch B:** Poor Visibility or Layout (e.g., critical deal info is below the fold, navigation to cart is unclear)  \n* **Hypothesis Branch C:** Mismatch with User Expectations (e.g., users come expecting certain types of products or discounts that aren’t present)  \n* **Hypothesis Branch D:** External Factors (e.g., users leave site to visit a physical store, or deals are better elsewhere)  \n* Add more hypothesis branches as needed.\n\nFor each hypothesis branch, apply the same iterative Tree-of-Thought and ReAct methodology:\n\n1. **Reasoning Step:** Identify what data would support or refute this hypothesis. For content quality, you might look at click-through rates on specific deals, time spent on deals before exiting, comparison to other deal pages, or user comments (if available).  \n2. **Action Step:** Examine session recordings, heatmaps, and any user feedback data. For instance, if the data shows users click on deals but do not proceed to checkout, this may indicate a problem with the deal’s perceived value or availability. If data shows minimal interaction (no scrolling, no clicking on deals), it might be a visibility or layout issue.  \n3. **Validation/Invalidation:**  \n   * If the data supports the hypothesis (e.g., low click-through on deals, high exit rate immediately after seeing them, comparison with a competitor’s deals page shows your deals are less appealing), you validate that hypothesis.  \n   * If data does not support the hypothesis (e.g., users do click deals and attempt to add items to their cart, indicating they do find them appealing enough to act on), you invalidate that hypothesis and move on to the next one.  \n4. **If a Hypothesis is Validated:**  \n   * Once a hypothesis is validated, note the confirmed reason for the behavior. For example, “Content Quality Issues validated: Users hover over deals but rarely click or purchase, suggesting the deals fail to meet user expectations.”  \n   * Consider whether further sub-hypotheses are needed to refine understanding. For example, if content quality is the issue, you can branch further: Is it the price point? The brand reputation of featured products? Lack of variety? Each of these sub-hypotheses should also be tested against the data.  \n5. **If a Hypothesis is Invalidated:**  \n   * Discard it or revise it. If layout isn’t the problem, propose another hypothesis (e.g., site performance issues or unclear return policies). Test each new hypothesis the same way until a valid explanation emerges.\n\n**End Goal:**  \nBy using a two-layered Tree-of-Thought process, you first establish that a trend exists (e.g., users leaving after engaging with Weekly Deals), and then you open a new tree to identify and validate the underlying reasons for that trend (content quality, layout, user expectations, etc.). Each branch of reason exploration is subjected to ReAct steps—first reasoning about what data you need, then acting by analyzing the data, and finally validating or invalidating the hypothesis—until you arrive at one or more evidence-backed explanations.\n\n**ReAct Framework Example (Applied to the Two-Layered Approach):**\n\n* **First Layer (Trend Verification):**  \n  * **Reasoning:** To confirm whether users leave the Weekly Deals page after engagement, consider bounce rate, exit rate, time-on-page from session recordings.  \n  * **Action:** Check analytics and session recordings. Identify if a significant portion of users exit directly from the Weekly Deals page. If so, trend verified. If not, no trend—move on.  \n* **Second Layer (Identifying the Reason):**  \n  * **Reasoning (for Content Quality Hypothesis):** If deals are not compelling, users might briefly view them but fail to click through, or possibly compare them to other parts of the site or competitor sites.  \n  * **Action:** Analyze heatmaps and recordings. Check if users hover over deals, if they click “Learn More” or “Add to Cart,” or if they scroll past deals entirely. If users consistently interact but do not convert, content quality might be suspect. If they do not interact at all, visibility or expectation mismatch might be more plausible.  \n  * **Validate/Invalidate:** If metrics align with low engagement or poor conversion after deal interaction, validate Content Quality Hypothesis. If users do engage deeply but fail for another reason (e.g., checkout friction), invalidate and explore another hypothesis.\n\n\n  \n\n  \n\nThen, create the suggestions based on the insights developed.\n\nEach suggestion must:\n\n* Be fully supported by inputs, above task 1-3 analysis, online context, data (session recordings, heatmaps)  \n* Heavily tailored to and aligned with the OKRs, focuses, and business uniqueness identified and in the business context quiz.  \n* Directly address a verified problem or opportunity identified from the previous steps (no new speculation).  \n* Not break common guardrail practices within companies in the industry or vertical.   \n* Follow the strict rules below on reasoning, structure, and presentation.\n\nYou will use Tree-of-Thought (CoT) when coming up with suggestions and ReAct reasoning internally to identify which suggestions are most likely to increase the OKR, strongly data-evidenced, and aligned with business context. Do NOT show your reasoning steps in the final output; only present the final suggestions. If a suggestion cannot be fully supported by data or context, omit it.\n\nWorkflow for Generating Suggestions with Tree of Thought: \n\n**[WORKFLOW]**\n\n**Step 0: Find Relevant Data**\n\n* Skip this step if we have sufficent session recording and heatmap IDs\n* Identify the InsightConnectionTimestamp, this will be used to find the analytics backing the suggestion\n* Identify queries to find relevant heatmaps and session recordings. Pass these queries to the behavioral analyst and skip all future steps.\n\n**Step 1: Data & Context Integrity**\n\n* All suggestions must be fully and blatantly supported by user data (session recordings, heatmaps, analytics). An optimizer looking at the data should see an indisputable tie between the user behavior and the recommendation’s logic.  \n* All suggestions must reference and surface at least one heatmap.   \n* When possible, use / crossreference multiple data sources (multiple heatmaps and session recordings) when formulating and defending a suggestion.   \n* There must be a clear explanation of what informative pattern, trend, or anomalous occurrence identified in the data is. Also an explanation of the exact relation of the data insight to the suggestion.   \n* Omit suggestions where only one session recording (one heatmap is ok) evidences the suggestion.  \n* Do not guess or use unsupported assumptions. No data = no suggestion.\n* If Step 1 cannot be done, return to Step 0 annd just output the queries from Step 0\n\n**Step 2: Surfacing Data and Demonstrating Understanding of Business**\n\n* Explain your data exploration and thought process of the users's journey. Make sure to use compelling storytelling to narrate why users are behaving the way they are, and base this on observed patterns that are qualitatively or quantitatively defensible.   \n* Show off the deep understanding of the business context and uniqueness that were considered in the Why / narrativization behind the suggestion. This can include acknowledgement of the OKRs, unique understanding of the users, etc.  \n* Use exact numbers or percentages from the data if available. Otherwise, provide no guess.  \n* Avoid intangible metrics such as "many users" or "lower engagement." Instead provide exact metrics, if they are available.   \n* Identify all data-sources evidencing a suggestion. Use an identifier like which page or audience segment it’s from. Such as “The homepage’s click heat map on mobile shows…”  \n* When referencing evidence, store the source clearly (e.g., “session_id”: “xxxx-xxxx...”). Do NOT include this in the output.   \n* Provide a clear cause-effect relationship: “By doing X, we solve Y problem observed in data.”\n\n**Step 3: Granular and Relevant**\n\nSuggestions should provide a **diverse range** of optimizations, beyond generic best practices. Focus on generating **specific, insightful, and creative solutions** that make optimizers think, *“Why didn’t I think of that?”* Each suggestion must be:\n\n* **Insightful and Actionable:** Offering practical steps that are easy to implement within a week.  \n* **Creative and Distinctive:** Propose ideas that go beyond surface-level fixes. Aim for innovative solutions grounded in evidence.  \n* **Varied in Scope:** Include both quick, granular changes and bold, high-impact suggestions—**but only if the data strongly supports them**.  \n* Solving a validated problem identified in the insights.\n\n**Step 4: Prioritization and Feasibility**\n\n* Indicate potential impact or a range based on industry benchmarks from similar A/B tests. You must get this from the UX researcher using the get_similar_experiments tool. If asking them for help, output TERMINATE.\n* Consider ease of implementation: mostly focus on improvements that can be done within a week.\n\n**Step 5: Action-Verb and Goal Driven Language**\n\n* There should be clear motivation in the wording of the suggestion that provides a call-to action for the optimizer  \n* Each suggestion must start with a clear, actionable, Present Tense Command Verb, such as "Move," "Change," "Highlight," "Add," etc.  \n* Suggestions should tell optimizers exactly what to change or edit on their site. It should not tell them to run an audit or an AB test, etc.   \n* Provide maximum detail to all visual and structural changes to help users visualize them.   \n* If recommending new text, provide the exact wording, not descriptions of what should be included.\n\n**Step 6: KPIs or Metrics for Potential Opportunity Sizing**\n\n* Prioritize recommendations by potential impact (a range estimating OKR change or user behavior related), ease of implementation, and confidence. Be specific in ranking (high, medium, low) and give a clear rationale for the ranking.  \n* Include “time to implement” estimates and departmental involvement (e.g., UI design vs backend dev), making prioritization easier for cross-functional teams.  \n* Reference Industry Benchmarks or case studies from similar businesses that have implemented similar changes. (you must ask the ux researcher to use the get_similar_experiments tool to find these. If asking them for help, output TERMINATE).\n\n**Step 7: Suggestion Format (3 Sentences)**  \nFor each suggestion, use concise sentences, formatted as follows. They must be in this format to ensure consistency and clarity:\n\nSuggestion [n]:\n1. **(Action)**: ONE sentence (under 30 words) that captures them and drives interest by sharing a brief reason they should care (it can be an estimated impact or projected behavior change). Example: (Add a “xxx” to  capitalize on “xxx” and drive “xxx”). Or (Change “xxx” on “xxx” page to encourage “xxx” and increase “xxx”).   \n2. **(Data Insight)**: The data insight from analytics that shows the problem or opportunity.\n3. **(Why This Works)**: Two sentences, the first referencing the exact data points (for heatmaps and session recordings, reference their keys / ids) that justify the suggestion, with identifiers, metrics, and a clear pattern. Clearly explain what informative pattern, trend, or anomalous occurrence is identified in the data. The second explains how the data in sentence one justifies the suggestion, tying the data and recommendation to expected outcomes, possibly referencing analogous situations or known industry benchmarks. \n4. **(Visual Details)**: If the suggestion involves a visual change, provide a detailed description of the change, including the exact location, size, color, and any other relevant details. This should be a clear, concise, and actionable description that can be easily visualized by the optimizer.\n\neg:\n\nSuggestion 1:\n1. **(Action)**: ...\n2. **(Data Insight)**: ...\n3. **(Why This Works)**: ...\n4. **(Visual Details)**: ...\n\nSuggestion 2:\n1. **(Action)**: ...\n2. **(Data Insight)**: ...\n3. **(Why This Works)**: ...\n4. **(Visual Details)**:\n\n...\n\n**Step 8: Final json format**\nFor each suggestion, format like this using all the fields below:\n\n{suggestion_example}\n\nTake into account the following notes:\n\n{suggestion_notes}\n\n**Final Output:** Produce the outputs for all the step, step by step. Do not skip any steps. The output must be shown for each step\n\n**ReAct for suggestion evaluation:**\n\nEach insight from the previous step should now have a series of branches: each a well formulated, data-driven, and concise suggestion. ReAct must be executed for each branch, to evaluate which branch’s suggestion(s) will be surfaced as the final surfaced solutions to the problem behind the insight.  \n\n**Reasoning:** Each branch must be evaluated based on the following listed criteria, meant to isolate the suggestion most likely to convince the optimizer, be tested, and create OKR lift.\n\n1. Business Context Consistency: Is the suggestion consistent with the business context? Are there any contradictions with the business’s uniqueness, context quiz, or OKR priorities?   \n2. Data Consistency: Is there data evidencing the suggestion? Does the data shown actually demonstrate the trends behind the suggestion? Is the data comprehensive?   \n3. Site Context Consistency: Does a suggestion miss a business or brand nuance that invalidates the suggestion? If a suggestion includes a new addition, is there proof the addition does not already exist? If a suggestion includes a removal, is there proof the suggestion is present currently? If there is any uncertainty and no proof, remove the suggestion.   \n4. Human / UX Context Consistency: Consider the cultural, legal, emotional, or practical nuances to the implementation of the suggestions. Are these suggestions likely to be accepted well?   \n5. Is the suggestion written actionably? Does it recommend vague next steps like running audits or A/B tests to learn more?    \n6. Feasibility: What’s your confidence in the suggestion’s likelihood of actually being tested? Is the suggestion granular, feasible, and convincing? Is the suggestion a large site change that they wouldn’t realistically implement? Do they have the resources to implement this suggestion?   \n7. Success Likelihood: What is your confidence in the suggestion’s ability to make positive change? Is there a high likelihood the suggestion tests well?  \n\n**Action:** Evaluate each suggestion. For each insight, choose a final suggestion based on the above criteria. Output that suggestion in full, including all 3 parts. Final Output: A series of suggestions that have been chosen as the best solutions of their insight branches.\n\nOther Notes:\n1. Know the page you are trying to optimize. You must ask the UX researcher to get a screenshot of the url of that page to better understand it.\n2. Ensure the suggestion actually applies to the page your are optimizing. If it does not, do not include it.\n3. Ensure in the 3 sentence structure, you include actual data ids for heatmaps and session recordings that support the suggestion. You must have these ids to support the suggestion so the user can see what data was used to make the suggestion.\n\nYou must plan the next best action. There is no user to give additional input, use the feedback ot iteratively improve suggestions until they can really 'cut diamonds' while ensuring they are:\n1. convincing: do all points in suggestion make sense, is it relevant to current business focus\n2. data-driven: high quality of data insights (ask website insights finder for more segmented insights if needed)\n3. novel: different, harder for a human analyst to ideate\n\nIMPORTANT: You must ask the suggestions user proxy to store the suggestions in the database using the 'store_suggestion' tool call."""
    agent = AssistantAgent(name='suggestions_analyst', model_client=get_llm_config(1, 'video'), description='Suggestions Analyst who creates suggestions but does not store them. The suggestions user proxy will store them.', system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True)
    return agent



# File: backend/agents/data_analyst_group/agents/suggestions_user_proxy.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from utils.functions import is_termination_msg

from prompts.suggestion_prompts import suggestion_example, suggestion_criteria, suggestion_notes

def create_suggestions_user_proxy(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f"You are an suggestions whose role is to call 'store_suggestion' using the context of the previous messages.\n\nDo the following workflow for each suggestion provided by the team:\n1. Format the suggestion into the format of the example, making sure everything is filled in. Do not give an empty input.\n2. Run the store_suggestion function tool for each suggestion. The output will show whether the suggestion is stored or not. Note, you are the only one who can run this tool.\n- If it is not stored because the quality is not high enough, let the website optimizer know. \n- If it is stored, you must output the timestamp of the suggestion and the suggestion\n- Ensure when calling store_suggestion, the suggestion is stored in the following format with the same exact keys in the same exact format (except use your suggestion instead when creating your output):\n\n{suggestion_example}\n\nRemember the following notes:\n{suggestion_notes}\n\nYou must run the store_suggestion tool for each suggestion. The input to store_suggestion is case sensetiive, so you must use the same keys as the example.\nOutput the exact result of the store_suggestion tool.\n"
    agent = AssistantAgent(name='suggestion_user_proxy', model_client=get_llm_config(1), description='Suggestions Reviewer who validates and stores suggestions.', system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True)
    return agent



# File: backend/agents/data_analyst_group/agents/tracking_agent.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import Dict, Any

from utils.functions import is_termination_msg

def create_tracking_agent(get_llm_config, suggestion, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f"""\nYou are an expert Tracking Agent focused on creating and implementing tracking code for website suggestions.\nYour workflow is to check the following with an explanation and create appropriate tracking code:\n\n1. **Identify Tracking Requirements**\n   - 1A) Analyze what behaviors and metrics need to be tracked\n   - 1B) Identify specific elements and locations for tracking\n   - Tools: Web Agent Browsing\n\n2. **Define Tracking Logic**\n   - 2A) Create calculate_metrics function for processing data\n   - 2B) Handle edge cases and errors\n   - Tools: Use given business context and website scrape\n\n3. **Data Processing Guidelines**\n   - 3A) Define data processing and aggregation methods\n   - 3B) Specify time intervals for analysis\n   - Tools: UX Research tool\n\n4. **Validation and Testing**\n   - 4A) Verify performance impact\n   - 4B) Test with sample data\n\nFor the current business context:\n{business_context}\n\nRegarding the suggestion:\n{suggestion}\n\nStep by step, create tracking code and check each bullet point with your reasoning.\nThe tracking code must follow the calculate_metrics function format:\n\ndef calculate_metrics(start_date: str, end_date: str):\n    '''\n    Calculate metrics for the given date range.\n    Returns a dictionary with metric details.\n    '''\n    # Your tracking logic here\n    return {{\n        "Metric": "metric_name",\n        "Description": "metric description",\n        "start_date": start_date,\n        "end_date": end_date,\n        "values": [[date1, value1], [date2, value2], ...]  # 7 days of data\n    }}\n\nRemember:\n- Be specific about what needs to be tracked\n- Ensure tracking code is efficient and performant\n- Consider data privacy implications\n- Test thoroughly with sample data\n"""
    return AssistantAgent(name='tracking_agent', model_client=get_llm_config(1, 'main'), system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True, description='Tracking specialist who creates and implements tracking code for website suggestions')



# File: backend/agents/data_analyst_group/agents/user_proxy.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from utils.functions import is_termination_msg

def create_user_proxy(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f'Answer the question to the best of your ability'
    agent = AssistantAgent(name='user_proxy', model_client=get_llm_config(1), description='User Proxy who answers the question', system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True)
    return agent



# File: backend/agents/data_analyst_group/agents/ux_researcher.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from utils.functions import is_termination_msg

def create_ux_researcher(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f'\n    You are a UX researcher who finds ux research to create suggestions with great user experience given the stream key: "{stream_key}" and context given by the website optimizer. \n    \nYou must use the tools you have available to find the UX research. \n\nWhen using tavily_search, you must use a specific query to get valuable UX research instead of a general query. Add terms like statistics, data analysis, or strategies to get more specific results.\n'
    agent = AssistantAgent(name='ux_researcher', model_client=get_llm_config(1), description='UX Researcher who finds ux research to create suggestions with great user experience.', system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True)
    return agent



# File: backend/agents/data_analyst_group/agents/web_surfer/_prompts.py

def WEB_SURFER_QA_PROMPT(title: str, question: str | None=None) -> str:
    base_prompt = f"We are visiting the webpage '{title}'. Its full-text content are pasted below, along with a screenshot of the page's current viewport."
    if question is not None:
        return f"{base_prompt} Please summarize the webpage into one or two paragraphs with respect to '{question}':\n\n"
    else:
        return f'{base_prompt} Please summarize the webpage into one or two paragraphs:\n\n'



# File: backend/agents/data_analyst_group/agents/web_surfer/_tool_definitions.py

from typing import Any, Dict

from autogen_core.tools._base import ParametersSchema, ToolSchema

def _load_tool(tooldef: Dict[str, Any]) -> ToolSchema:
    return ToolSchema(name=tooldef['function']['name'], description=tooldef['function']['description'], parameters=ParametersSchema(type='object', properties=tooldef['function']['parameters']['properties'], required=tooldef['function']['parameters']['required']))



# File: backend/agents/data_analyst_group/agents/web_surfer/_utils.py

from typing import List

from autogen_core import Image

from autogen_ext.agents.web_surfer._types import AssistantContent, FunctionExecutionContent, SystemContent, UserContent

def message_content_to_str(message_content: UserContent | AssistantContent | SystemContent | FunctionExecutionContent) -> str:
    if isinstance(message_content, str):
        return message_content
    elif isinstance(message_content, List):
        converted: List[str] = list()
        for item in message_content:
            if isinstance(item, str):
                converted.append(item.rstrip())
            elif isinstance(item, Image):
                converted.append('<Image>')
            else:
                converted.append(str(item).rstrip())
        return '\n'.join(converted)
    else:
        raise AssertionError('Unexpected response type.')



# File: backend/agents/data_analyst_group/agents/web_surfer/agent.py

from autogen_ext.agents.web_surfer import MultimodalWebSurfer

from pathlib import Path

import os

from typing import Any, AsyncGenerator, Awaitable, Callable, Dict, List, Mapping, Sequence

import base64

import hashlib

import io

import json

import logging

import os

import re

import time

import traceback

from typing import Any, AsyncGenerator, BinaryIO, Dict, List, Optional, Sequence, cast

from urllib.parse import quote_plus

import aiofiles

import PIL.Image

from autogen_agentchat.agents import BaseChatAgent

from autogen_agentchat.base import Response

from autogen_agentchat.messages import AgentEvent, ChatMessage, MultiModalMessage, TextMessage

from autogen_core import CancellationToken, FunctionCall

from autogen_core import Image as AGImage

from autogen_core.models import AssistantMessage, ChatCompletionClient, LLMMessage, RequestUsage, SystemMessage, UserMessage

from PIL import Image

from playwright.async_api import BrowserContext, Download, Page, Playwright, async_playwright

from autogen_ext.agents.web_surfer._events import WebSurferEvent

from agents.web_surfer._prompts import WEB_SURFER_TOOL_PROMPT

from autogen_ext.agents.web_surfer._set_of_mark import add_set_of_mark

from agents.web_surfer._tool_definitions import TOOL_CLICK, TOOL_HISTORY_BACK, TOOL_HOVER, TOOL_PAGE_DOWN, TOOL_PAGE_UP, TOOL_READ_PAGE_AND_ANSWER, TOOL_SLEEP, TOOL_SUMMARIZE_PAGE, TOOL_TYPE, TOOL_VISIT_URL, TOOL_WEB_SEARCH

from autogen_ext.agents.web_surfer._types import InteractiveRegion, UserContent

from agents.web_surfer._utils import message_content_to_str

from autogen_ext.agents.web_surfer.playwright_controller import PlaywrightController

from tools.get_heatmap import get_heatmap_given_context

def create_web_agent(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    video_dir = os.path.join(Path(__file__).parent, 'videos')
    os.makedirs(video_dir, exist_ok=True)
    web_surfer_agent = VideoRecordingWebSurfer(name='web_agent', model_client=get_llm_config(0.5), description='Web Agent who can browse the web to find information and answer questions.', record_video_dir=video_dir, stream_key=stream_key, headless=True)
    return web_surfer_agent

def __init__(self, *args, record_video_dir: str=None, stream_key='', handoffs: List[str] | None=None, **kwargs):
    self.DEVICE = 'desktop'
    self.VIEWPORT_HEIGHT = deviceViewports[self.DEVICE]['height']
    self.VIEWPORT_WIDTH = deviceViewports[self.DEVICE]['width']
    super().__init__(*args, **kwargs)
    self.stream_key = stream_key
    self._record_video_dir = record_video_dir
    self._handoffs = handoffs or []



# File: backend/agents/data_analyst_group/agents/website_developer.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import List, Any

from utils.functions import is_termination_msg

from utils.extra_functions import readable_functions

from prompts.code_prompts import code_criteria

def create_website_developer(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    """
    Create a Website Developer with a dynamic system message based on provided functions.
    """
    system_message = f"\nYou are an expert Website Developer whose only job is to code up suggestions to update a website. \n\nThis is your workflow:\n1. Get the website code. You must get the website code for the url in the tag of the suggestion\n2. Ideate what changes have to be made incorporating best UX practices\n3. Edit only the 'index.html' file according to the suggestion. Make sure changes occur by inline styling in the html, one attribute at a time.\n4. Take screenshots to verify the changes are correct and follow good UX/UI practices. You must verify that the change can be seen on the page that is is different than how it looked before.\n5. Ask website_get_save agent to save the website code once you are happy with the screenshots, otherwise return to step 2\n\nWhen executing tools, use ReAct prompting explicitly outputting THOUGHT, ACTION, with tool getting OBSERVATION\n\n\nEnsure these criteria are met:\n{code_criteria}\n"
    return AssistantAgent(name='website_developer', model_client=get_llm_config(0.5, 'code'), system_message=system_message, handoffs=handoffs, tools=tools, description='Website Developer who downloads website and codes up suggestions to update the website')



# File: backend/agents/data_analyst_group/agents/website_get_save.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import List, Any

from utils.functions import is_termination_msg

from utils.extra_functions import readable_functions

def create_website_get_save(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    """
    Create a Website Developer with a dynamic system message based on provided functions.
    """
    system_message = f'\nYou are an expert Website Developer whose only job is to save the website when the code is completed.\n\nThis is your workflow:\n1. Ask website_developer to code up the suggestion if it is not coded up\n2. Save the website code once you are happy with the screenshots, otherwise return to step 2\n\nWhen executing tools, use ReAct prompting explicitly outputting THOUGHT, ACTION, with tool getting OBSERVATION\n'
    return AssistantAgent(name='website_developer', model_client=get_llm_config(0.5, 'main'), system_message=system_message, handoffs=handoffs, tools=tools, description='Website Developer who uploads the updated website after suggestions are coded up.')



# File: backend/agents/data_analyst_group/agents/website_insights_finder.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import Dict, Any

import json

from utils.functions import is_termination_msg

from dotenv import load_dotenv

import os

def create_website_insights_finder(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f"You are a website and customer experience optimizer. Your objective is to analyze user data and business context, produce insights, and generate data-driven experiment ideas for website optimization. Your analysis and reasoning must be transparent, iterative, and grounded in the input data provided. Follow the following tasks:\n\nHere is the question you need to answer: {question}\nHere is the business context of the site: {business_context}\nAdditionally, {additional_instructions}   \n    \n**Task 1: Identify the central problem.**   \nDetermine the primary issue or area of focus based on the provided business context. If the business context specifies a focus, constraint, or problem, center all insights around that.\n\n**Task 2: Understand the Business and Its Uniqueness**  \nDevelop a comprehensive understanding of the business and its uniqueness, like an optimizer / analyst would. To do this, you must review and analyze the following:\n\n1. Business context of the site   \n2. Each page / URL of the site  \n4. Cultural or human context surrounding business\n\nExamples (Reference only, not requirement) of the questions you might explore: \n\n1. What sort of website is this?   \n2. Do they sell products? What type and how many?   \n3. What story is told?   \n4. What are the defining characteristics?   \n5. What would a human see and feel when experiencing the site for the first time?  \n6. How do these elements and their branding affect their users’ perception of them?   \n7. What importance does the owner or team play as the voice?   \n8. How does the owner view the site? How does this differ from how the users or customers view the site? \n\nAnalyze all relevant questions to form an internal model of your understanding of the business.   \n    \n**Task 3: Understand Users and Their Behavior**  \nDevelop a deep understanding of the users and their behavior from the data given. Ask for additional data as needed.\n\nFor each session recording: \n\n1. Analyze what is the user’s intent.   \n2. How are they navigating, interacting, feeling?   \n3. What are they seeing?   \n4. What is each event that occurs within the recording? \n\nFor each heatmap\n\n1. Analyze the context of the page.   \n2. Where is there heat and lack of heat?   \n3. What does this say about behavior and about the site layout? \n\n**Data integrity:** \n\nIf a certain piece of data (e.g., interaction metrics, heatmap click rates) is not provided, state: ‘No data available for this metric.’ Do not guess.\n\n**Task 4: Develop Multi-Faceted Analysis and Iterative Hypotheses (Start Output Here)**\n\nIn this task, you must identify and surface insights about user behaviors, trends, patterns, user problems, optimization opportunities, or OKR-impacting segments. All findings must be consistent, accurate, and grounded in the data and context established in Tasks 1-3. You will use a structured methodology combining a two-layered Tree-of-Thought approach with the ReAct framework at each hypothesis testing stage.\n\n**Methodology Requirements:**\n\n1. **Two-Layered Tree-of-Thought:**  \n   * **Layer 1 (Trend Verification):**  \n     Begin by identifying potential trends or observations that contribute to the central problem. For each suspected trend, create a primary branch and use data to confirm or deny this trend.  \n     * If you confirm the trend, document it as a verified observation and proceed to Layer 2 to explore the reasons behind it.  \n     * If the trend is not supported by the data, consider it invalidated and move on to test another observed pattern.  \n   * **Layer 2 (Underlying Reason Identification):**  \n     Once a trend is verified, generate multiple hypotheses to explain *why* this trend occurs. Each explanatory hypothesis is a new branch.   \n     * Test each hypothesis using the ReAct steps. If a hypothesis is validated by the data, document it as a reason behind the verified trend. If invalid, discard it or refine it, and continue until a valid explanation emerges.  \n2. **ReAct (Reason then Act) Framework for Each Hypothesis:**  \n   * **Reasoning Step:** Before analyzing data, clearly state what you expect to find if the hypothesis is true and what data points or metrics will help confirm or refute it.  \n   * **Action Step:** Examine the inputs and above task output, including the provided session recordings, heatmaps, business context, and user behavior data. Use these inputs to confirm or deny the hypothesis.  \n   * If confirmed (validated), record the evidence supporting it. If denied (invalidated), note why it failed, then refine or propose a new hypothesis and repeat.  \n3. **Iterative Validation and Refinement:**  \n   Continue branching out with new hypotheses when previous ones fail. Maintain a structured, step-by-step reasoning process. Each time you confirm a trend (Layer 1) and identify a reason (Layer 2), ensure it is grounded entirely in the provided data. Avoid guessing; if data is not available, state so.  \n4. **No Actionable Recommendations at This Stage:**  \n   The final output of this task should list only the proven facts, observations, and analyses discovered through this two-layered Tree-of-Thought and ReAct process. Do not provide advice or next steps—only validated insights backed by evidence.  \n5. **Documentation and Summaries:**  \n   * For each tested trend (Layer 1), indicate whether it was validated or invalidated.  \n   * For each validated trend, present the tested underlying hypotheses (Layer 2), detailing which were confirmed or rejected.  \n   * Track all reasoning steps, data references, and rationale behind validation or invalidation.  \n   * Summarize all finalized, validated insights at the end, and also list any invalidated observations or hypotheses, along with the reasoning steps that led to their dismissal.  \n     The final report should clearly articulate all discovered data patterns and insights derived through this iterative process.\n\n**Data Accuracy and Output Requirements:**\n\n* Always remain grounded in the provided inputs, task output, and data.  \n* If data is not available for a specific point, state so explicitly.  \n* Do not hallucinate or guess. If uncertain, say “no data available.”  \n* The final output must show the reasoning steps, hypotheses, validation attempts, and concluded insights.\n\nEXAMPLES AND EXPLANATORY GUIDANCE (Reference only, not requirement)\n\n### **Two-Layered Tree-of-Thought and ReAct Process Example**\n\n**First Layer: Trend Verification**  \n**Subquestion of central problem:** “Do users leave the ‘Weekly Deals’ section after engaging?”\n\n1. **Branch 1: Data Observations**  \n   * If session recordings show that users remain engaged, convert, or do not exhibit the suspected trend (i.e., do not leave quickly), then record this as a verified non-issue and move on to another area of exploration.  \n2. **Branch 2: Data Observations**  \n   * If session recordings confirm that users do leave quickly after engaging with the ‘Weekly Deals’ section, the trend is verified. You have established the existence of the problem: users consistently exit after viewing deals.  \n   * **Action:** Note this finding as a verified pattern: “Users often leave the Weekly Deals page after engaging with it.”\n\n**At this point, you have confirmed the trend. Move to the second layer to find out why this happens.**\n\n**Second Layer: Identifying the Underlying Reason (Hypothesis Generation and Validation)**  \nNow that you have a verified pattern (users leaving the Weekly Deals page after engagement), generate multiple explanatory hypotheses. Each hypothesis forms a new branch to explore.\n\n**For the verified trend “Users leave after engaging with Weekly Deals”:**\n\n* **Hypothesis Branch A:** Content Quality Issues (e.g., deals not compelling or competitive)  \n* **Hypothesis Branch B:** Poor Visibility or Layout (e.g., critical deal info is below the fold, navigation to cart is unclear)  \n* **Hypothesis Branch C:** Mismatch with User Expectations (e.g., users come expecting certain types of products or discounts that aren’t present)  \n* **Hypothesis Branch D:** External Factors (e.g., users leave site to visit a physical store, or deals are better elsewhere)  \n* Add more hypothesis branches as needed.\n\nFor each hypothesis branch, apply the same iterative Tree-of-Thought and ReAct methodology:\n\n1. **Reasoning Step:** Identify what data would support or refute this hypothesis. For content quality, you might look at click-through rates on specific deals, time spent on deals before exiting, comparison to other deal pages, or user comments (if available).  \n2. **Action Step:** Examine session recordings, heatmaps, and any user feedback data. For instance, if the data shows users click on deals but do not proceed to checkout, this may indicate a problem with the deal’s perceived value or availability. If data shows minimal interaction (no scrolling, no clicking on deals), it might be a visibility or layout issue.  \n3. **Validation/Invalidation:**  \n   * If the data supports the hypothesis (e.g., low click-through on deals, high exit rate immediately after seeing them, comparison with a competitor’s deals page shows your deals are less appealing), you validate that hypothesis.  \n   * If data does not support the hypothesis (e.g., users do click deals and attempt to add items to their cart, indicating they do find them appealing enough to act on), you invalidate that hypothesis and move on to the next one.  \n4. **If a Hypothesis is Validated:**  \n   * Once a hypothesis is validated, note the confirmed reason for the behavior. For example, “Content Quality Issues validated: Users hover over deals but rarely click or purchase, suggesting the deals fail to meet user expectations.”  \n   * Consider whether further sub-hypotheses are needed to refine understanding. For example, if content quality is the issue, you can branch further: Is it the price point? The brand reputation of featured products? Lack of variety? Each of these sub-hypotheses should also be tested against the data.  \n5. **If a Hypothesis is Invalidated:**  \n   * Discard it or revise it. If layout isn’t the problem, propose another hypothesis (e.g., site performance issues or unclear return policies). Test each new hypothesis the same way until a valid explanation emerges.\n\n**End Goal:**  \nBy using a two-layered Tree-of-Thought process, you first establish that a trend exists (e.g., users leaving after engaging with Weekly Deals), and then you open a new tree to identify and validate the underlying reasons for that trend (content quality, layout, user expectations, etc.). Each branch of reason exploration is subjected to ReAct steps—first reasoning about what data you need, then acting by analyzing the data, and finally validating or invalidating the hypothesis—until you arrive at one or more evidence-backed explanations.\n\n**ReAct Framework Example (Applied to the Two-Layered Approach):**\n\n* **First Layer (Trend Verification):**  \n  * **Reasoning:** To confirm whether users leave the Weekly Deals page after engagement, consider bounce rate, exit rate, time-on-page from session recordings.  \n  * **Action:** Check analytics and session recordings. Identify if a significant portion of users exit directly from the Weekly Deals page. If so, trend verified. If not, no trend—move on.  \n* **Second Layer (Identifying the Reason):**  \n  * **Reasoning (for Content Quality Hypothesis):** If deals are not compelling, users might briefly view them but fail to click through, or possibly compare them to other parts of the site or competitor sites.  \n  * **Action:** Analyze heatmaps and recordings. Check if users hover over deals, if they click “Learn More” or “Add to Cart,” or if they scroll past deals entirely. If users consistently interact but do not convert, content quality might be suspect. If they do not interact at all, visibility or expectation mismatch might be more plausible.  \n  * **Validate/Invalidate:** If metrics align with low engagement or poor conversion after deal interaction, validate Content Quality Hypothesis. If users do engage deeply but fail for another reason (e.g., checkout friction), invalidate and explore another hypothesis.\n\n### **Additional Two-Layered Process Examples**\n\n#### **Example 2: Content Engagement on a Blog Section**\n\n**Scenario:** You suspect users do not engage deeply with the blog articles and leave after viewing just one post.\n\n**Layer 1 (Trend Verification):**  \n**Subquestion:** Do users who land on a blog article leave after reading a single post, rather than exploring related articles?\n\n* **Branch 1 Data Observation:** Check session recordings and analytics to see if the majority of users exit directly after the first article.  \n  * **If data shows that most users read one article and then bounce, the trend is verified:** Users do not engage with multiple posts.  \n  * **If data shows users frequently continue to other articles, the trend is invalidated:** Move to examine another potential issue.\n\n**Outcome:** Suppose you verify the trend: Users often exit after a single blog post.\n\n**Layer 2 (Underlying Reason Identification):**  \nNow generate hypotheses to explain *why* this is happening.\n\n* **Hypothesis A (Content Quality):** The blog posts may not be compelling or may fail to prompt further exploration.  \n  **Reason:** If the content is not meeting user interest, session recordings might show minimal scrolling or no clicks on related article links.  \n  **Act:** Examine user scroll depth, time-on-page, and clicks on internal article recommendations. If you see that users rarely click recommended articles or do not scroll to the recommended section, this might confirm low interest or poor content quality.  \n* **Hypothesis B (Navigation/Layout):** Users do not see or notice related articles or have difficulty navigating to more content.  \n  **Reason:** If related articles are placed too far down or are visually inconspicuous, users might not realize there’s more to explore.  \n  **Act:** Check heatmaps to see if users’ attention drops off before related links are visible. If you find that most users never scroll past a certain point, it supports the idea that layout or placement is an issue.  \n* **Hypothesis C (User Intent):** Users might have come to the blog for a single, quick answer and leave satisfied. They’re not looking to engage further.  \n  **Reason:** If the page is ranking for specific informational queries, users may arrive, read what they need, and exit feeling no need to continue.  \n  **Act:** Compare referral paths and search queries. If visitors come from search engines with very specific queries and leave after getting their answer, you validate this hypothesis.\n\n**Validation/Invalidation:**\n\n* If you discover through recordings that users do scroll and notice recommended articles but still do not click, Hypothesis B (layout) is likely invalid, and Hypothesis A or C becomes more plausible.  \n* If you find that users often come from targeted search queries and show signs of satisfaction (long dwell time on the article they visited), you might validate Hypothesis C, concluding their behavior matches their intent.\n\n**Final Documentation:**\n\n* Verified Trend: Users often leave after one blog post.  \n* Validated Reason (if found): Users have informational intent and simply leave after getting what they needed (Hypothesis C).  \n* Invalidated Reasons: Layout issues (B) may be ruled out if heatmaps show no engagement issue; Content Quality (A) may be invalidated if users do spend ample time reading, indicating the content is sufficiently engaging.\n\n#### **Example 3: Checkout Drop-Off in an E-commerce Funnel**\n\n**Scenario:** You notice a high drop-off rate at the final checkout stage. You suspect that although users add items to the cart, they fail to complete the purchase.\n\n**Layer 1 (Trend Verification):**  \n**Subquestion:** Do users abandon their carts at the final checkout step?\n\n* **Branch 1 Data Observation:** Check session recordings and funnel analytics. Are a significant portion of sessions ending at checkout without completing purchase?  \n  * If yes, the trend is verified: Users are indeed dropping off at checkout.  \n  * If no, the trend is invalidated: Users might not be dropping off at this stage, so investigate another funnel stage.\n\n**Outcome:** Suppose you verify the trend: A significant number of users abandon at the final checkout page.\n\n**Layer 2 (Underlying Reason Identification):**  \nHypothesize why users abandon at checkout.\n\n* **Hypothesis A (Pricing or Fees):** Additional costs (e.g., shipping, taxes) revealed at checkout cause users to leave.  \n  **Reason:** If session recordings show users pausing at the order summary area, maybe they balk at unexpected costs.  \n  **Act:** Check if users hover over pricing details, scroll back up, or repeatedly toggle shipping options. Look for patterns in regions where extra fees differ.  \n* **Hypothesis B (Technical or Performance Issues):** Slow loading times or technical errors at checkout discourage completion.  \n  **Reason:** If heatmaps or recordings show frustration cues (e.g., multiple clicks on the same button, rapid page reloads), it may suggest technical glitches.  \n  **Act:** Check for evidence of lag, error messages in recordings, or abrupt navigation away after a long loading segment.  \n* **Hypothesis C (Trust and Security Concerns):** Users may not trust the checkout process or lack confidence in data security.  \n  **Reason:** If users reach the payment details section and exit immediately, it could indicate lack of trust.  \n  **Act:** See if users hover over trust badges, spend time reading return policies, or try to find security seals. If these elements are missing or users abandon right at payment info entry, this might confirm the hypothesis.  \n* **Hypothesis D (Better Offers Elsewhere):** Users might leave to compare prices or try coupons and never return.  \n  **Reason:** If the session recordings show users opening new tabs or quickly bouncing after viewing the final price, they may be shopping around.  \n  **Act:** Although direct competitor data isn’t always available, you can look for signs like users hovering over coupon fields or abruptly leaving right after viewing the final price.\n\n**Validation/Invalidation:**\n\n* If heatmaps and recordings show that most abandonments happen right after seeing final costs and no technical issues are present, you may validate Hypothesis A.  \n* If recordings show erratic clicking patterns, slow load icons, or partial page loads, you may validate Hypothesis B.  \n* If you never see trust badges inspected and no price increase is detected, you might invalidate Hypothesis C.  \n* If timing data suggests users leave to check elsewhere (harder to confirm directly), you may tentatively consider Hypothesis D but look for more concrete evidence.\n\n**Final Documentation:**\n\n* Verified Trend: Many users abandon at the final checkout step.  \n* Validated Reason (if found): Unexpected additional costs at checkout (Hypothesis A).  \n* Invalidated Reasons: If users do not appear concerned about trust or security (no signs of mistrust), Hypothesis C is invalidated. If no performance issues appear in recordings, Hypothesis B is invalidated. If user intent to compare prices cannot be confirmed, Hypothesis D remains speculative or invalidated due to lack of evidence.\n\n**Starting Examples of Analyst-Style Questions for the Two-Layered Approach (**Reference only, not requirement)**:**\n\n* To verify a trend:  \n  * Do users consistently show the same behavior (exiting after seeing deals) across multiple sessions or user segments?  \n  * Are observed behaviors consistent across different time periods (e.g., weekdays vs. weekends)?  \n  * Does the trend persist across various device types (mobile vs. desktop) or browser types?  \n  * Is there a significant difference in this behavior before and after certain site changes or promotional events?  \n  * Do users exhibit this behavior (e.g., exiting after viewing deals) only when arriving from certain referral sources?  \n* To find underlying reasons:  \n  * Among those users who leave, do they view any specific deal elements first?  \n  * Are there segments (e.g., users who can be classified together by action, behavior, demographic) that engage differently with the deals, hinting at content or layout issues?  \n  * Comparing user behavior on other sections of the site: if those have higher conversion rates, what’s different about them?  \n  * If users are consistently leaving, does their interaction pattern suggest confusion (e.g., multiple rapid clicks, hovering over elements without further action) or disinterest (minimal scrolling, immediate exit)?  \n  * Could external factors (e.g., seasonal shopping preferences, competitor promotions) explain why the trend occurs?  \n  * Are there particular elements within the page—such as images, price badges, discount codes, or unclear CTAs—that correlate with higher exit rates?  \n  * Do user comments, if any are provided, shed light on specific frustrations or unmet expectations within the verified trend areas?  \n  * If we identified content quality as a suspect, can we segment deals by category (e.g., electronics vs. apparel) to see if certain product types fare better or worse?  \n  * If layout is a suspect, are users failing to scroll or notice certain critical content zones? What elements remain unclicked or barely viewed?  \n  * If expectations are mismatched, can we find evidence that users expect different products or deals (based on their previous navigation paths or external referrals)?  \n  * Is there a correlation between how fast the page loads and the user’s subsequent behavior, suggesting performance issues as a cause?  \n  * Consider the use of inductive reasoning:   \n    * Induction by Generalization: Apply learnings from similar users or similar website elements. For instance, if certain parts of the page consistently perform poorly, generalize this insight to similar elements across the site. Alternatively, identify if some trends should be explored on a more granular level.  \n    * Induction by Analogy: Compare performance between similar sections, or between similar sites to identify insights based on analogous patterns. If an analysis on one element of a section like the navigation bar is done, explore the data around other elements too to create comprehensive learnings and actions for the optimizer. If there is a trend in one product page, does it exist in others as well?   \n    * Mathematical Induction: Prove the value of smaller changes by extrapolating their impact to other parts of the site. If one improvement shows a lift in conversions, explain why this can be applied to other elements using the same logic\n\n\nSpecify who should respond next. Here are the 3 options. You must pick one of these options:\n(a) Python Analyst: If there isn't sufficient data when following the above tasks, ask the Python Analyst to fetch any relevant heatmaps (and summaries) / session recordings (and summaries) / aggregate metrics to so you can create data-driven insights. This should be done when stuck.\n(b) Website Insights Finder: If sufficient data-driven insights are not found to answer the question, ask the Website Insights Finder to find more data-driven insights.\n(c) Website Optimizer: If all the tasks are followed to find data-driven insights, ask the Website Optimizer to create a data-driven experiment idea.\n\nHere is a sample output message for each option. Your message must be similar to 1 these examples:\n(a)\n```There isn't sufficient data to create data-driven insights. I need ... Python Analyst, please fetch any relevant heatmaps (and summaries) / session recordings (and summaries) / aggregate metrics so I can create data-driven insights.```\n\n(b)\n```[insights]. These insights can be improved. Website Insights Finder, respond next and please find more data-driven insights.```\n\n(c)\n```[insights]. These insights are sufficient. Website Optimizer, respond next and please create a data-driven experiment idea.```\n\n**Important:** You must replace [insights] with the insights you found.\n\nRemember, the end of your message must specify who should respond next.\n"
    agent = AssistantAgent(name='website_insights_finder', model_client=get_llm_config(1), description='Website Insights Finder who analyzes user data and business context and produces insights', system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True)
    return agent



# File: backend/agents/data_analyst_group/agents/website_optimizer.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import Dict, Any

import json

from utils.functions import is_termination_msg

from dotenv import load_dotenv

import os

def create_website_optimizer(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f"""You are a website and customer experience optimizer. Your role is to transform previously validated data-driven insights into highly specific, actionable, explainable, and credible suggestions. \n    \nHere is the question you need to answer: {question}\nHere is the business context of the site: {business_context}\nAdditionally, {additional_instructions}\n\nYou may analyze the heatmap images given to support suggestions.\n    \nEach suggestion must:\n\n* Be fully supported by inputs, above task 1-3 analysis, online context, data (session recordings, heatmaps)  \n* Heavily tailored to and aligned with the OKRs, focuses, and business uniqueness identified and in the business context quiz.  \n* Directly address a verified problem or opportunity identified from the previous steps (no new speculation).  \n* Not break common guardrail practices within companies in the industry or vertical.   \n* Follow the strict rules below on reasoning, structure, and presentation.\n\nYou will use Tree-of-Thought (CoT) when coming up with suggestions and ReAct reasoning internally to identify which suggestions are most likely to increase the OKR, strongly data-evidenced, and aligned with business context. Do NOT show your reasoning steps in the final output; only present the final suggestions. If a suggestion cannot be fully supported by data or context, omit it.\n\nGenerating Suggestions with Tree of Thought: \n\nFor each insight identified previously, generate numerous potential suggestions aimed at improving the OKR metrics identified in the business context. Each suggestion will be a new branch and must follow each of the coming rules in its creation.  \n\n**[RULES]**\n\n**Rule 1: Data & Context Integrity**\n\n* All suggestions must be fully and blatantly supported by user data (session recordings, heatmaps, analytics). An optimizer looking at the data should see an indisputable tie between the user behavior and the recommendation’s logic.  \n* All suggestions must reference and surface at least one heatmap.   \n* When possible, use / crossreference multiple data sources (multiple heatmaps and session recordings) when formulating and defending a suggestion.   \n* There must be a clear explanation of what informative pattern, trend, or anomalous occurrence identified in the data is. Also an explanation of the exact relation of the data insight to the suggestion.   \n* Omit suggestions where only one session recording (one heatmap is ok) evidences the suggestion.  \n* Do not guess or use unsupported assumptions. No data = no suggestion.\n\n**Rule 2: Surfacing Data and Demonstrating Understanding of Business**\n\n* Explain your data exploration and thought process of the users's journey. Make sure to use compelling storytelling to narrate why users are behaving the way they are, and base this on observed patterns that are qualitatively or quantitatively defensible.   \n* Show off the deep understanding of the business context and uniqueness that were considered in the Why / narrativization behind the suggestion. This can include acknowledgement of the OKRs, unique understanding of the users, etc.  \n* Use exact numbers or percentages from the data if available. Otherwise, provide no guess.  \n* Avoid intangible metrics such as "many users" or "lower engagement." Instead provide exact metrics, if they are available.   \n* Identify all data-sources evidencing a suggestion. Use an identifier like which page or audience segment it’s from. Such as “The homepage’s click heat map on mobile shows…”  \n* When referencing evidence, store the source clearly (e.g., “session_id”: “xxxx-xxxx...”). Do NOT include this in the output.   \n* Provide a clear cause-effect relationship: “By doing X, we solve Y problem observed in data.”\n\n**Rule 3: Granular and Relevant**\n\nSuggestions should provide a **diverse range** of optimizations, beyond generic best practices. Focus on generating **specific, insightful, and creative solutions** that make optimizers think, *“Why didn’t I think of that?”* Each suggestion must be:\n\n* **Insightful and Actionable:** Offering practical steps that are easy to implement within a week.  \n* **Creative and Distinctive:** Propose ideas that go beyond surface-level fixes. Aim for innovative solutions grounded in evidence.  \n* **Varied in Scope:** Include both quick, granular changes and bold, high-impact suggestions—**but only if the data strongly supports them**.  \n* Solving a validated problem identified in the insights.\n\n**Rule 4: Prioritization and Feasibility**\n\n* Indicate potential impact or a range based on industry benchmarks from similar A/B tests. You must get this from the UX researcher using the get_similar_experiments tool.\n* Consider ease of implementation: mostly focus on improvements that can be done within a week.\n\n**Rule 5: Action-Verb and Goal Driven Language**\n\n* There should be clear motivation in the wording of the suggestion that provides a call-to action for the optimizer  \n* Each suggestion must start with a clear, actionable, Present Tense Command Verb, such as "Move," "Change," "Highlight," "Add," etc.  \n* Suggestions should tell optimizers exactly what to change or edit on their site. It should not tell them to run an audit or an AB test, etc.   \n* Provide maximum detail to all visual and structural changes to help users visualize them.   \n* If recommending new text, provide the exact wording, not descriptions of what should be included.\n\n**Rule 6: KPIs or Metrics for Potential Opportunity Sizing**\n\n* Prioritize recommendations by potential impact (a range estimating OKR change or user behavior related), ease of implementation, and confidence. Be specific in ranking (high, medium, low) and give a clear rationale for the ranking.  \n* Include “time to implement” estimates and departmental involvement (e.g., UI design vs backend dev), making prioritization easier for cross-functional teams.  \n* Reference Industry Benchmarks or case studies from similar businesses that have implemented similar changes. (you must ask the ux researcher to use the get_similar_experiments tool to find these).\n\n**Rule 7: Suggestion Format (3 Sentences)**  \nFor each suggestion, use exactly three concise sentences, formatted as follows. They must be in this format to ensure consistency and clarity:\n\nSuggestion [n]:\n1. **(Action)**: ONE sentence (under 30 words) that captures them and drives interest by sharing a brief reason they should care (it can be an estimated impact or projected behavior change). Example: (Add a “xxx” to  capitalize on “xxx” and drive “xxx”). Or (Change “xxx” on “xxx” page to encourage “xxx” and increase “xxx”).   \n2. **(Data Insight)**: Two sentences, the first referencing the exact data points (for heatmaps and session recordings, reference their keys / ids) that justify the suggestion, with identifiers, metrics, and a clear pattern. Clearly explain what informative pattern, trend, or anomalous occurrence is identified in the data. The second explains how the data in sentence one justifies the suggestion, tying the data and recommendation to expected outcomes, possibly referencing analogous situations or known industry benchmarks. \n\neg:\n\nSuggestion 1:\n1. **(Action)**: ...\n2. **(Data Insight)**: ...\n\nSuggestion 2:\n1. **(Action)**: ...\n2. **(Data Insight)**: ...\n\n...\n\n**Final Output:** Produce the outputs for all the rules step by step. Do not skip any steps.\n\n**ReAct for suggestion evaluation:**\n\nEach insight from the previous step should now have a series of branches: each a well formulated, data-driven, and concise suggestion. ReAct must be executed for each branch, to evaluate which branch’s suggestion(s) will be surfaced as the final surfaced solutions to the problem behind the insight.  \n\n**Reasoning:** Each branch must be evaluated based on the following listed criteria, meant to isolate the suggestion most likely to convince the optimizer, be tested, and create OKR lift.\n\n1. Business Context Consistency: Is the suggestion consistent with the business context? Are there any contradictions with the business’s uniqueness, context quiz, or OKR priorities?   \n2. Data Consistency: Is there data evidencing the suggestion? Does the data shown actually demonstrate the trends behind the suggestion? Is the data comprehensive?   \n3. Site Context Consistency: Does a suggestion miss a business or brand nuance that invalidates the suggestion? If a suggestion includes a new addition, is there proof the addition does not already exist? If a suggestion includes a removal, is there proof the suggestion is present currently? If there is any uncertainty and no proof, remove the suggestion.   \n4. Human / UX Context Consistency: Consider the cultural, legal, emotional, or practical nuances to the implementation of the suggestions. Are these suggestions likely to be accepted well?   \n5. Is the suggestion written actionably? Does it recommend vague next steps like running audits or A/B tests to learn more?    \n6. Feasibility: What’s your confidence in the suggestion’s likelihood of actually being tested? Is the suggestion granular, feasible, and convincing? Is the suggestion a large site change that they wouldn’t realistically implement? Do they have the resources to implement this suggestion?   \n7. Success Likelihood: What is your confidence in the suggestion’s ability to make positive change? Is there a high likelihood the suggestion tests well?  \n\n**Action:** Evaluate each suggestion. For each insight, choose a final suggestion based on the above criteria. Output that suggestion in full, including all 3 parts. Final Output: A series of suggestions that have been chosen as the best solutions of their insight branches.\n\nOther Notes:\n1. Know the page you are trying to optimize. You must ask the UX researcher to get a screenshot of the url of that page to better understand it.\n2. Ensure the suggestion actually applies to the page your are optimizing. If it does not, do not include it.\n3. Ensure in the 3 sentence structure, you include actual data ids for heatmaps and session recordings that support the suggestion. You must have these ids to support the suggestion so the user can see what data was used to make the suggestion.\n\nYou must plan the next best action. There is no user to give additional input, use the feedback ot iteratively improve suggestions until they can really 'cut diamonds' while ensuring they are:\n1. convincing: do all points in suggestion make sense, is it relevant to current business focus\n2. data-driven: high quality of data insights (ask website insights finder for more segmented insights if needed)\n3. novel: different, harder for a human analyst to ideate\n\nSpecify who should respond next. Here are the 3 options. You must pick one of these options:\n(a) Website Insights Finder: If the insights are not clear or don't clearly lead to suggestions, ask the Websights Insights Finder to provide more insights.\n(b) UX Researcher: If the suggestions can be benefitted by best UX practices, ask the UX Researcher to find UX research to improve the suggestions.\n(c) Website Optimizer: If you are still working on the suggestions, ask the Website Optimizer to finish the suggestions.\n(d) Website Optimizer Full: If the suggestions are great and ready to be reviewed, ask the Website Optimizer Full to review the suggestions.\n\n**Important**: Regardless of who responds next, you must output your step by step process with the rules and where/if you stopped. You must replace [suggestion steps] with the steps you took to generate the suggestions. Do not use placeholders:\n\nHere is a sample output message for each option. Your message must be similar to 1 these examples:\n(a)\n```[suggestion steps]. The insights are not clear or don't clearly lead to suggestions. I want to insights that ... Website Insights Finder, please provide more insights.```\n\n(b)\n```[suggestion steps]. The suggestions can be benefitted by best UX practices. UX Researcher, please find UX research (screenshots, benchmarks, research) about ... to improve the suggestions.```\n\n(c)\n```[suggestion steps]. I am still working on the suggestions. Website Optimizer, please finish the suggestions.```\n\n(d)\n```[suggestion steps]. The suggestions are great and ready to be reviewed. Website Optimizer Full, please review the suggestions.```\n\nRemember, the end of your message must specify who should respond next.\n"""
    agent = AssistantAgent(name='website_optimizer', model_client=get_llm_config(1), description='Website and customer experience optimizer who transforms data-driven insights into highly specific, actionable, explainable, and credible suggestions.', system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True)
    return agent



# File: backend/agents/data_analyst_group/agents/website_optimizer_full.py

from autogen_agentchat.agents import AssistantAgent

from config.config import Config

from typing import Dict, Any

import json

from utils.functions import is_termination_msg

from dotenv import load_dotenv

import os

from tools.get_similar_experiments import get_similar_experiments

from prompts.suggestion_prompts import suggestion_example, suggestion_notes

def create_website_optimizer_full(get_llm_config, question, business_context, stream_key, functions=[], tools=None, handoffs=None, additional_instructions=''):
    system_message = f'You are a website and customer experience optimizer. Your role is to transform previously validated data-driven insights into highly specific, actionable, explainable, and credible suggestions. \n\nHere is the question you need to answer: {question}\nHere is the business context of the site: {business_context}\nAdditionally, {additional_instructions}\n\nFor each suggestion, create an output in this format (this includes suggestions from website optimizer and suggestions user proxy that need to be improved):\n```json\n{suggestion_example}\n```\n\nYou must use the same json format as the example. Do not use markdown or any other formatting. You must use JSON with the same keys, ensuring it uses real data.\n\nTake these notes into account when creating suggestions:\n{suggestion_notes}\n'
    agent = AssistantAgent(name='website_optimizer_full', model_client=get_llm_config(1), description='Website and customer experience optimizer who transforms 3 sentence suggestions into full suggestions. Acts as an intermediary suggestions user proxy.', system_message=system_message, handoffs=handoffs, tools=tools, reflect_on_tool_use=True)
    return agent



# File: backend/agents/data_analyst_group/app.py

import os

import json

import logging

from dotenv import load_dotenv

from pathlib import Path

from fastapi import FastAPI, HTTPException

from pydantic import BaseModel

from typing import Optional, Dict, Any

from src.group_chat import analyze_problem

from utils.functions import initialize_env, get_data

import asyncio

import weave

import traceback

from datetime import datetime

from boto3.dynamodb.conditions import Key

import argparse

@weave.op
def score(model_output):
    print(model_output)
    return {**model_output['evaluation_record'], 'summary': model_output['summary'], 'conversation': model_output['conversation']}



# File: backend/agents/data_analyst_group/config/config.py

import os

from dotenv import load_dotenv

from utils.functions import get_api_key

@staticmethod
def get_secret_name(is_local: bool) -> str:
    return Config.LOCAL_SECRET_NAME if is_local else Config.PROD_SECRET_NAME



# File: backend/agents/data_analyst_group/models/coordinates.py



# File: backend/agents/data_analyst_group/models/models.py

from pydantic import BaseModel

from typing import List



# File: backend/agents/data_analyst_group/prompts/agent_instructions.py

import sys

import os

from prompts.insights_prompts import insight_notes

from prompts.okr_prompts import all_okr_prompts, reach_example

import weave

from datetime import datetime, timedelta

def custom_agent_instructions(stream_key):
    start_time = int((datetime.now() - timedelta(days=7)).timestamp() * 1000)
    end_time = int(datetime.now().timestamp() * 1000)
    python_analyst_instructions = f'Notes on finding an insight:\n\n{insight_notes}\n\nNote that functions like store_insight, etc are held by the other analysts, so if you import them, it will lead to others, so absolutely do not import and use any other functions than the ones defined\n\nIn queries, get data from the past week, between start_time: {start_time} and end_time: {end_time}, which we got by:\nstart_time = int((datetime.now() - timedelta(days=7)).timestamp() * 1000)\nend_time = int(datetime.now().timestamp() * 1000)\n\nAsk the code execution agent to run the python code blocks. You must get all the calculations for the insight.\n\nAfter the code execution agent returns the results, do exactly one of the following:\n1. If the code fails, fix it and run again. \n2. If no OKRs are printed or the OKR you are tracking is 0 or suspicious, you must abandon trying to track it and print the result of getting a new OKR.'
    python_analyst_interpreter_instructions = f'Notes on finding an insight:\n\n{insight_notes}\n\nNote that functions like store_insight, etc are held by the other analysts, so if you import them, it will lead to others, so absolutely do not import and use any other functions than the ones defined.\n\nIn queries, we must get data from the past week, between start_time: {start_time} and end_time: {end_time}, which we got by:\nstart_time = int((datetime.now() - timedelta(days=7)).timestamp() * 1000)\nend_time = int(datetime.now().timestamp() * 1000)\n\nIf you continually get that the OKR you are tracking is 0 or suspicious, you must abandon trying to track it and ask the research analyst for another OKR to track. End this response with TERMINATE.\nIf the results answer the question and can be used to store an insights by the insights analyst, end the response with TERMINATE.'
    okr_python_analyst_instructions = f"You must get working OKR tracking code:\n\n{all_okr_prompts(stream_key)}\n\nEnsure there is a week's worth of data in each output, if necessary pad with 0s where the output is 0.\n\nIMPORTANT: Only use the database tables you have access to. Do not attempt to store the okr. \nOnly the okr_store_agent has the ability to do this. When your code is verified to work, it can be passed to the okr_store_agent to store it."
    okr_python_analyst_interpreter_instructions = f"You must get working OKR tracking code:\n\n{all_okr_prompts(stream_key)}\n\nEnsure there is a week's worth of data in each output, if necessary pad with 0s where the output is 0.\n\nIMPORTANT: Only use the database tables you have access to. Do not attempt to store the okr. \nOnly the okr_store_agent has the ability to do this. When your code is verified to work, it can be passed to the okr_store_agent to store it."
    return {'python_analyst': python_analyst_instructions, 'okr_python_analyst': okr_python_analyst_instructions, 'python_analyst_interpreter': python_analyst_interpreter_instructions, 'okr_python_analyst_interpreter': okr_python_analyst_interpreter_instructions}



# File: backend/agents/data_analyst_group/prompts/code_prompts.py



# File: backend/agents/data_analyst_group/prompts/design_prompts.py



# File: backend/agents/data_analyst_group/prompts/insights_prompts.py

import weave

from utils.functions import initialize_env



# File: backend/agents/data_analyst_group/prompts/okr_prompts.py

def okr_code_example(stream_key):
    return f'''\n# you must use these exact imports in your code, you cannot add, remove, or change any imports\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom datetime import timedelta\nfrom functions import run_sitewiz_query\nfrom typing import TypedDict, List, Tuple\n\nclass MetricOutput(TypedDict):\n    Metric: str\n    Description: str\n    start_date: str\n    end_date: str\n    values: List[Tuple[str, float]]\n\nstream_key = '{stream_key}'  # THIS MUST BE DEFINED AND USED IN THE QUERIES\n\n# Get yesterday's date as end_date\nend_date = (datetime.datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")\n# Get date 6 days before end_date as start_date\nstart_date = (datetime.datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")\n\ndef calculate_metrics(start_date: str, end_date: str) -> MetricOutput:  # do not change this function signature or MetricOutput\n    # Calculate daily signup conversion rate as (signup visits / total sessions)\n    # For signup visits, join funnels with session_recordings to use the materialized date column.\n    sql_signup = f"""\n    SELECT \n        sr.date AS date,\n        COUNT(DISTINCT f.session_id) AS signup_visits\n    FROM funnels f\n    JOIN session_recordings sr ON f.session_id = sr.session_id\n    WHERE f.base_url = '.../signup'\n      AND sr.stream_key = '{{stream_key}}'\n      AND sr.date BETWEEN '{{start_date}}' AND '{{end_date}}'\n    GROUP BY sr.date\n    ORDER BY sr.date;\n    """\n    results_signup = run_sitewiz_query(sql_signup)\n\n    # Total sessions are obtained from sessions joined with session_recordings,\n    # using the materialized date column (sr.date) as the main timestamp reference.\n    sql_total = f"""\n    SELECT \n        sr.date AS date,\n        COUNT(DISTINCT s.session_id) AS total_sessions\n    FROM sessions s\n    JOIN session_recordings sr ON s.session_id = sr.session_id\n    WHERE sr.stream_key = '{{stream_key}}'\n      AND sr.date BETWEEN '{{start_date}}' AND '{{end_date}}'\n    GROUP BY sr.date\n    ORDER BY sr.date;\n    """\n    results_total = run_sitewiz_query(sql_total)\n\n    # Convert query results to dictionaries for lookup by date\n    signup_dict = {{row[0]: row[1] for row in results_signup}}\n    total_dict = {{row[0]: row[1] for row in results_total}}\n\n    # Build a list of dates between start_date and end_date (inclusive)\n    date_range = pd.date_range(start=start_date, end=end_date)\n    values = []\n    for dt in date_range:\n        date_str = dt.strftime("%Y-%m-%d")\n        signup_count = signup_dict.get(date_str, 0)\n        total_count = total_dict.get(date_str, 0)\n        conversion_rate = signup_count / total_count if total_count > 0 else 0.0\n        values.append((date_str, conversion_rate))\n\n    return {{\n        "Metric": "signup_conversion",\n        "Description": "Daily signup conversion rate calculated as signup visits from funnels (exact match on '.../signup') over total sessions from sessions, grouped by the materialized date.",\n        "start_date": start_date,\n        "end_date": end_date,\n        "values": values\n    }}\n\n# print results for testing\nprint(calculate_metrics(start_date, end_date))\n'''

def reach_example(stream_key):
    return f'''\n# you must use these exact imports in your code\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom datetime import timedelta\nfrom functions import run_sitewiz_query \nfrom typing import TypedDict, List, Tuple\n\nclass ReachOutput(TypedDict):\n    Description: str\n    start_date: str\n    end_date: str\n    values: List[Tuple[str, float]]\n\nstream_key = '{stream_key}'  # THIS MUST BE DEFINED AND USED IN THE QUERIES\n\n# Get yesterday's date as end_date\nend_date = (datetime.datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")\n# Get date 6 days before end_date as start_date\nstart_date = (datetime.datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")\n\ndef calculate_reach(start_date: str, end_date: str) -> ReachOutput:   # do not change this function signature or ReachOutput\n    # Calculate total sessions per day using the materialized date column in session_recordings.\n    sql = f"""\n    SELECT \n        sr.date AS date,\n        COUNT(DISTINCT s.session_id) AS total_sessions\n    FROM sessions s\n    JOIN session_recordings sr ON s.session_id = sr.session_id\n    WHERE s.stream_key = '{{stream_key}}'\n      AND sr.date BETWEEN '{{start_date}}' AND '{{end_date}}'\n    GROUP BY sr.date\n    ORDER BY sr.date;\n    """\n    results = run_sitewiz_query(sql)\n    \n    # Convert query results to a dictionary for lookup by date\n    reach_dict = {{ row[0]: row[1] for row in results }}\n    \n    # Build a list of dates between start_date and end_date (inclusive)\n    date_range = pd.date_range(start=start_date, end=end_date)\n    values = []\n    for dt in date_range:\n        date_str = dt.strftime("%Y-%m-%d")\n        total_sessions = reach_dict.get(date_str, 0)\n        values.append((date_str, total_sessions))\n    \n    return {{\n        "Description": "Daily total sessions grouped by date from sessions joined with session_recordings.",\n        "start_date": start_date,\n        "end_date": end_date,\n        "values": values\n    }}\n\noutput = calculate_reach(start_date, end_date)\nprint("Calculate Reach Output:")\nprint(output)\n'''

def all_okr_prompts(stream_key):
    return f'OKR Requirements:\n{okr_criteria}\n\nExample Implementation:\n{okr_code_example(stream_key)}\n\nBest Practices:\n{okr_notes}\n\nWe also need to calculate the reach for each OKR in terms of number of sessions we are tracking the OKR for. Example:\n{reach_example(stream_key)}\n\nIMPORTANT: If the function names or imports are different from the examples, then the code will not compile and execution will fail when storing the OKR.\nPlease ensure the imports and function names and output formats are exactly the same.'



# File: backend/agents/data_analyst_group/prompts/society_instructions.py

from tools.get_okr import get_okr_given_context

from utils.functions import save_results, suggestion_to_markdown, fetch_results, process_data_statement, get_dynamodb_table

import json

from decimal import Decimal

import weave

def get_insights_analyst_group_instructions(stream_key):
    insights_group_instructions = 'TASK: Collect the python code and results to store an insight.'
    okrs, got_okrs = get_okr_given_context(stream_key)()
    if got_okrs:
        insights_group_instructions += f'\n\nUse these OKRs and tracking code to find insights relevant to improving the OKRs of the business. You must pick one of these and find smaller OKRs that affect these:\n{okrs}'
    return insights_group_instructions

def get_okr_group_instructions(stream_key):
    okr_group_instructions = 'TASK: Find OKR tracking code based on the business context. You must guide the analyst team'
    okrs, got_okrs = get_okr_given_context(stream_key)()
    if got_okrs:
        okr_group_instructions += f'These are previous OKRs and tracking code. Use them as reference and find seperate OKRs that are relevant to the business\n{okrs}'
    return okr_group_instructions

def process_options(stream_key, options):
    insights_str = ''
    suggestions_str = ''
    if 'InsightConnectionTimestamp' in options:
        insights_table = get_dynamodb_table('website-insights')
        timestamp = options['InsightConnectionTimestamp']
        insights_data = insights_table.get_item(Key={'streamKey': stream_key, 'timestamp': timestamp})
        insights_data = insights_data['Item']
        data_statement = insights_data['data_statement']
        variables = json.loads(insights_data['variables'])
        derivation = json.loads(insights_data['derivation'])
        cleaned_data_statement = process_data_statement(data_statement, variables, derivation, [])
        insights_str = f"\nInsight at InsightConnectionTimestamp '{str(timestamp)}' (use this same exact timestamp as InsightConnectionTimestamp when storing suggestion):\nCleaned data statement: {cleaned_data_statement}\nRaw values: {json.dumps(insights_data, indent=4, cls=DecimalEncoder)}"
        save_results('insights', insights_str)
    if 'SuggestionTimestamp' in options:
        suggestions_table = get_dynamodb_table('WebsiteReports')
        timestamp = options['SuggestionTimestamp']
        suggestions_data = suggestions_table.get_item(Key={'streamKey': stream_key, 'timestamp': timestamp})
        print('suggestions data')
        print(suggestions_data)
        suggestions_data = suggestions_data['Item']
        suggestions_str = f'\nThe suggestion with timestamp {timestamp} (use this same exact timestamp when coding up the suggestion):\n\n{suggestion_to_markdown(suggestions_data)}'
        print(suggestions_str)
        save_results('suggestions', suggestions_str)
    return (insights_str, suggestions_str)

def get_society_instructions(stream_key, options):
    """Get society of mind instructions for each group."""
    insights_str, suggestions_str = process_options(stream_key, options)
    return {'insights_analyst': {'description': 'Insights Analyst who finds insights from the python analyst output and stores them in the database.', 'start_message': get_insights_analyst_group_instructions(stream_key), 'response_prompt': 'Output with the exact responses by the insights_user_proxy. If store_insight failed each time, also output what went wrong and that the python analyst or research analyst needs to fix these issues.'}, 'suggestions_analyst': {'description': 'Suggestions analyst who creates and stores suggestions', 'start_message': suggestions_analyst_group_instructions, 'response_prompt': 'Ouptut with teh exact response by the suggestions analyst and suggestions user proxy'}, 'okr_python_analyst': {'description': 'Python Analyst who writes and executes Python code to retrieve and analyze data using provided functions (can perform analytics calculations, segmentations, and xPath calculations).', 'start_message': okr_python_group_instructions, 'response_prompt': "Output the exact code executed and key results of the code execution output. Don't omit any details."}, 'code_store_group': {'description': 'Website developer who stores websites', 'start_message': 'Your task is to run store_website if the code is complete and verified with a screenshot', 'response_prompt': 'If the website is stored, return the code url and related fields. If not, explain why'}, 'python_analyst': {'description': 'Python Analyst who writes and executes Python code to retrieve and analyze data using provided functions (can perform analytics calculations, segmentations, and xPath calculations).', 'start_message': python_group_instructions, 'response_prompt': "Output the exact code executed and key results of the code execution output. Don't omit any details."}, 'okr_store': {'description': 'OKR storer which properly stores the OKR. They must store the OKR once the information is there.', 'start_message': okr_store_group_instructions, 'response_prompt': 'Output whether the OKR is stored properly'}, 'okr': {'description': 'OKR group that finds and tracks OKRs relevant to a business', 'start_message': get_okr_group_instructions(stream_key), 'response_prompt': 'What are the stored OKRs?'}, 'insights': {'description': 'Insights analyst with an inner mind of a python analyst, research analyst, insights analyst, and behavioral analyst who creates and stores insights', 'start_message': insights_group_instructions, 'response_prompt': 'For stored insights, return all of them with their numeric timestamps. If there is no timestamp, assume the insight is not stored. If insights are not stored, explain why'}, 'suggestions': {'description': 'Suggestions analyst who creates and stores suggestions', 'start_message': suggestions_group_instructions + insights_str, 'response_prompt': 'Output the exact response by the suggestions analyst and suggestions user proxy.'}, 'guardrails': {'description': 'Guardrails agent who verifies suggestion meets business guardrails', 'start_message': guardrails_group_instructions + suggestions_str, 'response_prompt': 'For every suggestion, answer if the guardrails verifies it meets the business guardrails criteria'}, 'coding': {'description': 'Website developer who creates and stores websites', 'start_message': coding_group_instructions + suggestions_str, 'response_prompt': 'If the website is stored, return the code url and related fields. If not, explain why'}, 'dummy': {'description': 'Dummy group for testing if suggestion works', 'start_message': 'Create a random suggestion', 'response_prompt': 'test'}}

def default(self, obj):
    if isinstance(obj, Decimal):
        return str(obj)
    return super(DecimalEncoder, self).default(obj)



# File: backend/agents/data_analyst_group/prompts/suggestion_prompts.py



# File: backend/agents/data_analyst_group/src/create_group_chat.py

from pathlib import Path

from typing import Dict, Any, Sequence

import tempfile

import traceback

from decimal import Decimal

import ast

from autogen_agentchat.agents import CodeExecutorAgent

from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination

from autogen_agentchat.messages import AgentEvent, ChatMessage

from autogen_agentchat.teams import MagenticOneGroupChat, SelectorGroupChat, RoundRobinGroupChat

from autogen_core.tools import FunctionTool

from prompts.agent_instructions import custom_agent_instructions

from prompts.society_instructions import get_society_instructions

from utils.website_data_utils import update_website_data, get_website_data

from utils.functions import run_completion_with_fallback, process_messages_success, convert_message_to_dict, create_executor, initialize_env

from tools.run_sitewiz_query import run_sitewiz_query, run_sitewiz_query_description

from tools.store_okr import store_okr_given_context

from tools.get_okr import get_okr_given_context

from tools.store_insight import store_insight_given_context

from tools.get_website import get_website_given_context

from tools.store_website import store_website_given_context

from tools.str_replace_editor import str_replace_editor_given_context

from tools.website_screenshot import get_screenshot_given_context

from tools.firecrawl_tool import firecrawl_tool

from tools.get_screenshot import get_screenshot

from tools.tavily_search import tavily_search

from tools.get_similar_experiments import get_similar_experiments

from tools.get_heatmap import get_heatmap_given_context

from tools.get_element import get_element

from tools.get_similar_session_recordings import get_similar_session_recordings_given_context

from tools.get_session_recording_videos import get_session_recording_videos_given_context

from tools.get_top_pages import get_top_pages_given_context

from tools.store_suggestion import store_suggestion_given_context

from tools.get_stored_suggestions import get_stored_suggestions_given_context

from tools.get_previous_suggestions_summary import get_previous_suggestions_summary_given_context

from tools.store_tracking import store_tracking_given_context, remove_tracking_given_context

from agents.SocietyOfMindAgent import SocietyOfMindAgent

from agents.web_surfer.agent import create_web_agent

from agents.website_developer import create_website_developer

from agents.website_get_save import create_website_get_save

from agents.behavioral_analyst import create_behavioral_analyst

from agents.python_analyst import create_python_analyst

from agents.python_analyst_interpreter import create_python_analyst_interpreter

from agents.research_analyst import create_research_analyst

from agents.okr_research_agent import create_okr_research_agent

from agents.okr_creator_agent import create_okr_creator_agent

from agents.okr_store_agent import create_okr_store_agent

from agents.insights_analyst import create_insights_analyst

from agents.ux_researcher import create_ux_researcher

from agents.suggestions_analyst import create_suggestions_analyst

from agents.insights_analyst_code import create_insights_analyst_code

from agents.insights_user_proxy import create_insights_user_proxy

from agents.suggestions_user_proxy import create_suggestions_user_proxy

from agents.tracking_agent import create_tracking_agent

import json

def update_group_notes(group: str, stream_key: str, task_result):
    """Update notes for a group based on chat messages and previous notes."""
    data = get_website_data(stream_key)
    previous_notes = data.get(f'{group}-notes', '')
    chat_messages = task_result.messages
    chat_messages = [convert_message_to_dict(message) for message in chat_messages]
    chat_messages = list(filter(None, chat_messages))
    prompt = f"Previous notes for {group}:\n{previous_notes}\n\nNew chat history:\n```json\n{json.dumps(chat_messages, indent=4)}\n```\n\nUpdate the notes to include what worked and didn't work.\nFocus on information that could help future conversations, focus on what to avoid to have success in storing data. \n\nOnly include information that is relevant to the task - specifically with additional instructions to each agent to increase their success in storing data. Do not store information outside of this task.\n\nThe notes should be comprehensive so the agent is constantly learning how to properly store the data.\n"
    new_notes = run_completion_with_fallback(prompt=prompt, models=['reasoning-mini', 'main'])
    update_website_data(stream_key, {f'{group}-notes': new_notes})
    return new_notes

def get_recommendations(group: str, stream_key: str, messages, summary) -> str:
    chat_messages = [convert_message_to_dict(message) for message in messages]
    chat_messages = list(filter(None, chat_messages))
    prompt = f"Your role is to provide recommendations to improve an agentic workflow using it's chat history and tool execution notes.\n    \nNew chat history:\n```json\n{json.dumps(chat_messages, indent=4)}\n\nExecution notes: {summary}\n\nProvide actionable recommendations with what to fix"
    recommendations = run_completion_with_fallback(prompt=prompt, models=['deepseek', 'reasoning-mini', 'main'])
    print('Recommendations: ', recommendations)
    return recommendations

def evaluate_chat(task_result, stream_key: str, group: str) -> str:
    try:
        messages = task_result.messages
        group_to_tool = {'okr': 'store_okr', 'insights': 'store_insight', 'suggestions': 'store_suggestion', 'code': 'store_website', 'dummy': 'store_suggestion'}
        if group not in group_to_tool:
            group = 'dummy'
        tool_name = group_to_tool[group]
        print('evaluating # of', tool_name)
        eval_data = {group: {'stored': False, 'attempts': 0, 'successes': 0, 'failures': 0, 'failure_reasons': [], 'success_outputs': [], 'num_turns': 0}}
        eval_group = eval_data[group]
        stats, summary = process_messages_success(messages, tool_name, eval_group, group)
        print(summary)
        recommendations = get_recommendations(group, stream_key, messages, summary)
        evaluation_record = {'group': group, 'attempts': stats['attempts'], 'successes': stats['successes'], 'failure_reasons': stats['failure_reasons'], 'summary': summary, 'recommendations': recommendations, 'stream_key': stream_key, 'num_turns': stats['num_turns']}
        print('evaluation record')
        print(evaluation_record)
        return evaluation_record
    except Exception as e:
        print(e)
        traceback.print_exc()
        return None

def create_groupchat(question: str, business_context: str, stream_key: str, use_functions: dict, agent_instructions: dict, options: dict):
    """Set up autogen agents"""
    agent_instructions.update(custom_agent_instructions(stream_key))
    functions = []
    function_description_functions = []
    function_classes = {'run_sitewiz_query': [run_sitewiz_query, run_sitewiz_query_description]}
    for function_name, extra_parameters in use_functions.items():
        if function_name in function_classes:
            functions.append(function_classes[function_name][0])
            function_description_functions.append([function_classes[function_name][1], extra_parameters])
    executor, functions_module, temp_dir_functions = create_executor(functions)
    code_executor_agent = CodeExecutorAgent('code_executor', code_executor=executor)
    function_descriptions = []
    for function_description_function in function_description_functions:
        function_descriptions.append(function_description_function[0](functions_module, function_description_function[1]))
    temp_dir = tempfile.mkdtemp()
    website_workspace = Path(temp_dir)
    tools = {'store_insight': {'name': 'store_insight', 'description': 'Store the insight in the database.', 'function': store_insight_given_context(stream_key, executor)}, 'get_website': {'name': 'get_website', 'description': 'Get the website data.', 'function': get_website_given_context(website_workspace, stream_key)}, 'store_website': {'name': 'store_website', 'description': 'Save the website data.', 'function': store_website_given_context(website_workspace, stream_key)}, 'str_replace_editor': {'name': 'str_replace_editor', 'description': 'Edit website files.', 'function': str_replace_editor_given_context(website_workspace, stream_key)}, 'website_screenshot': {'name': 'website_screenshot', 'description': 'Get website screenshot.', 'function': get_screenshot_given_context(website_workspace, stream_key)}, 'firecrawl_tool': {'name': 'firecrawl_tool', 'description': 'Scrape website content.', 'function': firecrawl_tool}, 'store_okr': {'name': 'store_okr', 'description': 'Store OKR tracking code.', 'function': store_okr_given_context(stream_key, executor, business_context)}, 'get_okr': {'name': 'get_okr', 'description': 'Get OKR tracking code.', 'function': get_okr_given_context(stream_key)}, 'get_screenshot': {'name': 'get_screenshot', 'description': 'Get screenshot of URL.', 'function': get_screenshot}, 'tavily_search': {'name': 'tavily_search', 'description': 'Search for UX research.', 'function': tavily_search}, 'get_similar_experiments': {'name': 'get_similar_experiments', 'description': 'Get similar experiments.', 'function': get_similar_experiments}, 'get_heatmap': {'name': 'get_heatmap', 'description': 'Get heatmap data.', 'function': get_heatmap_given_context(stream_key)}, 'get_element': {'name': 'get_element', 'description': 'Get element by xpath.', 'function': get_element}, 'get_similar_session_recordings': {'name': 'get_similar_session_recordings', 'description': 'Get similar session recordings.', 'function': get_similar_session_recordings_given_context(stream_key)}, 'get_session_recording_videos': {'name': 'get_session_recording_videos', 'description': 'Get session recording videos.', 'function': get_session_recording_videos_given_context(stream_key)}, 'get_top_pages': {'name': 'get_top_pages', 'description': 'Get top pages.', 'function': get_top_pages_given_context(stream_key)}, 'store_suggestion': {'name': 'store_suggestion', 'description': 'Store suggestion.', 'function': store_suggestion_given_context(business_context, stream_key)}, 'get_stored_suggestions': {'name': 'get_stored_suggestions', 'description': 'Get stored suggestions.', 'function': get_stored_suggestions_given_context(stream_key)}, 'get_previous_suggestions_summary': {'name': 'get_previous_suggestions_summary', 'description': 'Get previous suggestions summary.', 'function': get_previous_suggestions_summary_given_context(stream_key)}, 'store_tracking': {'name': 'store_tracking', 'description': 'Store tracking code for a suggestion.', 'function': store_tracking_given_context(stream_key, executor)}, 'remove_tracking': {'name': 'remove_tracking', 'description': 'Remove tracking code for a suggestion.', 'function': remove_tracking_given_context(stream_key)}}
    agent_config = {'behavioral_analyst': {'create_func': create_behavioral_analyst, 'tools': [tools['get_heatmap'], tools['get_element'], tools['get_similar_session_recordings'], tools['get_session_recording_videos'], tools['get_top_pages']]}, 'web_agent': {'create_func': create_web_agent, 'tools': []}, 'python_analyst': {'create_func': create_python_analyst, 'tools': []}, 'okr_python_analyst': {'create_func': create_python_analyst, 'tools': []}, 'okr_research_agent': {'create_func': create_okr_research_agent, 'tools': []}, 'okr_creator_agent': {'create_func': create_okr_creator_agent, 'tools': []}, 'okr_store_agent': {'create_func': create_okr_store_agent, 'tools': [tools['store_okr']]}, 'tracking_agent': {'create_func': create_tracking_agent, 'tools': [tools['store_tracking'], tools['remove_tracking']]}, 'python_analyst_interpreter': {'create_func': create_python_analyst_interpreter, 'tools': []}, 'okr_python_analyst_interpreter': {'create_func': create_python_analyst_interpreter, 'tools': []}, 'insights_analyst': {'create_func': create_insights_analyst, 'tools': []}, 'insights_behavioral_analyst': {'create_func': create_behavioral_analyst, 'tools': [tools['get_heatmap'], tools['get_element'], tools['get_top_pages']]}, 'insights_analyst_code': {'create_func': create_insights_analyst_code, 'tools': []}, 'insights_user_proxy': {'create_func': create_insights_user_proxy, 'tools': [tools['store_insight']]}, 'research_analyst': {'create_func': create_research_analyst, 'tools': []}, 'ux_researcher': {'create_func': create_ux_researcher, 'tools': [tools['get_screenshot'], tools['tavily_search'], tools['get_similar_experiments']]}, 'suggestions_analyst': {'create_func': create_suggestions_analyst, 'tools': []}, 'suggestions_user_proxy': {'create_func': create_suggestions_user_proxy, 'tools': [tools['store_suggestion']]}, 'website_developer': {'create_func': create_website_developer, 'tools': [tools['get_website'], tools['str_replace_editor'], tools['website_screenshot']]}, 'website_get_save': {'create_func': create_website_get_save, 'tools': [tools['store_website']]}}
    data = get_website_data(stream_key)
    insights_notes = data.get('insights-notes', '')
    suggestions_notes = data.get('suggestions-notes', '')
    coding_notes = data.get('coding-notes', '')
    society_instructions = get_society_instructions(stream_key, options)
    if insights_notes:
        society_instructions['insights']['start_message'] += f'\n\nPrevious group notes:\n{insights_notes}'
    if suggestions_notes:
        society_instructions['suggestions']['start_message'] += f'\n\nPrevious group notes:\n{suggestions_notes}'
    if coding_notes:
        society_instructions['coding']['start_message'] += f'\n\nPrevious group notes:\n{coding_notes}'
    agent_args = (get_llm_config, question, business_context, stream_key)
    agents = {}
    for agent_name, config in agent_config.items():
        agent_tools = [FunctionTool(tool['function'], tool['description'], tool['name']) for tool in config['tools']] if config['tools'] else None
        agents[agent_name] = config['create_func'](*agent_args, functions=function_descriptions, tools=agent_tools, handoffs=None, additional_instructions=agent_instructions.get(agent_name, ''))

    def python_selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:
        """Selector function for Python analyst group chats"""
        message = messages[-1]
        transitions = {'python_analyst': 'code_executor', 'code_executor': 'python_analyst_interpreter', 'python_analyst_interpreter': 'python_analyst'}
        start = 'python_analyst'
        try:
            if message.source == 'code_executor':
                if 'Traceback' in message.content or 'errors' in message.content:
                    return 'python_analyst'
            if message.source == 'python_analyst_interpreter':
                return 'python_analysts'
            return transitions.get(message.source, start)
        except:
            return start
    okr_python_analyst_group = SelectorGroupChat([agents['okr_python_analyst'], code_executor_agent, agents['okr_python_analyst_interpreter']], termination_condition=MaxMessageTermination(20) | TextMentionTermination('TERMINATE'), selector_func=python_selector_func, model_client=get_llm_config(0.5, 'main'))
    python_analyst_group = SelectorGroupChat([agents['python_analyst'], code_executor_agent, agents['python_analyst_interpreter']], termination_condition=MaxMessageTermination(20) | TextMentionTermination('TERMINATE'), selector_func=python_selector_func, model_client=get_llm_config(0.5, 'main'))
    okr_python_analyst_group_agent = SocietyOfMindAgent('python_analysts', okr_python_analyst_group, get_llm_config(0.5, 'main'), **society_instructions['okr_python_analyst'])
    python_analyst_group_agent = SocietyOfMindAgent('python_analysts', python_analyst_group, get_llm_config(0.5, 'main'), **society_instructions['python_analyst'])
    insights_analyst_group = RoundRobinGroupChat([agents['insights_analyst'], agents['insights_analyst_code'], agents['insights_user_proxy']], termination_condition=MaxMessageTermination(10) | TextMentionTermination('TERMINATE'))
    insights_analyst_group_agent = SocietyOfMindAgent('insights_analyst', insights_analyst_group, get_llm_config(0.5, 'main'), verify_function='store_insight', **society_instructions['insights_analyst'])
    suggestions_analyst_group = RoundRobinGroupChat([agents['suggestions_analyst'], agents['suggestions_user_proxy']], max_turns=2)
    suggestions_analyst_group_agent = SocietyOfMindAgent('suggestions_analyst', suggestions_analyst_group, get_llm_config(0.5, 'main'), verify_function='store_suggestion', **society_instructions['suggestions_analyst'])
    insights_group_chat = MagenticOneGroupChat([agents['research_analyst'], python_analyst_group_agent, insights_analyst_group_agent, agents['insights_behavioral_analyst']], model_client=get_llm_config(0, 'deepseek'), termination_condition=MaxMessageTermination(250))
    insights_group_chat_manager = SocietyOfMindAgent('insights_analyst', insights_group_chat, get_llm_config(0, 'deepseek'), **society_instructions['insights'])
    suggestions_group_chat = MagenticOneGroupChat([suggestions_analyst_group_agent, agents['behavioral_analyst'], agents['ux_researcher']], model_client=get_llm_config(0, 'deepseek'), termination_condition=MaxMessageTermination(200))
    suggestions_group_chat_manager = SocietyOfMindAgent('suggestions_analyst', suggestions_group_chat, get_llm_config(0, 'main'), **society_instructions['suggestions'])
    website_code_group = RoundRobinGroupChat([agents['website_get_save']], max_turns=1)
    website_code_group_agent = SocietyOfMindAgent('website_get_save', website_code_group, get_llm_config(0, 'deepseek'), verify_function='store_website', **society_instructions['code_store_group'])
    coding_group_chat = MagenticOneGroupChat([agents['website_developer'], website_code_group_agent], model_client=get_llm_config(0.5, 'deepseek'), termination_condition=MaxMessageTermination(200))
    coding_group_chat_manager = SocietyOfMindAgent('website_developer', coding_group_chat, get_llm_config(0, 'main'), **society_instructions['coding'])
    okr_store_group = RoundRobinGroupChat([agents['okr_store_agent']], max_turns=1)
    okr_store_group_agent = SocietyOfMindAgent('okr_store', okr_store_group, get_llm_config(0.5, 'main'), verify_function='store_okr', **society_instructions['okr_store'])
    okr_group_chat = MagenticOneGroupChat([okr_python_analyst_group_agent, agents['behavioral_analyst'], agents['okr_research_agent'], okr_store_group_agent], model_client=get_llm_config(0, 'deepseek'), termination_condition=MaxMessageTermination(200))
    okr_group_chat_manager = SocietyOfMindAgent('okr_finder', okr_group_chat, get_llm_config(0, 'main'), **society_instructions['okr'])
    magentic_one_group_chat = SelectorGroupChat([okr_group_chat_manager, insights_group_chat_manager, suggestions_group_chat_manager, coding_group_chat_manager], model_client=get_llm_config(0.5, 'deepseek'), termination_condition=MaxMessageTermination(200))
    return {'function_descriptions': function_descriptions, 'insights_group_chat': insights_group_chat, 'suggestions_group_chat': suggestions_group_chat, 'coding_group_chat': coding_group_chat, 'okr_group_chat': okr_group_chat, 'dummy_group_chat': suggestions_analyst_group, 'magentic_one_group_chat': magentic_one_group_chat, 'temp_dirs': [temp_dir_functions, temp_dir], 'evaluate_chat': evaluate_chat}

def default(self, obj):
    if isinstance(obj, Decimal):
        return str(obj)
    return super(DecimalEncoder, self).default(obj)

def python_selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:
    """Selector function for Python analyst group chats"""
    message = messages[-1]
    transitions = {'python_analyst': 'code_executor', 'code_executor': 'python_analyst_interpreter', 'python_analyst_interpreter': 'python_analyst'}
    start = 'python_analyst'
    try:
        if message.source == 'code_executor':
            if 'Traceback' in message.content or 'errors' in message.content:
                return 'python_analyst'
        if message.source == 'python_analyst_interpreter':
            return 'python_analysts'
        return transitions.get(message.source, start)
    except:
        return start



# File: backend/agents/data_analyst_group/src/group_chat.py

import os

import json

import logging

import datetime

from pathlib import Path

from typing import Dict, Any

from dotenv import load_dotenv

import shutil

from autogen_agentchat.ui import Console

from autogen_agentchat.messages import AgentEvent, ChatMessage

from autogen_core import TRACE_LOGGER_NAME

from types import SimpleNamespace

from utils.website_data_utils import update_website_data, get_website_data

from utils.functions import summarize_chat, initialize_env, get_settings

import agentops

import weave

from src.create_group_chat import create_groupchat, update_group_notes, evaluate_chat

from prompts.society_instructions import get_society_instructions



# File: backend/agents/data_analyst_group/tools/checklist_validator.py

from typing import Dict, Any, List

from typing_extensions import Annotated

from pydantic import BaseModel

import json

from utils.functions import get_dynamodb_client, run_completion_with_fallback

def validate_single_item(suggestion: Annotated[Dict[str, Any], 'Suggestion data to validate'], item: Annotated[ChecklistItem, 'Checklist item to validate'], business_context: Annotated[str, 'Business context for validation']) -> Annotated[ChecklistItem, 'Validated checklist item with feedback']:
    """Validate a single checklist item using LLM"""
    prompt = f'\n\tEvaluate this suggestion against the following checklist item:\n\tCategory: {item.category}\n\tItem: {item.item}\n\t\n\tBusiness Context:\n\t{business_context}\n\t\n\tSuggestion:\n\t{json.dumps(suggestion, indent=2)}\n\t\n\tProvide a detailed analysis and determine if this item passes the check.\n\tReturn your response in this format:\n\t{{\n\t\t"passed": true/false,\n\t\t"feedback": "Detailed explanation of why it passed or failed and what needs to be improved"\n\t}}\n\t'
    response = run_completion_with_fallback([{'role': 'user', 'content': prompt}])
    result = json.loads(response)
    item.passed = result['passed']
    item.feedback = result['feedback']
    return item

def update_dynamodb_checklist(suggestion_id: Annotated[str, 'Timestamp of the suggestion'], stream_key: Annotated[str, 'Stream key for the website'], item: Annotated[ChecklistItem, 'Validated checklist item to store']) -> Annotated[bool, 'Success status of the update']:
    """Update a single checklist item in DynamoDB"""
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.update_item(TableName='WebsiteReports', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion_id)}}, UpdateExpression=f'SET guardrails_checklist.#category.#item = :item', ExpressionAttributeNames={'#category': item.category, '#item': item.item}, ExpressionAttributeValues={':item': {'M': {'passed': {'BOOL': item.passed}, 'feedback': {'S': item.feedback}, 'notes': {'S': item.notes}}}})
        return True
    except Exception as e:
        print(f'Error updating checklist item: {str(e)}')
        return False

def validate_checklist(suggestion_id: Annotated[str, 'Timestamp of the suggestion to validate'], stream_key: Annotated[str, 'Stream key for the website'], business_context: Annotated[str, 'Business context for validation']) -> Annotated[Dict[str, Any], 'Validation results or error message']:
    """Validate each checklist item one by one"""
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.get_item(TableName='WebsiteReports', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion_id)}})
        if 'Item' not in response:
            return {'error': 'Suggestion not found'}
        suggestion = response['Item']
        checklist_items = [ChecklistItem(category='Business Relevance', item='Website Implementation Check', passed=False, notes='', feedback=''), ChecklistItem(category='Business Relevance', item='Current Interactions', passed=False, notes='', feedback=''), ChecklistItem(category='Business Relevance', item='Implementation Locations', passed=False, notes='', feedback='')]
        results = []
        for item in checklist_items:
            validated_item = validate_single_item(suggestion, item, business_context)
            update_dynamodb_checklist(suggestion_id, stream_key, validated_item)
            results.append(validated_item.dict())
            if not validated_item.passed:
                break
        overall_status = all((item['passed'] for item in results))
        return {'items': results, 'overall_status': overall_status, 'completed': len(results)}
    except Exception as e:
        return {'error': str(e)}



# File: backend/agents/data_analyst_group/tools/depr_get_session_recording.py

import os

import json

import psycopg2

import decimal

import datetime

from typing import List, Tuple, Any, Dict

from utils.secrets import fetch_secret

from utils.functions import run_completion_with_fallback

from autogen_core.code_executor import with_requirements

import typing_extensions

import requests

import typing

import boto3

import aiohttp

import asyncio

def make_requests(payloads: List[str], function_url: str, timeout: int=900) -> List[dict]:
    """
    Synchronous wrapper for async requests. Makes concurrent POST requests and waits for all to complete.
    """

    async def _make_async_requests():
        async with aiohttp.ClientSession() as session:
            tasks = []
            for payload in payloads:
                tasks.append(asyncio.create_task(single_request(session, payload)))
            return await asyncio.gather(*tasks, return_exceptions=True)

    async def single_request(session, payload):
        headers = {'Content-Type': 'application/json'}
        try:
            async with session.request('POST', function_url, headers=headers, data=payload, timeout=timeout) as response:
                return await response.json()
        except Exception as e:
            print(f'Error converting session recording to video: {e}')
            return {'error': str(e)}
    return asyncio.run(_make_async_requests())

def get_session_recording_description(functions_module, extra_parameters):
    stream_key = extra_parameters.get('stream_key', 'None')
    name = 'get_heatmap'
    description = f'{name}: Retrieves session recording summary using session ID and stream key.\n\n**Important Rules:**\n- Ensure that a correct session recording ID is provided by fetching session_id from the database.\n\n**Example Code:**\n```python\nfrom {functions_module} import get_session_recording\n\nsession_id1 = "example_session_id1" # this must be a session ID given by the python analyst\nsession_id2 = "example_session_id2" # this must be a session ID given by the python analyst\nstream_key = "example_stream_key" # this must be a stream key given by the python analyst\nget_session_recording([session_id1, session_id2], stream_key, "session recording of ...")  # Fetch the session recording summary using session_id and stream_key\n```\n'
    return (name, description)

@with_requirements(python_packages=['boto3', 'datetime'], global_imports=['boto3', 'psycopg2', 'json', 'decimal', 'datetime', 'typing', 'typing_extensions', 'os'])
def get_session_recording(session_ids, stream_key, key=None, convert_to_video=False) -> str:
    key = ''.join([c if c.isalnum() else '_' for c in key]) if key else 'Temporary_Query'
    QUERY_TIMEOUT = 600000

    def is_running_locally() -> bool:
        """
        Determines if the Lambda function is running locally based on Serverless environment or local testing indicators.
        """
        return os.getenv('IS_LOCAL', 'false').lower() == 'true' or os.getenv('AWS_SAM_LOCAL', 'false').lower() == 'true' or 'AWS_LAMBDA_RUNTIME_API' not in os.environ

    def save_results(results: typing_extensions.Annotated[typing.List[typing.Tuple], 'The results to save.'], key: typing_extensions.Annotated[str, 'The key to save the results under.']):
        script_dir = os.path.dirname(__file__)
        results_dir = os.path.join(script_dir, 'results')
        os.makedirs(results_dir, exist_ok=True)
        results_file = os.path.join(results_dir, f'{key}.json')
        with open(results_file, 'w') as f:
            json.dump(results, f)

    def get_secret_fetch():
        if is_running_locally():
            secret_name = 'heatmap/credentials'
        else:
            secret_name = 'heatmap/credentials-fetch'
        region_name = 'us-east-1'
        session = boto3.session.Session()
        client = session.client(service_name='secretsmanager', region_name=region_name)
        get_secret_value_response = client.get_secret_value(SecretId=secret_name)
        return json.loads(get_secret_value_response['SecretString'])

    def get_db_connection():
        """Get database connection with optimizations"""
        secret = get_secret_fetch()
        conn = psycopg2.connect(dbname=secret['dbname'], user=secret['username'], password=secret['password'], host=secret['host'], port=secret['port'], options=f'-c statement_timeout={QUERY_TIMEOUT}')
        return conn

    def execute_query(query: str):
        """Execute a query and return the results"""
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(query)
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        return convert_decimal(results)

    def convert_decimal(obj):
        if isinstance(obj, decimal.Decimal):
            return float(obj)
        if isinstance(obj, list):
            return [convert_decimal(item) for item in obj]
        if isinstance(obj, tuple):
            return tuple((convert_decimal(item) for item in obj))
        if isinstance(obj, datetime.date):
            return obj.isoformat()
        return obj
    try:
        session_ids_str = "', '".join(session_ids)
        query = f"\n            SELECT\n                s.device_form,\n                s.region,\n                s.utm_source,\n                s.os,\n                s.user_agent,\n                sr.start_time,\n                sr.end_time,\n                sr.duration,\n                sr.active_seconds,\n                sr.click_count,\n                sr.start_url,\n                ss.behavior,\n                ss.feeling,\n                ss.category,\n                ss.summary,\n                sr.filepath\n            FROM sessions s\n            JOIN session_recordings sr\n                ON s.session_id = sr.session_id\n            LEFT JOIN session_summaries ss\n                ON s.session_id = ss.session_id\n            WHERE s.stream_key = '{stream_key}' AND s.session_id IN ('{session_ids_str}')\n            ORDER BY sr.start_time DESC;\n        "
        result = execute_query(query)
        if key:
            save_results(result, key)
        if convert_to_video:
            function_url = 'https://xnnyw6culr2eev3r4s4fwajboq0hmywf.lambda-url.us-east-1.on.aws/'
            filepaths = [row[-1] for row in result]
            payloads = [json.dumps({'filepath': filepath}) for filepath in filepaths]
            outputs = make_requests(payloads, function_url)
            print(outputs)
        headers = ['Device Form', 'Region', 'UTM Source', 'OS', 'User Agent', 'Start Time', 'End Time', 'Duration', 'Active Seconds', 'Click Count', 'Start URL', 'Behavior', 'Feeling', 'Category', 'Summary']
        result_md = '| ' + ' | '.join(headers) + ' |\n'
        result_md += '| ' + ' | '.join(['-' * len(header) for header in headers]) + ' |\n'
        if result:
            for row in result:
                result_md += '| ' + ' | '.join(map(str, row[:-1])) + ' |\n'
            print(result_md)
            return result_md
        else:
            return 'No session recordings found for the provided session IDs and stream key.'
    except Exception as e:
        print(f'Error fetching session recording: {e}')
        return 'There was an error processing the request. Please try again later.'

def is_running_locally() -> bool:
    """
        Determines if the Lambda function is running locally based on Serverless environment or local testing indicators.
        """
    return os.getenv('IS_LOCAL', 'false').lower() == 'true' or os.getenv('AWS_SAM_LOCAL', 'false').lower() == 'true' or 'AWS_LAMBDA_RUNTIME_API' not in os.environ

def save_results(results: typing_extensions.Annotated[typing.List[typing.Tuple], 'The results to save.'], key: typing_extensions.Annotated[str, 'The key to save the results under.']):
    script_dir = os.path.dirname(__file__)
    results_dir = os.path.join(script_dir, 'results')
    os.makedirs(results_dir, exist_ok=True)
    results_file = os.path.join(results_dir, f'{key}.json')
    with open(results_file, 'w') as f:
        json.dump(results, f)

def get_secret_fetch():
    if is_running_locally():
        secret_name = 'heatmap/credentials'
    else:
        secret_name = 'heatmap/credentials-fetch'
    region_name = 'us-east-1'
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region_name)
    get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    return json.loads(get_secret_value_response['SecretString'])

def get_db_connection():
    """Get database connection with optimizations"""
    secret = get_secret_fetch()
    conn = psycopg2.connect(dbname=secret['dbname'], user=secret['username'], password=secret['password'], host=secret['host'], port=secret['port'], options=f'-c statement_timeout={QUERY_TIMEOUT}')
    return conn

def execute_query(query: str):
    """Execute a query and return the results"""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute(query)
    results = cursor.fetchall()
    cursor.close()
    conn.close()
    return convert_decimal(results)

def convert_decimal(obj):
    if isinstance(obj, decimal.Decimal):
        return float(obj)
    if isinstance(obj, list):
        return [convert_decimal(item) for item in obj]
    if isinstance(obj, tuple):
        return tuple((convert_decimal(item) for item in obj))
    if isinstance(obj, datetime.date):
        return obj.isoformat()
    return obj



# File: backend/agents/data_analyst_group/tools/design_requirements.py

from typing import Dict, Any, List

from typing_extensions import Annotated

from utils.functions import get_dynamodb_client

import json

from datetime import datetime

from pydantic import BaseModel

def create_design_requirement(suggestion_id: Annotated[str, 'Timestamp of the suggestion'], stream_key: Annotated[str, 'Stream key for the website'], requirement: Annotated[DesignRequirement, 'Design requirement data to create']) -> Annotated[Dict[str, Any], 'Success status or error message']:
    """Create a new design requirement for a suggestion"""
    try:
        dynamodb = get_dynamodb_client()
        requirement_dict = {'locations': {'L': [{'S': loc} for loc in requirement.locations]}, 'ui_specifications': {'M': requirement.ui_specifications}, 'accessibility': {'M': requirement.accessibility}, 'design_patterns': {'L': [{'S': pattern} for pattern in requirement.design_patterns]}, 'implementation_notes': {'S': requirement.implementation_notes}, 'visual_references': {'L': [{'S': ref} for ref in requirement.visual_references]}, 'created_at': {'N': str(int(datetime.now().timestamp()))}}
        response = dynamodb.update_item(TableName='WebsiteReports', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion_id)}}, UpdateExpression='SET design_requirements = :req', ExpressionAttributeValues={':req': {'M': requirement_dict}}, ReturnValues='ALL_NEW')
        return {'success': True, 'message': 'Design requirements created successfully'}
    except Exception as e:
        return {'error': str(e)}

def update_design_requirement(suggestion_id: Annotated[str, 'Timestamp of the suggestion'], stream_key: Annotated[str, 'Stream key for the website'], updates: Annotated[Dict[str, Any], 'Fields to update in the design requirement']) -> Annotated[Dict[str, Any], 'Success status or error message']:
    """Update existing design requirements"""
    try:
        dynamodb = get_dynamodb_client()
        update_expressions = []
        expression_values = {}
        expression_names = {}
        for key, value in updates.items():
            update_expressions.append(f'#dr.#{key} = :{key}')
            expression_names[f'#dr'] = 'design_requirements'
            expression_names[f'#{key}'] = key
            if isinstance(value, list):
                expression_values[f':{key}'] = {'L': [{'S': str(item)} for item in value]}
            elif isinstance(value, dict):
                expression_values[f':{key}'] = {'M': value}
            else:
                expression_values[f':{key}'] = {'S': str(value)}
        update_expressions.append('#dr.#updated_at = :updated_at')
        expression_names['#updated_at'] = 'updated_at'
        expression_values[':updated_at'] = {'N': str(int(datetime.now().timestamp()))}
        update_expression = 'SET ' + ', '.join(update_expressions)
        response = dynamodb.update_item(TableName='WebsiteReports', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion_id)}}, UpdateExpression=update_expression, ExpressionAttributeNames=expression_names, ExpressionAttributeValues=expression_values, ReturnValues='ALL_NEW')
        return {'success': True, 'message': 'Design requirements updated successfully'}
    except Exception as e:
        return {'error': str(e)}

def get_design_requirement(suggestion_id: Annotated[str, 'Timestamp of the suggestion'], stream_key: Annotated[str, 'Stream key for the website']) -> Annotated[Dict[str, Any], 'Design requirement data or error message']:
    """Get design requirements for a suggestion"""
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.get_item(TableName='WebsiteReports', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion_id)}})
        if 'Item' not in response:
            return {'error': 'Suggestion not found'}
        design_requirements = response['Item'].get('design_requirements', {})
        return design_requirements
    except Exception as e:
        return {'error': str(e)}



# File: backend/agents/data_analyst_group/tools/final_guardrails.py

from typing import Dict, Any

from typing_extensions import Annotated

from utils.functions import get_dynamodb_client, run_completion_with_fallback

import json

def perform_final_check(suggestion_id: Annotated[str, 'Timestamp of the suggestion to check'], stream_key: Annotated[str, 'Stream key for the website'], business_context: Annotated[str, 'Business context for evaluation']) -> Annotated[Dict[str, Any], 'Final evaluation result or error message']:
    """Perform final guardrails check after all checklist items are validated"""
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.get_item(TableName='WebsiteReports', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion_id)}})
        if 'Item' not in response:
            return {'error': 'Suggestion not found'}
        suggestion = response['Item']
        checklist = suggestion.get('guardrails_checklist', {})
        if not checklist:
            return {'error': 'Checklist validation not completed'}
        prompt = f'\n\t\tPerform a final comprehensive evaluation of this suggestion considering all checklist validations.\n\t\t\n\t\tBusiness Context:\n\t\t{business_context}\n\t\t\n\t\tSuggestion:\n\t\t{json.dumps(suggestion, indent=2)}\n\t\t\n\t\tChecklist Results:\n\t\t{json.dumps(checklist, indent=2)}\n\t\t\n\t\tReturn your evaluation in this format:\n\t\t{{\n\t\t\t"approved": true/false,\n\t\t\t"summary": "Overall evaluation summary",\n\t\t\t"recommendations": ["List of recommendations if any"],\n\t\t\t"risk_level": "low/medium/high",\n\t\t\t"implementation_priority": "low/medium/high"\n\t\t}}\n\t\t'
        response = run_completion_with_fallback([{'role': 'user', 'content': prompt}])
        evaluation = json.loads(response)
        dynamodb.update_item(TableName='WebsiteReports', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion_id)}}, UpdateExpression='SET guardrails_final_check = :check', ExpressionAttributeValues={':check': {'M': {'approved': {'BOOL': evaluation['approved']}, 'summary': {'S': evaluation['summary']}, 'recommendations': {'L': [{'S': r} for r in evaluation['recommendations']]}, 'risk_level': {'S': evaluation['risk_level']}, 'implementation_priority': {'S': evaluation['implementation_priority']}}}})
        return evaluation
    except Exception as e:
        return {'error': str(e)}



# File: backend/agents/data_analyst_group/tools/firecrawl_tool.py

from typing import Optional, Dict, Any, Annotated

from firecrawl import FirecrawlApp

import time

import requests

from utils.functions import get_api_key

def firecrawl_tool(url: Annotated[str, 'The URL to crawl or scrape'], mode: Annotated[str, "Either 'crawl' or 'scrape'"]='crawl', limit: Annotated[int, 'Maximum number of pages to crawl (only for crawl mode)']=10, formats: Annotated[Optional[list], 'List of formats to return the content in']=None) -> Dict[str, Any]:
    """
	Crawl or scrape a webpage using Firecrawl API.
	
	Args:
		url (str): The URL to crawl or scrape
		mode (str): Either "crawl" or "scrape"
		limit (int): Maximum number of pages to crawl (only for crawl mode)
		formats (list): List of formats to return the content in (default: ['markdown'])
	
	Returns:
		Dict[str, Any]: The crawl or scrape results
	"""
    api_keys = get_api_key('AI_KEYS')
    api_key = api_keys['FIRECRAWL_API_KEY']
    app = FirecrawlApp(api_key=api_key)
    if formats is None:
        formats = ['markdown']
    if mode == 'crawl':
        try:
            params = {'limit': limit, 'scrapeOptions': {'formats': formats}}
            result = app.crawl_url(url, params=params)
            job_id = result.get('jobId')
            if not job_id:
                if result.get('success', False):
                    return result
                else:
                    raise ValueError('No job ID returned from crawl request')
            max_retries = 30
            retry_count = 0
            while retry_count < max_retries:
                try:
                    status = app.check_crawl_status(job_id)
                    current_status = status.get('status', '')
                    if current_status == 'completed':
                        return status
                    elif current_status == 'failed':
                        raise Exception(f'Crawl failed: {status}')
                    elif current_status in ['cancelled']:
                        raise Exception(f'Crawl was cancelled: {status}')
                    time.sleep(5)
                    retry_count += 1
                except requests.exceptions.HTTPError as e:
                    if '404' in str(e):
                        raise Exception(f'Job ID {job_id} not found. The job might have expired or been deleted.')
                    raise
            raise Exception('Maximum retries reached while checking crawl status')
        except Exception as e:
            raise Exception(f'Error during crawl: {str(e)}')
    elif mode == 'scrape':
        try:
            params = {'formats': formats}
            result = app.scrape_url(url=url, params=params)
            return result
        except Exception as e:
            raise Exception(f'Error during scrape: {str(e)}')
    else:
        raise ValueError("Mode must be either 'crawl' or 'scrape'")



# File: backend/agents/data_analyst_group/tools/get_element.py

import os

import json

import typing

from autogen_core.code_executor import with_requirements

import typing_extensions

import requests

import lxml.etree

def get_element_description(functions_module, extra_parameters):
    name = 'get_element'
    description = f"""{name} Retrieves the outerHTML of an element given its XPath and URL.\n\n**Important Rules:**\n- Ensure the URL and XPath are valid.\n- Handle exceptions gracefully.\n\n**Example Code:**\n```python\nfrom {functions_module} import get_element\n\nurl = "https://example.com" # this must be a url given by the python analyst\nxpath = "//div[@id='example']"\nget_element(url, xpath, key="example element")  # Save the outerHTML of the element under the key 'example_element'\n```\n"""
    return (name, description)

@with_requirements(python_packages=['requests', 'lxml'], global_imports=['requests', 'typing_extensions', 'lxml.etree', 'json', 'os'])
def get_element(url: typing_extensions.Annotated[str, 'The URL of the page.'], xpath: typing_extensions.Annotated[str, 'The XPath of the element.'], key: typing_extensions.Annotated[str, 'Easy to remember key to store result of query']=None) -> typing_extensions.Annotated[str, 'The outerHTML of the element or a message if the element is not found.']:

    def save_results(results: typing_extensions.Annotated[typing.List[typing.Tuple], 'The results to save.'], key: typing_extensions.Annotated[str, 'The key to save the results under.']):
        script_dir = os.path.dirname(__file__)
        results_dir = os.path.join(script_dir, 'results')
        os.makedirs(results_dir, exist_ok=True)
        results_file = os.path.join(results_dir, f'{key}.json')
        with open(results_file, 'w') as f:
            json.dump(results, f)
    key = ''.join([c if c.isalnum() else '_' for c in key]) if key else 'Temporary_Element_Query'
    try:
        response = requests.get(url)
        response.raise_for_status()
        html_content = response.text
    except requests.RequestException as e:
        return f'Failed to fetch HTML content from URL: {e}. Perhaps element is no longer there.'
    parser = lxml.etree.HTMLParser()
    tree = lxml.etree.fromstring(html_content, parser)
    element = tree.xpath(xpath)
    if element:
        outer_html = lxml.etree.tostring(element[0], pretty_print=True).decode('utf-8')
        save_results([outer_html], key)
        output_str = f"outerHTML of the element with XPath '{xpath}' is '{outer_html}'"
        print(output_str)
        return outer_html
    else:
        output_str = f"Element with XPath '{xpath}' not found in the HTML content."
        print(output_str)
        return output_str

def save_results(results: typing_extensions.Annotated[typing.List[typing.Tuple], 'The results to save.'], key: typing_extensions.Annotated[str, 'The key to save the results under.']):
    script_dir = os.path.dirname(__file__)
    results_dir = os.path.join(script_dir, 'results')
    os.makedirs(results_dir, exist_ok=True)
    results_file = os.path.join(results_dir, f'{key}.json')
    with open(results_file, 'w') as f:
        json.dump(results, f)



# File: backend/agents/data_analyst_group/tools/get_heatmap.py

import os

import json

import typing

from autogen_core.code_executor import with_requirements

import typing_extensions

import requests

def get_heatmap_description(functions_module, extra_parameters):
    stream_key = extra_parameters.get('stream_key', 'None')
    name = 'get_heatmap'
    description = f'''{name}: Retrieves heatmap data from a specified URL and device type.\n\n**Important Rules:**\n- Ensure the URL and device type are valid.\n- Handle exceptions gracefully.\n\n**Example Code:**\n```python\nfrom {functions_module} import get_heatmap\n\nurl = "https://example.com" # this must be a url given by the python analyst\ndevice_type = "desktop" # 'mobile' or 'desktop'\nheatmap_type = "click" # or "hover", use "hover" if "click" data is minimal\nget_heatmap("{stream_key}", url, device_type, heatmap_type, key="example heatmap")  # Save the heatmap data under the key 'example_heatmap', ), "{stream_key}" must be provided\n```\n'''
    return (name, description)

def get_heatmap_given_context(stream_key):

    def get_heatmap(stream_key: typing_extensions.Annotated[str, 'Stream key to use for the query'], url: typing_extensions.Annotated[str, 'URL of the page to get heatmap data from'], device_type: typing_extensions.Annotated[str, "Device type, either 'mobile', 'tablet', or 'desktop'"], key: typing_extensions.Annotated[str, 'Easy to remember key to store result of query']=None, scrollY: typing_extensions.Annotated[int, 'The location of the screenshot (there is usually no need)']=None) -> typing_extensions.Annotated[str, 'Heatmap image and top data points']:

        def save_results(results: typing_extensions.Annotated[typing.List[typing.Tuple], 'The results to save.'], key: typing_extensions.Annotated[str, 'The key to save the results under.']):
            script_dir = os.path.dirname(__file__)
            results_dir = os.path.join(script_dir, 'results')
            os.makedirs(results_dir, exist_ok=True)
            results_file = os.path.join(results_dir, f'{key}.json')
            with open(results_file, 'w') as f:
                json.dump(results, f)
        key = ''.join([c if c.isalnum() else '_' for c in key]) if key else 'Temporary_Heatmap_Query'
        heatmap_type = 'all'
        options = {'revenue': False, 'format': 'all', 'heatmapType': heatmap_type, 'split': True}
        if scrollY is not None:
            options = {'revenue': False, 'format': 'all', 'heatmapType': heatmap_type, 'scrollY': scrollY}
        event = {'streamKey': stream_key, 'url': url, 'deviceType': device_type, 'outputs': [{'type': 'clickmap', 'options': options}]}
        payload = json.dumps(event)
        headers = {'Content-Type': 'application/json'}
        api_endpoint = 'https://lsruh6ekhnf6g5octogvi3vouu0vixrs.lambda-url.us-east-1.on.aws/'

        def make_request():
            print(payload)
            response = requests.post(api_endpoint, headers=headers, data=payload)
            print(response.text)
            if response.status_code == 200:
                response_data = json.loads(response.text)
                readable_response = json.dumps(response_data, indent=4)
                return {'statusCode': 200, 'body': readable_response}
            else:
                return {'statusCode': response.status_code, 'body': response.text}
        allDataPoints = []
        for _ in range(3):
            result = make_request()
            if result['statusCode'] == 200 and 'outputs' in json.loads(result['body']):
                outputs = json.loads(result['body'])['outputs']
                if len(outputs) > 0 and 'error' not in outputs[0]:
                    ids = outputs[0]['heatmapScreenshotKeys']
                    output_str = f'Heatmap types (choose one) (click id: {ids['click']}, hover id: {ids['hover']}, scroll id: {ids['scroll']}) data for {url} and device {device_type}:'
                    for i, screenshot_url in enumerate(outputs[0]['urls']):
                        output_str += f'\n- Part {i + 1}: <img {screenshot_url}>'
                    allTopDataPoints = outputs[0]['metadata']['allTopDataPoints']
                    for heatmap_type in ['click', 'hover', 'scroll']:
                        output_str += f'\n\nTop data for {heatmap_type}:\n{json.dumps(allTopDataPoints[heatmap_type])}'
                    print(output_str)
                    return output_str
        return 'There was an error processing the request. Please try again later.'
    return get_heatmap

def get_heatmap(stream_key: typing_extensions.Annotated[str, 'Stream key to use for the query'], url: typing_extensions.Annotated[str, 'URL of the page to get heatmap data from'], device_type: typing_extensions.Annotated[str, "Device type, either 'mobile', 'tablet', or 'desktop'"], key: typing_extensions.Annotated[str, 'Easy to remember key to store result of query']=None, scrollY: typing_extensions.Annotated[int, 'The location of the screenshot (there is usually no need)']=None) -> typing_extensions.Annotated[str, 'Heatmap image and top data points']:

    def save_results(results: typing_extensions.Annotated[typing.List[typing.Tuple], 'The results to save.'], key: typing_extensions.Annotated[str, 'The key to save the results under.']):
        script_dir = os.path.dirname(__file__)
        results_dir = os.path.join(script_dir, 'results')
        os.makedirs(results_dir, exist_ok=True)
        results_file = os.path.join(results_dir, f'{key}.json')
        with open(results_file, 'w') as f:
            json.dump(results, f)
    key = ''.join([c if c.isalnum() else '_' for c in key]) if key else 'Temporary_Heatmap_Query'
    heatmap_type = 'all'
    options = {'revenue': False, 'format': 'all', 'heatmapType': heatmap_type, 'split': True}
    if scrollY is not None:
        options = {'revenue': False, 'format': 'all', 'heatmapType': heatmap_type, 'scrollY': scrollY}
    event = {'streamKey': stream_key, 'url': url, 'deviceType': device_type, 'outputs': [{'type': 'clickmap', 'options': options}]}
    payload = json.dumps(event)
    headers = {'Content-Type': 'application/json'}
    api_endpoint = 'https://lsruh6ekhnf6g5octogvi3vouu0vixrs.lambda-url.us-east-1.on.aws/'

    def make_request():
        print(payload)
        response = requests.post(api_endpoint, headers=headers, data=payload)
        print(response.text)
        if response.status_code == 200:
            response_data = json.loads(response.text)
            readable_response = json.dumps(response_data, indent=4)
            return {'statusCode': 200, 'body': readable_response}
        else:
            return {'statusCode': response.status_code, 'body': response.text}
    allDataPoints = []
    for _ in range(3):
        result = make_request()
        if result['statusCode'] == 200 and 'outputs' in json.loads(result['body']):
            outputs = json.loads(result['body'])['outputs']
            if len(outputs) > 0 and 'error' not in outputs[0]:
                ids = outputs[0]['heatmapScreenshotKeys']
                output_str = f'Heatmap types (choose one) (click id: {ids['click']}, hover id: {ids['hover']}, scroll id: {ids['scroll']}) data for {url} and device {device_type}:'
                for i, screenshot_url in enumerate(outputs[0]['urls']):
                    output_str += f'\n- Part {i + 1}: <img {screenshot_url}>'
                allTopDataPoints = outputs[0]['metadata']['allTopDataPoints']
                for heatmap_type in ['click', 'hover', 'scroll']:
                    output_str += f'\n\nTop data for {heatmap_type}:\n{json.dumps(allTopDataPoints[heatmap_type])}'
                print(output_str)
                return output_str
    return 'There was an error processing the request. Please try again later.'

def save_results(results: typing_extensions.Annotated[typing.List[typing.Tuple], 'The results to save.'], key: typing_extensions.Annotated[str, 'The key to save the results under.']):
    script_dir = os.path.dirname(__file__)
    results_dir = os.path.join(script_dir, 'results')
    os.makedirs(results_dir, exist_ok=True)
    results_file = os.path.join(results_dir, f'{key}.json')
    with open(results_file, 'w') as f:
        json.dump(results, f)

def make_request():
    print(payload)
    response = requests.post(api_endpoint, headers=headers, data=payload)
    print(response.text)
    if response.status_code == 200:
        response_data = json.loads(response.text)
        readable_response = json.dumps(response_data, indent=4)
        return {'statusCode': 200, 'body': readable_response}
    else:
        return {'statusCode': response.status_code, 'body': response.text}



# File: backend/agents/data_analyst_group/tools/get_insight.py



# File: backend/agents/data_analyst_group/tools/get_okr.py

from typing_extensions import Annotated

import json

from utils.functions import get_dynamodb_table

from agentops import record_action

from boto3.dynamodb.conditions import Key, Attr

def get_okr_given_context(stream_key):

    @record_action()
    def get_okr() -> Annotated[tuple[str, bool], 'Result message with OKR data and success status']:
        try:
            okr_table = get_dynamodb_table('website-okrs')
            okr_response = okr_table.query(KeyConditionExpression=Key('streamKey').eq(stream_key), FilterExpression=Attr('verified').eq(True))
            okrs = okr_response.get('Items', [])
            if len(okrs) == 0:
                return ('No OKRs found for this stream key', False)
            okrs = []
            for item in sorted(okrs, key=lambda x: x['timestamp'], reverse=True):
                okrs.append(okr_to_markdown(okr))
            result = {'okrs': okrs}
            return (json.dumps(result, indent=2), len(okrs) > 0)
        except Exception as e:
            return (f'Error retrieving OKRs: {e}', True)
    return get_okr

@record_action()
def get_okr() -> Annotated[tuple[str, bool], 'Result message with OKR data and success status']:
    try:
        okr_table = get_dynamodb_table('website-okrs')
        okr_response = okr_table.query(KeyConditionExpression=Key('streamKey').eq(stream_key), FilterExpression=Attr('verified').eq(True))
        okrs = okr_response.get('Items', [])
        if len(okrs) == 0:
            return ('No OKRs found for this stream key', False)
        okrs = []
        for item in sorted(okrs, key=lambda x: x['timestamp'], reverse=True):
            okrs.append(okr_to_markdown(okr))
        result = {'okrs': okrs}
        return (json.dumps(result, indent=2), len(okrs) > 0)
    except Exception as e:
        return (f'Error retrieving OKRs: {e}', True)



# File: backend/agents/data_analyst_group/tools/get_previous_suggestions_summary.py

import boto3

from typing import Annotated, Optional

from utils.functions import run_completion_with_fallback, get_dynamodb_client

from datetime import datetime, timedelta

from boto3.dynamodb.conditions import Key

import os

from botocore.exceptions import ClientError

import logging

def validate_stream_key(stream_key: str) -> bool:
    """
    Validate the stream key format and content.
    
    Args:
        stream_key (str): The stream key to validate
        
    Returns:
        bool: True if valid, False otherwise
    """
    return bool(stream_key and isinstance(stream_key, str))

def get_previous_suggestions_summary_given_context(stream_key: str):
    """
    Create a function to get previous suggestions summary for a given stream key.
    
    Args:
        stream_key (str): The key used to identify the stream
        
    Returns:
        Callable: A function that retrieves the suggestions summary
    """

    def get_previous_suggestions_summary() -> Annotated[Optional[str], 'The summary of previous suggestions if available, otherwise None']:
        """
        Retrieve the summary of previous suggestions from DynamoDB.

        Returns:
            Optional[str]: The summary of previous suggestions if available, otherwise None.
            
        Raises:
            ValueError: If stream key is invalid
            ClientError: If DynamoDB operation fails
        """
        if not validate_stream_key(stream_key):
            raise ValueError('Invalid stream key provided')
        try:
            response = dynamodb.get_item(TableName='WebsiteData', Key={'streamKey': {'S': stream_key}})
            item = response.get('Item', {})
            summary = item.get('summary', {}).get('S')
            return summary or None
        except ClientError as e:
            raise
        except Exception as e:
            raise
    return get_previous_suggestions_summary

def get_previous_suggestions_summary() -> Annotated[Optional[str], 'The summary of previous suggestions if available, otherwise None']:
    """
        Retrieve the summary of previous suggestions from DynamoDB.

        Returns:
            Optional[str]: The summary of previous suggestions if available, otherwise None.
            
        Raises:
            ValueError: If stream key is invalid
            ClientError: If DynamoDB operation fails
        """
    if not validate_stream_key(stream_key):
        raise ValueError('Invalid stream key provided')
    try:
        response = dynamodb.get_item(TableName='WebsiteData', Key={'streamKey': {'S': stream_key}})
        item = response.get('Item', {})
        summary = item.get('summary', {}).get('S')
        return summary or None
    except ClientError as e:
        raise
    except Exception as e:
        raise



# File: backend/agents/data_analyst_group/tools/get_query_result.py

from autogen_core.code_executor import with_requirements

import os

import json

import typing_extensions

import typing

def get_query_result_description(functions_module, extra_parameters):
    name = 'get_query_result'
    description = f"{name}: Retrieves the results of a previously run query using its key.\n\n**Important Rules:**\n- Ensure the key corresponds to a stored query result.\n- Handle exceptions gracefully.\n\n**Example Code:**\n```python\nfrom {functions_module} import get_query_result\n\nresults = get_query_result('session_data') # note this will not work if the result was not saved and there was an error in execution\nprint(results)\n```\n"
    return (name, description)

@with_requirements(python_packages=[], global_imports=['json', 'typing', 'typing_extensions'])
def get_query_result(key):
    key = ''.join([c if c.isalnum() else '_' for c in key])
    script_dir = os.path.dirname(__file__)
    results_dir = os.path.join(script_dir, 'results')
    results_file = os.path.join(results_dir, f'{key}.json')
    with open(results_file, 'r') as f:
        results = json.load(f)
    return results



# File: backend/agents/data_analyst_group/tools/get_rendered_html.py

from playwright.sync_api import sync_playwright

import boto3

import os

import time

from datetime import datetime

def get_rendered_html(url: str, wait_time: int=3) -> dict:
    """
    Opens a website using Playwright, waits for rendering, and saves the HTML and CSS to S3.

    Args:
        url (str): The URL of the website to capture
        wait_time (int): Time to wait in seconds for the page to render (default: 3)

    Returns:
        dict: Dictionary containing the S3 paths of saved HTML and CSS files
    """
    s3_client = boto3.client('s3', aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'), aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'), region_name=os.environ.get('AWS_REGION'))
    bucket_name = 'sitewiz-rendered-content'
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    base_key = f'rendered/{timestamp}'
    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(url)
        time.sleep(wait_time)
        html_content = page.content()
        css_contents = page.evaluate("() => {\n            const styleSheets = Array.from(document.styleSheets);\n            const cssTexts = [];\n\n            for (const sheet of styleSheets) {\n                try {\n                    if (sheet.href) {\n                        cssTexts.push(`/* ${sheet.href} */\\n${Array.from(sheet.cssRules).map(rule => rule.cssText).join('\\n')}`);\n                    } else {\n                        cssTexts.push(Array.from(sheet.cssRules).map(rule => rule.cssText).join('\\n'));\n                    }\n                } catch (e) {\n                    console.warn('Could not access stylesheet:', e);\n                }\n            }\n\n            return cssTexts.join('\\n\\n');\n        }")
        browser.close()
        html_key = f'{base_key}/page.html'
        s3_client.put_object(Bucket=bucket_name, Key=html_key, Body=html_content, ContentType='text/html')
        css_key = f'{base_key}/styles.css'
        s3_client.put_object(Bucket=bucket_name, Key=css_key, Body=css_contents, ContentType='text/css')
        return {'html_path': f's3://{bucket_name}/{html_key}', 'css_path': f's3://{bucket_name}/{css_key}'}



# File: backend/agents/data_analyst_group/tools/get_screenshot.py

import requests

from PIL import Image

from io import BytesIO

def get_screenshot(url: str, deviceType: str, max_splits: int=5) -> list[str]:
    """
    Get a full-page screenshot of the specified URL and split it into parts.

    Args:
        url (str): The URL of the website to screenshot.
        max_splits (int): The maximum number of splits for the screenshot.

    Returns:
        list[str]: A list of URLs of the split screenshot images.
    """
    api_endpoint = 'https://lsruh6ekhnf6g5octogvi3vouu0vixrs.lambda-url.us-east-1.on.aws/'
    payload = {'url': url, 'outputs': [{'type': 'screenshot', 'options': {'split': True, 'max_splits': max_splits, 'fullPage': True, 'deviceType': deviceType}}]}
    headers = {'Content-Type': 'application/json'}

    def make_request():
        response = requests.post(api_endpoint, headers=headers, json=payload)
        response.raise_for_status()
        return response
    for attempt in range(3):
        try:
            response = make_request()
            break
        except requests.exceptions.RequestException as e:
            if attempt == 2:
                raise
            continue
    response_data = response.json()
    print(response_data)
    if 'outputs' in response_data and len(response_data['outputs']) > 0:
        output = response_data['outputs'][0]
        if 'urls' in output:
            urls = [f'<img {url}>' for url in output['urls']]
            return urls
        else:
            return 'No URLs found in the response'
    else:
        return 'Invalid response format'

def make_request():
    response = requests.post(api_endpoint, headers=headers, json=payload)
    response.raise_for_status()
    return response



# File: backend/agents/data_analyst_group/tools/get_session_recording.py

import os

import json

import psycopg2

import decimal

import datetime

from typing import List, Tuple, Any, Dict

from utils.secrets import fetch_secret

from utils.functions import run_completion_with_fallback

from autogen_core.code_executor import with_requirements

import typing_extensions

import requests

import typing

import boto3

import aiohttp

import asyncio

def make_requests(payloads: List[str], function_url: str, timeout: int=900) -> List[dict]:
    """
    Synchronous wrapper for async requests. Makes concurrent POST requests and waits for all to complete.
    """

    async def _make_async_requests():
        async with aiohttp.ClientSession() as session:
            tasks = []
            for payload in payloads:
                tasks.append(asyncio.create_task(single_request(session, payload)))
            return await asyncio.gather(*tasks, return_exceptions=True)

    async def single_request(session, payload):
        headers = {'Content-Type': 'application/json'}
        try:
            async with session.request('POST', function_url, headers=headers, data=payload, timeout=timeout) as response:
                return await response.json()
        except Exception as e:
            print(f'Error converting session recording to video: {e}')
            return {'error': str(e)}
    return asyncio.run(_make_async_requests())

def get_session_recording_description(functions_module, extra_parameters):
    stream_key = extra_parameters.get('stream_key', 'None')
    name = 'get_heatmap'
    description = f'{name}: Retrieves session recording summary using session ID and stream key.\n\n**Important Rules:**\n- Ensure that a correct session recording ID is provided by fetching session_id from the database.\n\n**Example Code:**\n```python\nfrom {functions_module} import get_session_recording\n\nsession_id1 = "example_session_id1" # this must be a session ID given by the python analyst\nsession_id2 = "example_session_id2" # this must be a session ID given by the python analyst\nstream_key = "example_stream_key" # this must be a stream key given by the python analyst\nget_session_recording([session_id1, session_id2], stream_key, "session recording of ...")  # Fetch the session recording summary using session_id and stream_key\n```\n'
    return (name, description)

@with_requirements(python_packages=['boto3', 'datetime'], global_imports=['boto3', 'psycopg2', 'json', 'decimal', 'datetime', 'typing', 'typing_extensions', 'os'])
def get_session_recording(session_ids, stream_key, key=None, convert_to_video=False) -> str:
    key = ''.join([c if c.isalnum() else '_' for c in key]) if key else 'Temporary_Query'
    QUERY_TIMEOUT = 600000

    def is_running_locally() -> bool:
        """
        Determines if the Lambda function is running locally based on Serverless environment or local testing indicators.
        """
        return os.getenv('IS_LOCAL', 'false').lower() == 'true' or os.getenv('AWS_SAM_LOCAL', 'false').lower() == 'true' or 'AWS_LAMBDA_RUNTIME_API' not in os.environ

    def save_results(results: typing_extensions.Annotated[typing.List[typing.Tuple], 'The results to save.'], key: typing_extensions.Annotated[str, 'The key to save the results under.']):
        script_dir = os.path.dirname(__file__)
        results_dir = os.path.join(script_dir, 'results')
        os.makedirs(results_dir, exist_ok=True)
        results_file = os.path.join(results_dir, f'{key}.json')
        with open(results_file, 'w') as f:
            json.dump(results, f)

    def get_secret_fetch():
        if is_running_locally():
            secret_name = 'heatmap/credentials'
        else:
            secret_name = 'heatmap/credentials-fetch'
        region_name = 'us-east-1'
        session = boto3.session.Session()
        client = session.client(service_name='secretsmanager', region_name=region_name)
        get_secret_value_response = client.get_secret_value(SecretId=secret_name)
        return json.loads(get_secret_value_response['SecretString'])

    def get_db_connection():
        """Get database connection with optimizations"""
        secret = get_secret_fetch()
        conn = psycopg2.connect(dbname=secret['dbname'], user=secret['username'], password=secret['password'], host=secret['host'], port=secret['port'], options=f'-c statement_timeout={QUERY_TIMEOUT}')
        return conn

    def execute_query(query: str):
        """Execute a query and return the results"""
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(query)
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        return convert_decimal(results)

    def convert_decimal(obj):
        if isinstance(obj, decimal.Decimal):
            return float(obj)
        if isinstance(obj, list):
            return [convert_decimal(item) for item in obj]
        if isinstance(obj, tuple):
            return tuple((convert_decimal(item) for item in obj))
        if isinstance(obj, datetime.date):
            return obj.isoformat()
        return obj
    try:
        session_ids_str = "', '".join(session_ids)
        query = f"\n            SELECT\n                s.device_form,\n                s.region,\n                s.utm_source,\n                s.os,\n                s.user_agent,\n                sr.start_time,\n                sr.end_time,\n                sr.duration,\n                sr.active_seconds,\n                sr.click_count,\n                sr.start_url,\n                ss.behavior,\n                ss.feeling,\n                ss.category,\n                ss.summary,\n                sr.filepath\n            FROM sessions s\n            JOIN session_recordings sr\n                ON s.session_id = sr.session_id\n            LEFT JOIN session_summaries ss\n                ON s.session_id = ss.session_id\n            WHERE s.stream_key = '{stream_key}' AND s.session_id IN ('{session_ids_str}')\n            ORDER BY sr.start_time DESC;\n        "
        result = execute_query(query)
        if key:
            save_results(result, key)
        if convert_to_video:
            function_url = 'https://xnnyw6culr2eev3r4s4fwajboq0hmywf.lambda-url.us-east-1.on.aws/'
            filepaths = [row[-1] for row in result]
            payloads = [json.dumps({'filepath': filepath}) for filepath in filepaths]
            outputs = make_requests(payloads, function_url)
            print(outputs)
        headers = ['Device Form', 'Region', 'UTM Source', 'OS', 'User Agent', 'Start Time', 'End Time', 'Duration', 'Active Seconds', 'Click Count', 'Start URL', 'Behavior', 'Feeling', 'Category', 'Summary']
        result_md = '| ' + ' | '.join(headers) + ' |\n'
        result_md += '| ' + ' | '.join(['-' * len(header) for header in headers]) + ' |\n'
        if result:
            for row in result:
                result_md += '| ' + ' | '.join(map(str, row[:-1])) + ' |\n'
            print(result_md)
            return result_md
        else:
            return 'No session recordings found for the provided session IDs and stream key.'
    except Exception as e:
        print(f'Error fetching session recording: {e}')
        return 'There was an error processing the request. Please try again later.'

def is_running_locally() -> bool:
    """
        Determines if the Lambda function is running locally based on Serverless environment or local testing indicators.
        """
    return os.getenv('IS_LOCAL', 'false').lower() == 'true' or os.getenv('AWS_SAM_LOCAL', 'false').lower() == 'true' or 'AWS_LAMBDA_RUNTIME_API' not in os.environ

def save_results(results: typing_extensions.Annotated[typing.List[typing.Tuple], 'The results to save.'], key: typing_extensions.Annotated[str, 'The key to save the results under.']):
    script_dir = os.path.dirname(__file__)
    results_dir = os.path.join(script_dir, 'results')
    os.makedirs(results_dir, exist_ok=True)
    results_file = os.path.join(results_dir, f'{key}.json')
    with open(results_file, 'w') as f:
        json.dump(results, f)

def get_secret_fetch():
    if is_running_locally():
        secret_name = 'heatmap/credentials'
    else:
        secret_name = 'heatmap/credentials-fetch'
    region_name = 'us-east-1'
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region_name)
    get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    return json.loads(get_secret_value_response['SecretString'])

def get_db_connection():
    """Get database connection with optimizations"""
    secret = get_secret_fetch()
    conn = psycopg2.connect(dbname=secret['dbname'], user=secret['username'], password=secret['password'], host=secret['host'], port=secret['port'], options=f'-c statement_timeout={QUERY_TIMEOUT}')
    return conn

def execute_query(query: str):
    """Execute a query and return the results"""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute(query)
    results = cursor.fetchall()
    cursor.close()
    conn.close()
    return convert_decimal(results)

def convert_decimal(obj):
    if isinstance(obj, decimal.Decimal):
        return float(obj)
    if isinstance(obj, list):
        return [convert_decimal(item) for item in obj]
    if isinstance(obj, tuple):
        return tuple((convert_decimal(item) for item in obj))
    if isinstance(obj, datetime.date):
        return obj.isoformat()
    return obj



# File: backend/agents/data_analyst_group/tools/get_session_recording_videos.py

import asyncio

import aiohttp

from typing import List, Dict, Any, Callable

import cv2

import numpy as np

import tempfile

from utils.functions import run_completion_with_fallback, get_db_connection

import psycopg2

from psycopg2.extras import RealDictCursor

import boto3

import requests

from PIL import Image

from io import BytesIO

def get_session_recording_videos_given_context(stream_key: str) -> Callable[[List[str]], List[Dict[str, Any]]]:
    """
    Returns a function to fetch session recording videos based on the provided stream key
    and generate detailed summaries using AI completions with fallback models.

    Args:
        stream_key (str): The stream key to filter session recordings.

    Returns:
        Callable[[List[str]], List[Dict[str, Any]]]: A function that takes a list of session IDs and fetches their videos with detailed summaries.
    """
    function_url = 'https://xnnyw6culr2eev3r4s4fwajboq0hmywf.lambda-url.us-east-1.on.aws/'
    video_model_fallback_list = ['vertex_ai/gemini-2.0-flash-exp', 'vertex_ai/gemini-1.5-flash', 'vertex_ai/gemini-1.5-pro']

    def get_filepath_by_session_id(session_id: str) -> str:
        """
        Retrieves the filepath for a given session_id from the RDS database.

        Args:
            session_id (str): The session ID.

        Returns:
            str: The filepath associated with the session ID.

        Raises:
            ValueError: If no filepath is found for the given session_id.
            Exception: If an error occurs during the database operation.
        """
        connection = None
        cursor = None
        try:
            connection = get_db_connection()
            cursor = connection.cursor(cursor_factory=RealDictCursor)
            query = '\n                SELECT filepath\n                FROM session_recordings\n                WHERE session_id = %s\n                LIMIT 1\n            '
            cursor.execute(query, (session_id,))
            result = cursor.fetchone()
            if result and 'filepath' in result:
                filepath = result['filepath']
                return filepath
            else:
                raise ValueError(f'No filepath found for session_id: {session_id}')
        except Exception as e:
            print(f'Error retrieving filepath for session_id {session_id}: {e}')
            raise
        finally:
            if cursor:
                cursor.close()
            if connection:
                connection.close()

    def get_video_duration(video_url: str) -> float:
        """
        Downloads a video from URL and calculates its duration in seconds.
        
        Args:
            video_url (str): URL of the video file
            
        Returns:
            float: Duration of the video in seconds, or -1 if there's an error
        """
        try:
            response = requests.get(video_url, stream=True)
            response.raise_for_status()
            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=True) as temp_file:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        temp_file.write(chunk)
                temp_file.flush()
                cap = cv2.VideoCapture(temp_file.name)
                if not cap.isOpened():
                    return -1
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                fps = cap.get(cv2.CAP_PROP_FPS)
                duration = frame_count / fps if fps > 0 else -1
                cap.release()
                return duration
        except Exception as e:
            print(f'Error calculating video duration: {e}')
            return -1

    async def fetch_video(session: aiohttp.ClientSession, session_id: str) -> Dict[str, Any]:
        """
        Asynchronously fetches video details for a single session ID, retrieves the filepath, and includes it in the request.

        Args:
            session (aiohttp.ClientSession): The aiohttp session to use for the request.
            session_id (str): The session ID to fetch the video for.

        Returns:
            Dict[str, Any]: The video details or an error message.
        """
        for i in range(3):
            try:
                filepath = get_filepath_by_session_id(session_id)
                if not filepath:
                    raise ValueError(f'No filepath found for session_id: {session_id}')
                payload = {'session_id': session_id, 'stream_key': stream_key, 'filepath': filepath}
                async with session.post(function_url, json=payload) as response:
                    response.raise_for_status()
                    video_data = await response.json()
                    video_data['filepath'] = filepath
                    if 'file' in video_data:
                        video_duration = get_video_duration(video_data['file'])
                        video_data['duration_seconds'] = video_duration
                    return video_data
            except Exception as e:
                print(f'Error fetching video for session_id {session_id}: {e}')

    async def get_session_recording_videos(session_ids: List[str]) -> List[Dict[str, Any]]:
        """
        Asynchronously fetches videos for a list of session IDs in parallel and generates detailed summaries.

        Args:
            session_ids (List[str]): A list of session IDs to fetch videos for.

        Returns:
            List[Dict[str, Any]]: A list of video details with detailed summaries.
        """
        async with aiohttp.ClientSession() as session:
            tasks = [fetch_video(session, session_id) for session_id in session_ids]
            videos = await asyncio.gather(*tasks, return_exceptions=True)
        processed_videos = []
        for video in videos:
            if isinstance(video, dict) and 'error' not in video:
                video_url = video.get('file')
                filepath = video.get('filepath')
                if video_url and filepath:
                    print('Duration: ', video['duration_seconds'])
                    if video['duration_seconds'] < 5:
                        video['detailed_summary'] = 'Video is too short to generate summary. Do not use it.'
                        processed_videos.append(video)
                        continue
                    summary = run_completion_with_fallback(messages=[{'role': 'user', 'content': [{'type': 'image_url', 'image_url': {'url': video_url}}, {'type': 'text', 'text': f'The duration of the video is {video['duration_seconds']} seconds.\n{prompt}'}]}], models=video_model_fallback_list, response_format=None)
                    video['detailed_summary'] = summary
                else:
                    video['filepath'] = filepath if filepath else 'No filepath provided.'
                    video['detailed_summary'] = 'Insufficient data to generate summary.'
                processed_videos.append(video)
            else:
                processed_videos.append({'error': str(video)})
        return processed_videos

    def get_session_recording_videos_sync(session_ids: List[str]) -> List[Dict[str, Any]]:
        """
        Synchronously fetches videos and generates detailed summaries by running the asynchronous function.

        Args:
            session_ids (List[str]): A list of session IDs to fetch videos for.

        Returns:
            List[Dict[str, Any]]: A list of video details with detailed summaries.
        """
        return asyncio.run(get_session_recording_videos(session_ids))
    return get_session_recording_videos_sync

def get_filepath_by_session_id(session_id: str) -> str:
    """
        Retrieves the filepath for a given session_id from the RDS database.

        Args:
            session_id (str): The session ID.

        Returns:
            str: The filepath associated with the session ID.

        Raises:
            ValueError: If no filepath is found for the given session_id.
            Exception: If an error occurs during the database operation.
        """
    connection = None
    cursor = None
    try:
        connection = get_db_connection()
        cursor = connection.cursor(cursor_factory=RealDictCursor)
        query = '\n                SELECT filepath\n                FROM session_recordings\n                WHERE session_id = %s\n                LIMIT 1\n            '
        cursor.execute(query, (session_id,))
        result = cursor.fetchone()
        if result and 'filepath' in result:
            filepath = result['filepath']
            return filepath
        else:
            raise ValueError(f'No filepath found for session_id: {session_id}')
    except Exception as e:
        print(f'Error retrieving filepath for session_id {session_id}: {e}')
        raise
    finally:
        if cursor:
            cursor.close()
        if connection:
            connection.close()

def get_video_duration(video_url: str) -> float:
    """
        Downloads a video from URL and calculates its duration in seconds.
        
        Args:
            video_url (str): URL of the video file
            
        Returns:
            float: Duration of the video in seconds, or -1 if there's an error
        """
    try:
        response = requests.get(video_url, stream=True)
        response.raise_for_status()
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=True) as temp_file:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    temp_file.write(chunk)
            temp_file.flush()
            cap = cv2.VideoCapture(temp_file.name)
            if not cap.isOpened():
                return -1
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = frame_count / fps if fps > 0 else -1
            cap.release()
            return duration
    except Exception as e:
        print(f'Error calculating video duration: {e}')
        return -1

def get_session_recording_videos_sync(session_ids: List[str]) -> List[Dict[str, Any]]:
    """
        Synchronously fetches videos and generates detailed summaries by running the asynchronous function.

        Args:
            session_ids (List[str]): A list of session IDs to fetch videos for.

        Returns:
            List[Dict[str, Any]]: A list of video details with detailed summaries.
        """
    return asyncio.run(get_session_recording_videos(session_ids))



# File: backend/agents/data_analyst_group/tools/get_similar_experiments.py

from typing_extensions import Annotated

from utils.functions import run_completion_with_fallback

def get_similar_experiments(suggestion: Annotated[str, 'The suggestion string to find similar experiments for.']) -> Annotated[str, 'A string containing similar experiments with percent changes in metrics and sources.']:
    """
    Runs the completion with fallback on the input suggestion and returns similar experiments.
    
    Args:
        suggestion (str): The suggestion string to find similar experiments for.
        
    Returns:
        str: A string containing similar experiments with percent changes in metrics and sources.
    """
    prompt = f"\n    Find a/b experimentation results from your knowledge base \n    (they must include the percent change in metric as well as have the source, eg who ran it) \n    similar to this suggestion so I can predict the results of this suggestion according to similar experiments run: \n    \n    {suggestion}\n\n\nYou must use real A/B testing results from real sources. If you don't have any, you can say so.\n    "
    try:
        similar_experiments = run_completion_with_fallback(prompt=prompt)
        return similar_experiments
    except Exception as e:
        return f'An error occurred while trying to find similar experiments: {str(e)}'



# File: backend/agents/data_analyst_group/tools/get_similar_session_recordings.py

import psycopg2

from typing import List, Dict, Any, Callable, Annotated

import traceback

from utils.functions import get_db_connection, bge_en_base_embedding

from datetime import datetime, timezone, timedelta

def get_similar_session_recordings_given_context(stream_key: Annotated[str, 'The stream key used to filter session recordings']) -> Callable[[str, int, str | None], List[Dict[str, Any]]]:
    """
    Returns a function to fetch similar session recordings based on the provided stream key.

    Args:
        stream_key (str): The stream key to filter session recordings.

    Returns:
        Callable[[str, int, str | None], List[Dict[str, Any]]]: A function that takes a query, limit, and optional SQL query to fetch similar sessions.
    """

    def get_similar_session_recordings(query: Annotated[str, 'The search query to find similar session recordings'], limit: Annotated[int, 'Maximum number of similar sessions to retrieve']=50, sql_query: Annotated[str | None, 'Optional custom SQL query. If None, uses default query']=None) -> List[Dict[str, Any]]:
        """
        Fetch session recordings from the RDS database similar to the given query using BGE embeddings.
    
        Args:
            query (str): The search query to find similar session recordings.
            limit (int): The maximum number of similar sessions to retrieve.
            sql_query (str | None): Optional custom SQL query. If None, uses default query.
    
        Returns:
            List[Dict[str, Any]]: A list of similar session recordings.
        """
        connection = get_db_connection()
        cursor = connection.cursor()
        try:
            embedding = bge_en_base_embedding(query)
            default_sql = '\n                SELECT ss.session_id, ss.summary, sr.duration, s.device_name, s.region,\n                       1 - (ss.bge_base_en <=> %s::vector) AS similarity\n                FROM session_summaries ss\n                JOIN session_recordings sr ON ss.session_id = sr.session_id\n                JOIN sessions s ON s.session_id = sr.session_id\n                WHERE ss.stream_key = %s\n                  AND sr.start_time >= %s\n                ORDER BY similarity DESC\n                LIMIT %s;\n            '
            sql = sql_query if sql_query is not None else default_sql
            threshold_time = int((datetime.now(timezone.utc) - timedelta(days=7)).timestamp())
            cursor.execute(sql, (embedding, stream_key, threshold_time, limit))
            results = cursor.fetchall()
            similar_sessions = []
            for row in results:
                session = {'session_id': row[0], 'summary': row[1], 'duration': row[2], 'device_name': row[3], 'region': row[4], 'similarity': row[5]}
                similar_sessions.append(session)
            return similar_sessions
        except Exception as e:
            print(f'Error fetching similar session recordings: {e}')
            traceback.print_exc()
            return []
        finally:
            cursor.close()
            connection.close()
    return get_similar_session_recordings

def get_similar_session_recordings(query: Annotated[str, 'The search query to find similar session recordings'], limit: Annotated[int, 'Maximum number of similar sessions to retrieve']=50, sql_query: Annotated[str | None, 'Optional custom SQL query. If None, uses default query']=None) -> List[Dict[str, Any]]:
    """
        Fetch session recordings from the RDS database similar to the given query using BGE embeddings.
    
        Args:
            query (str): The search query to find similar session recordings.
            limit (int): The maximum number of similar sessions to retrieve.
            sql_query (str | None): Optional custom SQL query. If None, uses default query.
    
        Returns:
            List[Dict[str, Any]]: A list of similar session recordings.
        """
    connection = get_db_connection()
    cursor = connection.cursor()
    try:
        embedding = bge_en_base_embedding(query)
        default_sql = '\n                SELECT ss.session_id, ss.summary, sr.duration, s.device_name, s.region,\n                       1 - (ss.bge_base_en <=> %s::vector) AS similarity\n                FROM session_summaries ss\n                JOIN session_recordings sr ON ss.session_id = sr.session_id\n                JOIN sessions s ON s.session_id = sr.session_id\n                WHERE ss.stream_key = %s\n                  AND sr.start_time >= %s\n                ORDER BY similarity DESC\n                LIMIT %s;\n            '
        sql = sql_query if sql_query is not None else default_sql
        threshold_time = int((datetime.now(timezone.utc) - timedelta(days=7)).timestamp())
        cursor.execute(sql, (embedding, stream_key, threshold_time, limit))
        results = cursor.fetchall()
        similar_sessions = []
        for row in results:
            session = {'session_id': row[0], 'summary': row[1], 'duration': row[2], 'device_name': row[3], 'region': row[4], 'similarity': row[5]}
            similar_sessions.append(session)
        return similar_sessions
    except Exception as e:
        print(f'Error fetching similar session recordings: {e}')
        traceback.print_exc()
        return []
    finally:
        cursor.close()
        connection.close()



# File: backend/agents/data_analyst_group/tools/get_stored_suggestions.py

import json

import os

import traceback

from typing import List, Optional, Dict, Any

from typing_extensions import Annotated, TypedDict

from pydantic import BaseModel

from litellm import completion

import boto3

from datetime import datetime, timedelta, timezone

from boto3.dynamodb.conditions import Key

import logging

from decimal import Decimal

from concurrent.futures import ThreadPoolExecutor, as_completed

from litellm.utils import trim_messages

import math

from utils.functions import run_completion_with_fallback, suggestion_to_markdown, suggestion_to_markdown_parts, get_dynamodb_table

import boto3

def get_stored_suggestions_given_context(stream_key: Annotated[str, 'The stream key for which to retrieve suggestions']) -> Annotated[str, 'The suggestions for the given stream key']:

    def get_suggestions_for_week(stream_key: str):
        """
        Retrieve suggestions generated for the past week from DynamoDB.
        """
        now = datetime.now(timezone.utc)
        end_timestamp = int(now.timestamp())
        start_timestamp = int((now - timedelta(days=7)).timestamp())
        query_params = {'KeyConditionExpression': 'streamKey = :streamKey AND #ts BETWEEN :start AND :end', 'ExpressionAttributeNames': {'#ts': 'timestamp'}, 'ExpressionAttributeValues': {':streamKey': stream_key, ':start': start_timestamp, ':end': end_timestamp}}
        print('Query Params:', query_params)
        dynamodb = boto3.resource('dynamodb')
        table_name = os.getenv('DYNAMODB_TABLE_NAME', 'WebsiteReports')
        table = dynamodb.Table(table_name)
        response = table.query(**query_params)
        return response.get('Items', [])

    def get_all_suggestions(stream_key: str):
        """
        Retrieve suggestions generated for the week from DynamoDB.
        """
        query_params = {'TableName': os.getenv('DYNAMODB_TABLE_NAME', 'WebsiteReports'), 'KeyConditionExpression': 'streamKey = :streamKey', 'FilterExpression': '#reviewed = :reviewed', 'ExpressionAttributeNames': {'#reviewed': 'Reviewed'}, 'ExpressionAttributeValues': {':streamKey': stream_key, ':reviewed': True}}
        response = table.query(**query_params)
        return response.get('Items', [])

    def get_stored_suggestions(query: Annotated[str, 'Get most relevant suggestions given a query'], limit: Annotated[int, 'Max relevant suggestions to get']=5) -> Annotated[str, 'The suggestions for the given stream key']:
        """
        Retrieve stored suggestions for the given stream key.
        """
        suggestions = get_suggestions_for_week(stream_key) + get_all_suggestions(stream_key)

        def get_suggestion_header_timestamp(suggestion):
            return suggestion['timestamp'] + '\n' + suggestion['header']
        suggestion_markdowns = [get_suggestion_header_timestamp(suggestion_to_markdown_parts(suggestion, True)) for suggestion in suggestions]
        joined_suggestions = '\n\n'.join(suggestion_markdowns)
        if not query:
            return joined_suggestions
        relevant_joined_suggestions_prompt = f'Get at max {limit} relevant suggestions to this query: {query}\n\nHere are all the stored suggestions:\n{joined_suggestions}\n\n\nOutput the {limit} most relevant suggestions. Weight more recent (later) suggestions more.\n'
        relevant_joined_suggestions = run_completion_with_fallback(prompt=relevant_joined_suggestions_prompt, models=['main-mini'])
        return relevant_joined_suggestions
    return get_stored_suggestions

def get_suggestions_for_week(stream_key: str):
    """
        Retrieve suggestions generated for the past week from DynamoDB.
        """
    now = datetime.now(timezone.utc)
    end_timestamp = int(now.timestamp())
    start_timestamp = int((now - timedelta(days=7)).timestamp())
    query_params = {'KeyConditionExpression': 'streamKey = :streamKey AND #ts BETWEEN :start AND :end', 'ExpressionAttributeNames': {'#ts': 'timestamp'}, 'ExpressionAttributeValues': {':streamKey': stream_key, ':start': start_timestamp, ':end': end_timestamp}}
    print('Query Params:', query_params)
    dynamodb = boto3.resource('dynamodb')
    table_name = os.getenv('DYNAMODB_TABLE_NAME', 'WebsiteReports')
    table = dynamodb.Table(table_name)
    response = table.query(**query_params)
    return response.get('Items', [])

def get_all_suggestions(stream_key: str):
    """
        Retrieve suggestions generated for the week from DynamoDB.
        """
    query_params = {'TableName': os.getenv('DYNAMODB_TABLE_NAME', 'WebsiteReports'), 'KeyConditionExpression': 'streamKey = :streamKey', 'FilterExpression': '#reviewed = :reviewed', 'ExpressionAttributeNames': {'#reviewed': 'Reviewed'}, 'ExpressionAttributeValues': {':streamKey': stream_key, ':reviewed': True}}
    response = table.query(**query_params)
    return response.get('Items', [])

def get_stored_suggestions(query: Annotated[str, 'Get most relevant suggestions given a query'], limit: Annotated[int, 'Max relevant suggestions to get']=5) -> Annotated[str, 'The suggestions for the given stream key']:
    """
        Retrieve stored suggestions for the given stream key.
        """
    suggestions = get_suggestions_for_week(stream_key) + get_all_suggestions(stream_key)

    def get_suggestion_header_timestamp(suggestion):
        return suggestion['timestamp'] + '\n' + suggestion['header']
    suggestion_markdowns = [get_suggestion_header_timestamp(suggestion_to_markdown_parts(suggestion, True)) for suggestion in suggestions]
    joined_suggestions = '\n\n'.join(suggestion_markdowns)
    if not query:
        return joined_suggestions
    relevant_joined_suggestions_prompt = f'Get at max {limit} relevant suggestions to this query: {query}\n\nHere are all the stored suggestions:\n{joined_suggestions}\n\n\nOutput the {limit} most relevant suggestions. Weight more recent (later) suggestions more.\n'
    relevant_joined_suggestions = run_completion_with_fallback(prompt=relevant_joined_suggestions_prompt, models=['main-mini'])
    return relevant_joined_suggestions

def get_suggestion_header_timestamp(suggestion):
    return suggestion['timestamp'] + '\n' + suggestion['header']



# File: backend/agents/data_analyst_group/tools/get_top_pages.py

from typing import List, Dict, Any, Optional

from datetime import datetime, timedelta

import typing_extensions

from tools.run_sitewiz_query import run_sitewiz_query

def get_top_pages_description(functions_module, extra_parameters):
    stream_key = extra_parameters.get('stream_key', 'None')
    name = 'get_top_pages'
    description = f'{name}: Retrieves statistics about the most visited pages including visit counts, duration, revenue, and errors.\n\n**Important Rules:**\n- Ensure valid stream_key is provided\n- Order results by specified metric\n- Handle pagination using limit parameter\n\n**Example Code:**\n```python\nfrom {functions_module} import get_top_pages\n\n# Get top 10 pages ordered by visit count\nresults = get_top_pages("{stream_key}", limit=10, order_by="visit_count")\n```\n'
    return (name, description)

def get_top_pages_given_context(stream_key: typing_extensions.Annotated[str, 'Stream key to use for the query']) -> typing_extensions.Annotated[Any, 'Function that returns page statistics']:

    def get_top_pages() -> typing_extensions.Annotated[List[Dict[str, Any]], 'List of dictionaries containing page statistics of page, visits']:
        limit = 100
        "\n        Get top pages with their statistics including visit count, duration, revenue, and error count.\n        \n        Args:\n            limit: Maximum number of results to return. Defaults to 50.\n            order_by: Field to order results by. Can be 'visit_count', 'total_duration', \n                     'total_revenue', or 'error_count'. Defaults to 'visit_count'.\n        \n        Returns:\n            List of dictionaries containing page statistics with keys:\n                - url (str): Base URL of the page\n                - visit_count (int): Number of unique session visits\n                - total_duration (int): Total time spent on page in milliseconds\n                - total_revenue (int): Total revenue generated from the page\n                - error_count (int): Number of errors recorded on the page\n        "
        valid_order_by = {'visit_count': 'visit_count DESC', 'total_duration': 'total_duration DESC', 'total_revenue': 'total_revenue DESC', 'error_count': 'error_count DESC'}
        end_time = int(datetime.now().timestamp() * 1000)
        start_time = int((datetime.now() - timedelta(days=7)).timestamp() * 1000)
        query = f"\n        SELECT\n            url,\n            visit_count\n        FROM\n            public.mv_most_visited_urls\n        WHERE\n            stream_key = '{stream_key}'\n        ORDER BY\n            visit_count DESC\n        LIMIT {limit};\n        "
        results = run_sitewiz_query(query)
        return results
    return get_top_pages

def get_top_pages() -> typing_extensions.Annotated[List[Dict[str, Any]], 'List of dictionaries containing page statistics of page, visits']:
    limit = 100
    "\n        Get top pages with their statistics including visit count, duration, revenue, and error count.\n        \n        Args:\n            limit: Maximum number of results to return. Defaults to 50.\n            order_by: Field to order results by. Can be 'visit_count', 'total_duration', \n                     'total_revenue', or 'error_count'. Defaults to 'visit_count'.\n        \n        Returns:\n            List of dictionaries containing page statistics with keys:\n                - url (str): Base URL of the page\n                - visit_count (int): Number of unique session visits\n                - total_duration (int): Total time spent on page in milliseconds\n                - total_revenue (int): Total revenue generated from the page\n                - error_count (int): Number of errors recorded on the page\n        "
    valid_order_by = {'visit_count': 'visit_count DESC', 'total_duration': 'total_duration DESC', 'total_revenue': 'total_revenue DESC', 'error_count': 'error_count DESC'}
    end_time = int(datetime.now().timestamp() * 1000)
    start_time = int((datetime.now() - timedelta(days=7)).timestamp() * 1000)
    query = f"\n        SELECT\n            url,\n            visit_count\n        FROM\n            public.mv_most_visited_urls\n        WHERE\n            stream_key = '{stream_key}'\n        ORDER BY\n            visit_count DESC\n        LIMIT {limit};\n        "
    results = run_sitewiz_query(query)
    return results



# File: backend/agents/data_analyst_group/tools/get_website.py

import requests

from PIL import Image

from io import BytesIO

from typing import Annotated, Literal

import os

import boto3

from pathlib import Path

import time

def download_s3_files(bucket: str, folder: str, destination: str):
    """Download files from S3 bucket to local workspace"""
    s3 = boto3.client('s3')
    paginator = s3.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=bucket, Prefix=folder):
        for obj in page.get('Contents', []):
            key = obj['Key']
            if not key.endswith('/'):
                relative_path = os.path.relpath(key, folder)
                file_path = os.path.join(destination, relative_path)
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                s3.download_file(bucket, key, file_path)

def remove_files(workspace_base: str):
    """Remove files in workspace"""
    for file in os.listdir(workspace_base):
        file_path = os.path.join(workspace_base, file)
        if os.path.isfile(file_path):
            os.remove(file_path)

def get_website_given_context(workspace_base: str, stream_key: str):

    def get_website(url: Annotated[str, 'The URL of the website to screenshot'], deviceType: Annotated[Literal['desktop', 'tablet', 'mobile'], 'The device type to render the website']) -> Annotated[str, 'Get website result string']:
        """
        Get HTML Snapshot of the specified URL.

        Args:
            url (str): The URL of the website to screenshot.
            deviceType (str): The device type to render the website.

        Returns:
            str: Get website result string
        """
        try:
            remove_files(workspace_base)
        except Exception as e:
            pass
        api_endpoint = 'https://lsruh6ekhnf6g5octogvi3vouu0vixrs.lambda-url.us-east-1.on.aws/'
        payload = {'url': url, 'deviceType': deviceType, 'streamKey': stream_key, 'outputs': [{'type': 'htmlSnapshot'}]}
        headers = {'Content-Type': 'application/json'}

        def make_request():
            response = requests.post(api_endpoint, headers=headers, json=payload)
            response.raise_for_status()
            return response
        for attempt in range(3):
            try:
                response = make_request()
                break
            except requests.exceptions.RequestException as e:
                if attempt == 2:
                    raise
                continue
        response = response.json()
        response_data = None
        if 'outputs' in response and len(response['outputs']) > 0:
            response_data = response['outputs'][0]
        else:
            return 'Invalid response format, download failed'
        if 'url' in response_data:
            url_parts = response_data['url'].split('/')
            bucket = url_parts[2].split('.')[0]
            folder = '/'.join(url_parts[3:-1])
            download_s3_files(bucket, folder, workspace_base)
            msg = "Downloaded website files, main file at '/index.html'"
            timestamp = str(int(time.time()))
            with open(os.path.join(workspace_base, 'sitewiz_change_info.json'), 'w') as f:
                f.write(f'{{"bucket": "{bucket}", "folder": "{folder}", "timestamp": "{timestamp}"}}')
            return msg
        else:
            return 'Invalid response format, download failed'
    return get_website

def get_website(url: Annotated[str, 'The URL of the website to screenshot'], deviceType: Annotated[Literal['desktop', 'tablet', 'mobile'], 'The device type to render the website']) -> Annotated[str, 'Get website result string']:
    """
        Get HTML Snapshot of the specified URL.

        Args:
            url (str): The URL of the website to screenshot.
            deviceType (str): The device type to render the website.

        Returns:
            str: Get website result string
        """
    try:
        remove_files(workspace_base)
    except Exception as e:
        pass
    api_endpoint = 'https://lsruh6ekhnf6g5octogvi3vouu0vixrs.lambda-url.us-east-1.on.aws/'
    payload = {'url': url, 'deviceType': deviceType, 'streamKey': stream_key, 'outputs': [{'type': 'htmlSnapshot'}]}
    headers = {'Content-Type': 'application/json'}

    def make_request():
        response = requests.post(api_endpoint, headers=headers, json=payload)
        response.raise_for_status()
        return response
    for attempt in range(3):
        try:
            response = make_request()
            break
        except requests.exceptions.RequestException as e:
            if attempt == 2:
                raise
            continue
    response = response.json()
    response_data = None
    if 'outputs' in response and len(response['outputs']) > 0:
        response_data = response['outputs'][0]
    else:
        return 'Invalid response format, download failed'
    if 'url' in response_data:
        url_parts = response_data['url'].split('/')
        bucket = url_parts[2].split('.')[0]
        folder = '/'.join(url_parts[3:-1])
        download_s3_files(bucket, folder, workspace_base)
        msg = "Downloaded website files, main file at '/index.html'"
        timestamp = str(int(time.time()))
        with open(os.path.join(workspace_base, 'sitewiz_change_info.json'), 'w') as f:
            f.write(f'{{"bucket": "{bucket}", "folder": "{folder}", "timestamp": "{timestamp}"}}')
        return msg
    else:
        return 'Invalid response format, download failed'

def make_request():
    response = requests.post(api_endpoint, headers=headers, json=payload)
    response.raise_for_status()
    return response



# File: backend/agents/data_analyst_group/tools/run_bigquery_query.py

import os

import json

import typing

from autogen_core.code_executor import with_requirements

import typing_extensions

import google.cloud.bigquery

import google.oauth2.service_account

import boto3

def run_bigquery_query_description(functions_module, extra_parameters):
    key = extra_parameters.get('key', 'bigquery/credentials/default')
    table = extra_parameters.get('table', 'bigquery-public-data.ga4_obfuscated_sample_ecommerce')
    name = 'run_bigquery_query'
    description = f"""{name}: Executes a SQL query in BigQuery using the specified dataset: {table}.\n\n**BigQuery Context:**\n- Dataset: {table}\n- Schema details may vary; verify by querying the dataset.\n\n**Important Rules:**\n- Ensure the query adheres to the known schema of {table}.\n- Ensure that credential_key='{key}' is used in every call to this function since this is used to authenticate with BigQuery.\n- Optimize queries for performance and accuracy.\n- Handle exceptions gracefully.\n\n**Example Code:**\n```python\nfrom {functions_module} import run_bigquery_query\n\nquery = f'''\n    SELECT table_name\n    FROM `{table}.INFORMATION_SCHEMA.TABLES`;\n'''\nresults = run_bigquery_query(query, credential_key=\"{key}", key="Tables in dataset") # Save the results under the key 'Tables in dataset'\nprint(results[:5])\n```\n"""
    return (name, description)

@with_requirements(python_packages=['google-cloud-bigquery', 'boto3'], global_imports=['google.cloud.bigquery', 'google.oauth2.service_account', 'boto3', 'typing_extensions', 'typing', 'json', 'os', 'datetime'])
def run_bigquery_query(query, credential_key='bigquery/credentials/default', key=None):

    def save_results(results, key):
        script_dir = os.path.dirname(__file__)
        results_dir = os.path.join(script_dir, 'results')
        os.makedirs(results_dir, exist_ok=True)
        results_file = os.path.join(results_dir, f'{key}.json')
        with open(results_file, 'w') as f:
            json.dump(results, f)
    key = ''.join([c if c.isalnum() else '_' for c in key]) if key else 'Temporary_Query'

    def fetch_secret(secret_name):
        session = boto3.session.Session()
        client = session.client(service_name='secretsmanager', region_name='us-east-1')
        get_secret_value_response = client.get_secret_value(SecretId=secret_name)
        return json.loads(get_secret_value_response['SecretString'])
    try:
        credentials_info = fetch_secret(credential_key)
        credentials = google.oauth2.service_account.Credentials.from_service_account_info(credentials_info)
        client = google.cloud.bigquery.Client(credentials=credentials, project=credentials.project_id)
        query_job = client.query(query)
        results = query_job.result()
        rows = [dict(row) for row in results]
        if key:
            save_results(rows, key)
        results_str = f"Query results (top 5) for key '{key}' (total {len(results)} rows):\n\n{results[:5]}"
        print(results_str)
        return rows
    except Exception as e:
        print(f'An error occurred while executing the BigQuery query: {e}')
        return []

def save_results(results, key):
    script_dir = os.path.dirname(__file__)
    results_dir = os.path.join(script_dir, 'results')
    os.makedirs(results_dir, exist_ok=True)
    results_file = os.path.join(results_dir, f'{key}.json')
    with open(results_file, 'w') as f:
        json.dump(results, f)

def fetch_secret(secret_name):
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name='us-east-1')
    get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    return json.loads(get_secret_value_response['SecretString'])



# File: backend/agents/data_analyst_group/tools/run_sitewiz_query.py

import os

import json

import psycopg2

import decimal

import datetime

from typing import List, Tuple, Any, Dict

from utils.secrets import fetch_secret

from autogen_core.code_executor import with_requirements

import typing_extensions

import requests

import typing

import boto3

def sample_code(functions_module, stream_key):
    return f"\n    Sample working code for how to execute queries and get aggregate metrics for insights.\n    As long as the instructions below are followed and queries follow the SQL schema, queries will work.\n\n```python\nimport pandas as pd\nfrom {functions_module} import run_sitewiz_query\nfrom datetime import datetime, timedelta\n\nstream_key = '{stream_key}'  # YOU MUST DEFINE THIS IN EACH CODE BLOCK WITHOUT EXCEPTION\n\npd.set_option('display.max_columns', None)  # Set this to print all columns\n\n# Define time range: past 7 days\nend_time = int(datetime.now().timestamp() * 1000)\nstart_time = int((datetime.now() - timedelta(days=7)).timestamp() * 1000)\n\n# Query to get session data overview grouped by date.\n# Note: Revenue and purchase are now sourced from ecommerce_metrics (joined on session_id),\n# and the materialized date column in session_recordings is used for grouping.\nquery = f'''\n    SELECT \n        sr.date AS date,\n        COUNT(DISTINCT s.session_id) AS total_sessions,\n        SUM(CASE WHEN em.purchase THEN 1 ELSE 0 END) AS total_purchases,\n        SUM(em.revenue) AS total_revenue,\n    FROM sessions s\n    JOIN session_recordings sr ON s.session_id = sr.session_id\n    LEFT JOIN ecommerce_metrics em ON s.session_id = em.session_id\n    WHERE s.stream_key = '{{stream_key}}'\n      AND sr.start_time >= {{start_time}}\n      AND sr.end_time <= {{end_time}}\n    GROUP BY sr.date\n    ORDER BY sr.date\n'''\nresults = run_sitewiz_query(query, 'Query to get session data overview grouped by date')\n\n# Define column names\ncolumns = ['date', 'total_sessions', 'total_purchases', 'total_revenue']\n\n# Create DataFrame with column names\ndf = pd.DataFrame(results, columns=columns)\nprint(df)\n```\n\nSample code for getting the top element clicked on a page:\n\n```python\nimport pandas as pd\nfrom {functions_module} import run_sitewiz_query\nfrom datetime import datetime, timedelta\n\nstream_key = '{stream_key}'  # YOU MUST DEFINE THIS IN EACH CODE BLOCK WITHOUT EXCEPTION\n\npd.set_option('display.max_columns', None)  # Set this to print all columns\n\nend_time = int(datetime.now().timestamp() * 1000)\nstart_time = int((datetime.now() - timedelta(days=7)).timestamp() * 1000)\n\n# Query to retrieve click data from heatmaps for the past week.\nquery_clicks = f'''\n    SELECT \n        h.xpath,\n        h.url,\n        COUNT(*) AS click_count,\n        COUNT(DISTINCT h.session_id) AS unique_sessions\n    FROM heatmaps h\n    WHERE h.stream_key = '{{stream_key}}'\n      AND h.timestamp >= {{start_time}}\n      AND h.timestamp <= {{end_time}}\n      AND h.type = 1\n    GROUP BY h.xpath, h.url\n    ORDER BY click_count DESC\n    LIMIT 50\n'''\nclick_results = run_sitewiz_query(query_clicks, 'Query to get click data')\nprint(click_results)\n```\n"

def run_sitewiz_query_description(functions_module, extra_parameters):
    stream_key = extra_parameters.get('stream_key', 'None')
    name = 'run_sitewiz_query'
    description = f'{name}: Uses python code to run a SQL query on the Sitewiz Analytics DB and optionally stores the results with a specified key.\n\nSQL Database Context:\n\n- Schema Information:\n```json\n{json.dumps(SCHEMA_INFO, indent=4)}\n```\n\n- Sample Code:\n{sample_code(functions_module, stream_key)}\n'
    return (name, description)

@with_requirements(python_packages=['boto3', 'datetime'], global_imports=['boto3', 'psycopg2', 'json', 'decimal', 'datetime', 'typing', 'typing_extensions', 'os'])
def run_sitewiz_query(query: typing_extensions.Annotated[str, 'A SQL query to execute.'], key: typing_extensions.Annotated[str, 'Easy to remember key to store result of query']=None) -> typing_extensions.Annotated[typing.List[tuple], 'Query results as a list of tuples.']:

    def is_running_locally() -> bool:
        """
        Determines if the Lambda function is running locally based on Serverless environment or local testing indicators.
        """
        return os.getenv('IS_LOCAL', 'false').lower() == 'true' or os.getenv('AWS_SAM_LOCAL', 'false').lower() == 'true' or 'AWS_LAMBDA_RUNTIME_API' not in os.environ
    key = ''.join([c if c.isalnum() else '_' for c in key]) if key else 'Temporary_Query'
    CACHE_DIR = os.path.join(os.path.dirname(__file__), 'task_query_cache')
    QUERY_TIMEOUT = 60000
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_key = str(hash(query))
    cache_file = os.path.join(CACHE_DIR, f'{cache_key}.json')
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                results = json.load(f)
                results_str = f'Query results (from cache) (top 5) (total {len(results)} rows):\n{results[:5]}'
                print(results_str)
                return results
        except (json.JSONDecodeError, IOError):
            pass

    def get_secret_fetch():
        if is_running_locally():
            secret_name = 'heatmap/credentials'
        else:
            secret_name = 'heatmap/credentials-fetch'
        region_name = 'us-east-1'
        session = boto3.session.Session()
        client = session.client(service_name='secretsmanager', region_name=region_name)
        get_secret_value_response = client.get_secret_value(SecretId=secret_name)
        return json.loads(get_secret_value_response['SecretString'])

    def get_db_connection():
        """Get database connection with optimizations"""
        secret = get_secret_fetch()
        conn = psycopg2.connect(dbname=secret['dbname'], user=secret['username'], password=secret['password'], host=secret['host'], port=secret['port'], options=f'-c statement_timeout={QUERY_TIMEOUT}')
        return conn

    def execute_query(query: str):
        """Execute a query and return the results"""
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(query)
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        return convert_decimal(results)

    def convert_decimal(obj):
        if isinstance(obj, decimal.Decimal):
            return float(obj)
        if isinstance(obj, list):
            return [convert_decimal(item) for item in obj]
        if isinstance(obj, tuple):
            return tuple((convert_decimal(item) for item in obj))
        if isinstance(obj, datetime.date):
            return obj.isoformat()
        return obj
    if 'LIKE' in query:
        return 'LIKE operator is not allowed in this task. Use specific xpaths and URLs verified to work.'
    results = execute_query(query)
    try:
        with open(cache_file, 'w') as f:
            json.dump(results, f)
    except IOError:
        print('Warning: Failed to cache query results')
    results_str = f"Query results (top 5) for key '{key}' (total {len(results)} rows):\n\n{results[:5]}"
    print(results_str)
    return results

def is_running_locally() -> bool:
    """
        Determines if the Lambda function is running locally based on Serverless environment or local testing indicators.
        """
    return os.getenv('IS_LOCAL', 'false').lower() == 'true' or os.getenv('AWS_SAM_LOCAL', 'false').lower() == 'true' or 'AWS_LAMBDA_RUNTIME_API' not in os.environ

def get_secret_fetch():
    if is_running_locally():
        secret_name = 'heatmap/credentials'
    else:
        secret_name = 'heatmap/credentials-fetch'
    region_name = 'us-east-1'
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region_name)
    get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    return json.loads(get_secret_value_response['SecretString'])

def get_db_connection():
    """Get database connection with optimizations"""
    secret = get_secret_fetch()
    conn = psycopg2.connect(dbname=secret['dbname'], user=secret['username'], password=secret['password'], host=secret['host'], port=secret['port'], options=f'-c statement_timeout={QUERY_TIMEOUT}')
    return conn

def execute_query(query: str):
    """Execute a query and return the results"""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute(query)
    results = cursor.fetchall()
    cursor.close()
    conn.close()
    return convert_decimal(results)

def convert_decimal(obj):
    if isinstance(obj, decimal.Decimal):
        return float(obj)
    if isinstance(obj, list):
        return [convert_decimal(item) for item in obj]
    if isinstance(obj, tuple):
        return tuple((convert_decimal(item) for item in obj))
    if isinstance(obj, datetime.date):
        return obj.isoformat()
    return obj



# File: backend/agents/data_analyst_group/tools/run_web_agent.py

from autogen_core.code_executor import with_requirements

import typing_extensions

import typing

import json

import requests

def run_web_agent_description(functions_module, extra_parameters):
    name = 'run_web_agent'
    description = f'{name}: Makes an API call to run a web agent task with specified URL and instruction.\n\n**Important Rules:**\n- Provide both instruction and URL parameters\n- Handle API response and errors gracefully\n\n**Example Code:**\n```python\nfrom {functions_module} import run_web_agent\n\nresults = run_web_agent(\n    instruction="Find problems in the navigation bar",\n    url="https://example.com"\n)\nprint(results)'
    return (name, description)

@with_requirements(python_packages=['requests'], global_imports=['json', 'requests', 'typing', 'typing_extensions'])
def run_web_agent(instruction, url):
    api_url = 'https://9hbch8s6dt.us-east-1.awsapprunner.com/run_task'
    payload = json.dumps({'instruction': instruction, 'url': url})
    headers = {'Content-Type': 'application/json'}
    try:
        response = requests.request('POST', api_url, headers=headers, data=payload)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        error_message = f'API call failed: {str(e)}'
        print(error_message)
        return {'error': error_message}
    except json.JSONDecodeError as e:
        error_message = f'Failed to decode API response: {str(e)}'
        print(error_message)
        return {'error': error_message}



# File: backend/agents/data_analyst_group/tools/save_graph.py

import boto3

import matplotlib.pyplot as plt

import os

from datetime import datetime

import uuid

from typing_extensions import Annotated

import os

import boto3

import matplotlib.pyplot as plt

from datetime import datetime

import uuid

from typing_extensions import Annotated

import typing

from autogen_core.code_executor import with_requirements

def save_graph_description(functions_module, extra_parameters):
    stream_key = extra_parameters.get('stream_key', 'None')
    name = 'save_graph'
    description = f'{name}: Saves a matplotlib figure to S3 and returns the public URL.\n\n**Important Rules:**\n- Provide a valid matplotlib figure\n- Handle exceptions gracefully\n\n**Example Code:**\n```python\nfrom {functions_module} import save_graph\nimport matplotlib.pyplot as plt\n\n# Create a sample plot\nplt.figure()\nplt.plot([1, 2, 3], [1, 2, 3])\nfigure = plt.gcf()\n\n# Save the graph and get URL\nurl = save_graph(figure, stream_key="{stream_key}")\nprint(f"Graph saved at: <url>")\n```\n'
    return (name, description)

@with_requirements(python_packages=['boto3', 'matplotlib'], global_imports=['boto3', 'matplotlib.pyplot', 'uuid', 'typing_extensions', 'os'])
def save_graph(figure, stream_key):
    """Save a matplotlib figure to S3"""
    try:
        s3_client = boto3.client('s3')
        unique_id = uuid.uuid4()
        temp_file = f'/tmp/{unique_id}.png'
        bucket_name = 'sitewiz-websites'
        s3_key = f'graphs/{stream_key}/{unique_id}.png'
        figure.savefig(temp_file)
        s3_client.upload_file(temp_file, bucket_name, s3_key, ExtraArgs={'ContentType': 'image/png'})
        if os.path.exists(temp_file):
            os.remove(temp_file)
        url = f'https://{bucket_name}.s3.amazonaws.com/{s3_key}'
        return url
    except Exception as e:
        return f'Error saving graph: {str(e)}'



# File: backend/agents/data_analyst_group/tools/store_design.py

import sys

import os

from tools.run_sitewiz_query import run_sitewiz_query, run_sitewiz_query_description

from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor

from typing_extensions import Annotated

from datetime import datetime

import json

import re

from utils.functions import get_dynamodb_client

from prompts.design_prompts import design_questions

from utils.evaluation import run_evaluation

def validate_implementation_details(details):
    """Validate implementation details structure and content."""
    if not isinstance(details, dict):
        return (False, 'Implementation details must be a dictionary')
    required_fields = ['urls', 'selectors', 'technical_requirements']
    for field in required_fields:
        if field not in details:
            return (False, f'Missing required field: {field}')
    for url in details['urls']:
        try:
            re.compile(url)
        except re.error:
            return (False, f'Invalid URL regex pattern: {url}')
    return (True, 'Implementation details are valid')

def store_design_given_context(stream_key, business_context, tech_stack):

    def store_design(suggestion: Annotated[str, 'The design suggestion description'], rationale: Annotated[str, 'The rationale behind the suggestion'], implementation_details: Annotated[dict, 'Dictionary containing URLs, selectors, and technical requirements'], current_design: Annotated[str, 'Description of current design state']) -> Annotated[tuple[str, bool], 'Result message and success boolean']:
        try:
            is_valid, validation_msg = validate_implementation_details(implementation_details)
            if not is_valid:
                return (f'The storage is not successful. Error: {validation_msg}', False)
            documents = {'suggestion': {'type': 'text', 'content': suggestion, 'description': 'Design Suggestion'}, 'rationale': {'type': 'text', 'content': rationale, 'description': 'Design Rationale'}, 'implementation_details': {'type': 'text', 'content': json.dumps(implementation_details), 'description': 'Implementation Details'}, 'current_design': {'type': 'text', 'content': current_design, 'description': 'Current Design State'}, 'business_context': {'type': 'text', 'content': business_context, 'description': 'Business Context'}, 'tech_stack': {'type': 'text', 'content': tech_stack, 'description': 'Technical Stack Information'}, 'urls': {'type': 'text', 'content': json.dumps(implementation_details['urls']), 'description': 'Target URLs'}, 'selectors': {'type': 'text', 'content': json.dumps(implementation_details['selectors']), 'description': 'Element Selectors'}, 'technical_requirements': {'type': 'text', 'content': json.dumps(implementation_details['technical_requirements']), 'description': 'Technical Requirements'}}
            validation_results = run_evaluation(documents, design_questions)
            for result in validation_results:
                if result.answer == 'No':
                    return (f'Design validation failed: {result.explanation}. Please fix this issue', False)
            dynamodb = get_dynamodb_client()
            try:
                suggestion_id = f'{stream_key}-{datetime.now().strftime('%Y%m%d%H%M%S')}'
                item = {'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}, 'suggestion': {'S': suggestion}, 'rationale': {'S': rationale}, 'implementationDetails': {'S': json.dumps(implementation_details)}, 'currentDesign': {'S': current_design}, 'timestamp': {'N': str(int(datetime.now().timestamp() * 1000))}}
                dynamodb.put_item(TableName='website-design-suggestions', Item=item)
                return (f'Successfully stored design suggestion with ID: {suggestion_id}', True)
            except Exception as e:
                return (f'Error storing design suggestion in DynamoDB: {e}', False)
        except Exception as e:
            import traceback
            traceback.print_exc()
            return (f'Error in store_design: {e}', False)
    return store_design

def remove_design_given_context(stream_key):

    def remove_design(suggestion_id: Annotated[str, 'ID of the design suggestion to remove']) -> Annotated[str, 'Result message']:
        try:
            dynamodb = get_dynamodb_client()
            response = dynamodb.get_item(TableName='website-design-suggestions', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}})
            if 'Item' not in response:
                return f'No design suggestion found with ID: {suggestion_id}'
            dynamodb.delete_item(TableName='website-design-suggestions', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}})
            return f'Successfully removed design suggestion: {suggestion_id}'
        except Exception as e:
            return f'Error removing design suggestion: {e}'
    return remove_design

def store_design(suggestion: Annotated[str, 'The design suggestion description'], rationale: Annotated[str, 'The rationale behind the suggestion'], implementation_details: Annotated[dict, 'Dictionary containing URLs, selectors, and technical requirements'], current_design: Annotated[str, 'Description of current design state']) -> Annotated[tuple[str, bool], 'Result message and success boolean']:
    try:
        is_valid, validation_msg = validate_implementation_details(implementation_details)
        if not is_valid:
            return (f'The storage is not successful. Error: {validation_msg}', False)
        documents = {'suggestion': {'type': 'text', 'content': suggestion, 'description': 'Design Suggestion'}, 'rationale': {'type': 'text', 'content': rationale, 'description': 'Design Rationale'}, 'implementation_details': {'type': 'text', 'content': json.dumps(implementation_details), 'description': 'Implementation Details'}, 'current_design': {'type': 'text', 'content': current_design, 'description': 'Current Design State'}, 'business_context': {'type': 'text', 'content': business_context, 'description': 'Business Context'}, 'tech_stack': {'type': 'text', 'content': tech_stack, 'description': 'Technical Stack Information'}, 'urls': {'type': 'text', 'content': json.dumps(implementation_details['urls']), 'description': 'Target URLs'}, 'selectors': {'type': 'text', 'content': json.dumps(implementation_details['selectors']), 'description': 'Element Selectors'}, 'technical_requirements': {'type': 'text', 'content': json.dumps(implementation_details['technical_requirements']), 'description': 'Technical Requirements'}}
        validation_results = run_evaluation(documents, design_questions)
        for result in validation_results:
            if result.answer == 'No':
                return (f'Design validation failed: {result.explanation}. Please fix this issue', False)
        dynamodb = get_dynamodb_client()
        try:
            suggestion_id = f'{stream_key}-{datetime.now().strftime('%Y%m%d%H%M%S')}'
            item = {'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}, 'suggestion': {'S': suggestion}, 'rationale': {'S': rationale}, 'implementationDetails': {'S': json.dumps(implementation_details)}, 'currentDesign': {'S': current_design}, 'timestamp': {'N': str(int(datetime.now().timestamp() * 1000))}}
            dynamodb.put_item(TableName='website-design-suggestions', Item=item)
            return (f'Successfully stored design suggestion with ID: {suggestion_id}', True)
        except Exception as e:
            return (f'Error storing design suggestion in DynamoDB: {e}', False)
    except Exception as e:
        import traceback
        traceback.print_exc()
        return (f'Error in store_design: {e}', False)

def remove_design(suggestion_id: Annotated[str, 'ID of the design suggestion to remove']) -> Annotated[str, 'Result message']:
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.get_item(TableName='website-design-suggestions', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}})
        if 'Item' not in response:
            return f'No design suggestion found with ID: {suggestion_id}'
        dynamodb.delete_item(TableName='website-design-suggestions', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}})
        return f'Successfully removed design suggestion: {suggestion_id}'
    except Exception as e:
        return f'Error removing design suggestion: {e}'

import traceback



# File: backend/agents/data_analyst_group/tools/store_insight.py

from typing_extensions import Annotated, TypedDict

from datetime import datetime

import boto3

from pydantic import BaseModel

import json

from prompts.insights_prompts import insight_criteria

from utils.evaluation import run_evaluation, store_evaluations, interpret_evaluations

from utils.functions import get_dynamodb_client, process_data_statement, save_results, insight_to_markdown, get_previous_insights

from typing import List, Dict, Any, Tuple

import math

from autogen_core.code_executor import CodeBlock

from autogen_core import CancellationToken

import asyncio

from tools.run_sitewiz_query import run_sitewiz_query, run_sitewiz_query_description

from tools.run_bigquery_query import run_bigquery_query, run_bigquery_query_description

from tools.save_graph import save_graph, save_graph_description

from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor

from pathlib import Path

import tempfile

from prompts.insights_prompts import insight_questions

def verify_derivations(derivations, executor):
    for derivation in derivations:
        expr = derivation['derivation']
        variable_name = derivation['variable_name']
        exit_code, res = asyncio.run(evaluate_derivation(expr, variable_name, executor))
        derivation['result'] = res
        if exit_code == 1:
            raise Exception(res)

def store_insight_given_context(stream_key, executor):

    def evaluate_insight(insight_data, cleaned_data_statement, max_tries=3) -> tuple[int, str]:
        print(insight_data)
        insight_markdown = insight_to_markdown(insight_data)
        previous_insights, previous_insight_markdowns = get_previous_insights(stream_key)
        previous_insight_markdowns = '\n\n'.join(previous_insight_markdowns)
        documents = {'data_statement': {'type': 'text', 'content': insight_data['data_statement'], 'description': 'Insight data statement'}, 'problem_statement': {'type': 'text', 'content': insight_data['problem_statement'], 'description': 'Problem statement'}, 'business_objective': {'type': 'text', 'content': insight_data['business_objective'], 'description': 'Business objective'}, 'hypothesis': {'type': 'text', 'content': insight_data['hypothesis'], 'description': 'Hypothesis for change'}, 'frequency': {'type': 'text', 'content': str(insight_data['frequency']), 'description': 'Occurrence count (%)'}, 'severity': {'type': 'text', 'content': str(insight_data['severity']), 'description': 'Severity rating (1-5)'}, 'severity_reasoning': {'type': 'text', 'content': insight_data['severity_reasoning'], 'description': 'Reasoning for severity'}, 'confidence': {'type': 'text', 'content': str(insight_data['confidence']), 'description': 'Confidence score (0-1)'}, 'confidence_reasoning': {'type': 'text', 'content': insight_data['confidence_reasoning'], 'description': 'Reasoning for confidence'}, 'derivation': {'type': 'text', 'content': json.dumps(insight_data['derivation']), 'description': 'Variable derivations'}, 'variables': {'type': 'text', 'content': json.dumps(insight_data['variables']), 'description': 'Variable definitions'}, 'insight_markdown': {'type': 'text', 'content': insight_markdown, 'description': 'Insight in markdown format'}, 'previous_insight_markdowns': {'type': 'text', 'content': previous_insight_markdowns, 'description': 'All previous insights in markdown format'}, 'query_documentation': {'type': 'text', 'content': 'Query documentation', 'description': 'Documentation for executed queries'}}
        validation_results = run_evaluation(documents, insight_questions)
        validation_message, pass_validation = interpret_evaluations(validation_results, 'Insight validation failed.')
        if not pass_validation:
            return (0, validation_message, validation_results)
        return (100, 'Validation passed. ' + validation_message, validation_results)

    def verify_okr(okr_name):
        dynamodb = get_dynamodb_client()
        response = dynamodb.get_item(TableName='website-okrs', Key={'streamKey': {'S': stream_key}, 'name': {'S': okr_name}})
        if 'Item' not in response:
            return (f"Could not find a verified OKR with name '{okr_name}'", False)
        return ('Verified OKR', True)

    def store_into_dynamodb(insight_data, evaluation, cleaned_data_statement, okr_name, timestamp) -> tuple[str, bool]:
        try:
            insight = Insight(**insight_data)
            derivation = json.dumps([d.model_dump() for d in insight.derivation])
            variables = json.dumps([v.model_dump() for v in insight.variables])
            verify_message, verified_okr = verify_okr(okr_name)
            if not verified_okr:
                return (verify_message, False)
            dynamodb = get_dynamodb_client()
            item = {'streamKey': {'S': stream_key}, 'okr_name': {'S': okr_name}, 'timestamp': {'N': str(timestamp)}, 'data_statement': {'S': insight.data_statement}, 'problem_statement': {'S': insight.problem_statement}, 'business_objective': {'S': insight.business_objective}, 'hypothesis': {'S': insight.hypothesis}, 'frequency': {'N': str(insight.frequency)}, 'severity': {'N': str(insight.severity)}, 'severity_reasoning': {'S': insight.severity_reasoning}, 'confidence': {'N': str(insight.confidence)}, 'confidence_reasoning': {'S': insight.confidence_reasoning}, 'derivation': {'S': derivation}, 'evaluation': {'N': str(evaluation)}, 'variables': {'S': variables}, 'verified': {'BOOL': True}}
            dynamodb.put_item(TableName='website-insights', Item=item)
            save_results('insights', f'Insight stored at {str(timestamp)}.\nCleaned data statement: {cleaned_data_statement}\nRaw values: {json.dumps(insight_data, indent=4)}')
            output = f'Insight stored at {str(timestamp)}.\nCleaned data statement:\n{cleaned_data_statement}'
            print(output)
            return (output, True)
        except Exception as e:
            error_msg = f'Error storing insight: {e}'
            return (error_msg, False)

    def store_insight(insight_data: Annotated[Insight, 'Insight data to be stored; must be connected to a verified OKR'], okr_name: Annotated[str, 'The name of the OKR this insight is meant to improve']) -> Annotated[tuple[str, bool], 'Result message and success boolean']:
        print('OKR Name:', okr_name)
        verify_message, verified_okr = verify_okr(okr_name)
        if not verified_okr:
            return (verify_message, False)
        if 'derivation' not in insight_data or not insight_data['derivation']:
            return ('Derivation is required for the insight to be actionable', False)
        cleaned_data_statement = None
        try:
            verify_derivations(insight_data['derivation'], executor)
            cleaned_data_statement = process_data_statement(insight_data['data_statement'], insight_data['variables'], insight_data['derivation'], [])
            print('Cleaned data statement:', cleaned_data_statement)
        except Exception as e:
            output = f'Error processing data statement: {e}'
            print(output)
            return (output, False)
        evaluation, reasoning, validation_results = evaluate_insight(insight_data, cleaned_data_statement)
        if evaluation == 0:
            output = f'Insight evaluation did not pass. Reasoning:\n{reasoning}\n Please have the python analyst fix this issue. TERMINATE'
            print(output)
            return (output, False)
        timestamp = int(datetime.now().timestamp() * 1000)
        message, success = store_into_dynamodb(insight_data, evaluation, cleaned_data_statement, okr_name, timestamp)
        if success:
            store_evaluations(stream_key, f'insight#{timestamp}', validation_results)
        return (message + '\n\n' + reasoning, success)
    return store_insight

def evaluate_insight(insight_data, cleaned_data_statement, max_tries=3) -> tuple[int, str]:
    print(insight_data)
    insight_markdown = insight_to_markdown(insight_data)
    previous_insights, previous_insight_markdowns = get_previous_insights(stream_key)
    previous_insight_markdowns = '\n\n'.join(previous_insight_markdowns)
    documents = {'data_statement': {'type': 'text', 'content': insight_data['data_statement'], 'description': 'Insight data statement'}, 'problem_statement': {'type': 'text', 'content': insight_data['problem_statement'], 'description': 'Problem statement'}, 'business_objective': {'type': 'text', 'content': insight_data['business_objective'], 'description': 'Business objective'}, 'hypothesis': {'type': 'text', 'content': insight_data['hypothesis'], 'description': 'Hypothesis for change'}, 'frequency': {'type': 'text', 'content': str(insight_data['frequency']), 'description': 'Occurrence count (%)'}, 'severity': {'type': 'text', 'content': str(insight_data['severity']), 'description': 'Severity rating (1-5)'}, 'severity_reasoning': {'type': 'text', 'content': insight_data['severity_reasoning'], 'description': 'Reasoning for severity'}, 'confidence': {'type': 'text', 'content': str(insight_data['confidence']), 'description': 'Confidence score (0-1)'}, 'confidence_reasoning': {'type': 'text', 'content': insight_data['confidence_reasoning'], 'description': 'Reasoning for confidence'}, 'derivation': {'type': 'text', 'content': json.dumps(insight_data['derivation']), 'description': 'Variable derivations'}, 'variables': {'type': 'text', 'content': json.dumps(insight_data['variables']), 'description': 'Variable definitions'}, 'insight_markdown': {'type': 'text', 'content': insight_markdown, 'description': 'Insight in markdown format'}, 'previous_insight_markdowns': {'type': 'text', 'content': previous_insight_markdowns, 'description': 'All previous insights in markdown format'}, 'query_documentation': {'type': 'text', 'content': 'Query documentation', 'description': 'Documentation for executed queries'}}
    validation_results = run_evaluation(documents, insight_questions)
    validation_message, pass_validation = interpret_evaluations(validation_results, 'Insight validation failed.')
    if not pass_validation:
        return (0, validation_message, validation_results)
    return (100, 'Validation passed. ' + validation_message, validation_results)

def verify_okr(okr_name):
    dynamodb = get_dynamodb_client()
    response = dynamodb.get_item(TableName='website-okrs', Key={'streamKey': {'S': stream_key}, 'name': {'S': okr_name}})
    if 'Item' not in response:
        return (f"Could not find a verified OKR with name '{okr_name}'", False)
    return ('Verified OKR', True)

def store_into_dynamodb(insight_data, evaluation, cleaned_data_statement, okr_name, timestamp) -> tuple[str, bool]:
    try:
        insight = Insight(**insight_data)
        derivation = json.dumps([d.model_dump() for d in insight.derivation])
        variables = json.dumps([v.model_dump() for v in insight.variables])
        verify_message, verified_okr = verify_okr(okr_name)
        if not verified_okr:
            return (verify_message, False)
        dynamodb = get_dynamodb_client()
        item = {'streamKey': {'S': stream_key}, 'okr_name': {'S': okr_name}, 'timestamp': {'N': str(timestamp)}, 'data_statement': {'S': insight.data_statement}, 'problem_statement': {'S': insight.problem_statement}, 'business_objective': {'S': insight.business_objective}, 'hypothesis': {'S': insight.hypothesis}, 'frequency': {'N': str(insight.frequency)}, 'severity': {'N': str(insight.severity)}, 'severity_reasoning': {'S': insight.severity_reasoning}, 'confidence': {'N': str(insight.confidence)}, 'confidence_reasoning': {'S': insight.confidence_reasoning}, 'derivation': {'S': derivation}, 'evaluation': {'N': str(evaluation)}, 'variables': {'S': variables}, 'verified': {'BOOL': True}}
        dynamodb.put_item(TableName='website-insights', Item=item)
        save_results('insights', f'Insight stored at {str(timestamp)}.\nCleaned data statement: {cleaned_data_statement}\nRaw values: {json.dumps(insight_data, indent=4)}')
        output = f'Insight stored at {str(timestamp)}.\nCleaned data statement:\n{cleaned_data_statement}'
        print(output)
        return (output, True)
    except Exception as e:
        error_msg = f'Error storing insight: {e}'
        return (error_msg, False)

def store_insight(insight_data: Annotated[Insight, 'Insight data to be stored; must be connected to a verified OKR'], okr_name: Annotated[str, 'The name of the OKR this insight is meant to improve']) -> Annotated[tuple[str, bool], 'Result message and success boolean']:
    print('OKR Name:', okr_name)
    verify_message, verified_okr = verify_okr(okr_name)
    if not verified_okr:
        return (verify_message, False)
    if 'derivation' not in insight_data or not insight_data['derivation']:
        return ('Derivation is required for the insight to be actionable', False)
    cleaned_data_statement = None
    try:
        verify_derivations(insight_data['derivation'], executor)
        cleaned_data_statement = process_data_statement(insight_data['data_statement'], insight_data['variables'], insight_data['derivation'], [])
        print('Cleaned data statement:', cleaned_data_statement)
    except Exception as e:
        output = f'Error processing data statement: {e}'
        print(output)
        return (output, False)
    evaluation, reasoning, validation_results = evaluate_insight(insight_data, cleaned_data_statement)
    if evaluation == 0:
        output = f'Insight evaluation did not pass. Reasoning:\n{reasoning}\n Please have the python analyst fix this issue. TERMINATE'
        print(output)
        return (output, False)
    timestamp = int(datetime.now().timestamp() * 1000)
    message, success = store_into_dynamodb(insight_data, evaluation, cleaned_data_statement, okr_name, timestamp)
    if success:
        store_evaluations(stream_key, f'insight#{timestamp}', validation_results)
    return (message + '\n\n' + reasoning, success)



# File: backend/agents/data_analyst_group/tools/store_okr.py

from tools.run_sitewiz_query import run_sitewiz_query, run_sitewiz_query_description

from tools.run_bigquery_query import run_bigquery_query, run_bigquery_query_description

from tools.save_graph import save_graph, save_graph_description

from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor

from pathlib import Path

import tempfile

from prompts.okr_prompts import okr_questions, okr_criteria

from typing_extensions import Annotated

from datetime import datetime

import json

from utils.functions import get_dynamodb_client, get_all_okrs_markdown, okr_to_markdown

from pydantic import BaseModel

from typing import List

from autogen_core.code_executor import CodeBlock

from autogen_core import CancellationToken

import asyncio

from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor

from pathlib import Path

import ast

from tools.run_sitewiz_query import run_sitewiz_query_description

from utils.evaluation import run_evaluation, interpret_evaluations, get_reach, store_evaluations

def validate_code(code: str) -> (str, bool):
    try:
        tree = ast.parse(code)
    except SyntaxError as e:
        return (str(e), False)
    node = None
    for tree_node in tree.body:
        if isinstance(tree_node, ast.FunctionDef):
            node = tree_node
            break
    if isinstance(node, ast.FunctionDef):
        node.name = 'calculate_metrics'
        function_code = ast.unparse(node)
        print('Extracted Function:\n', function_code)
        return (function_code, True)
    else:
        return ('calculate_metrics is not the top-level function', False)

def store_okr_given_context(stream_key, executor, business_context):

    def store_okr(code: Annotated[str, 'The calculate_metrics function code'], reach_code: Annotated[str, 'The function code to calculate reach'], queries: Annotated[str, 'The SQL queries run by the code to fetch data from the database'], name: Annotated[str, 'Name of the OKR'], description: Annotated[str, 'Description of the OKR being tracked']) -> Annotated[tuple[str, bool], 'Result message and success boolean']:
        try:
            main_message = 'Please return to the python analyst to fix all the issues and verify the code works and formatted in the example provided before returning to store the okr. We got this error:\n'
            print('storing okr code')
            print(code)
            output_dict, is_valid_code, original_reach_code = get_reach(reach_code, executor, stream_key)
            if not is_valid_code:
                output = str(output_dict)
                return (main_message + output, False)
            reach = output_dict['reach']
            code_validation_output, is_valid_code = validate_code(code)
            if not is_valid_code:
                return (main_message + "The storage is not successful. Error: The provided code must contain only the 'calculate_metrics' function definition. " + code_validation_output, False)
            original_code = code
            before_code = f"""# you must use these exact imports in your code\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom datetime import timedelta\nfrom functions import run_sitewiz_query \nfrom typing import TypedDict, List, Tuple\n\nclass MetricOutput(TypedDict):\n    Metric: str\n    Description: str\n    start_date: str\n    end_date: str\n    values: List[Tuple[str, float]]\n\nstream_key = '{stream_key}'\n\n# Get yesterday's date as end_date\nend_date = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime("%Y-%m-%d")\n# Get date 6 days before end_date as  start_date\nstart_date = (datetime.datetime.now() - datetime.timedelta(days=7)).strftime("%Y-%m-%d")\n\nstart_time = int(datetime.datetime.strptime(start_date, "%Y-%m-%d").timestamp())\nend_time = int(datetime.datetime.strptime(end_date, "%Y-%m-%d").timestamp())"""
            end_code = 'output = calculate_metrics(start_date, end_date)\nprint("Calculate Metrics Function Output:")\nprint(output)'
            print(code_validation_output)
            code = before_code + '\n' + code_validation_output + '\n' + end_code
            exit_code, result = asyncio.run(evaluate_code(code, executor))
            print(result)
            if exit_code == 1:
                return (main_message + 'The storage is not successful. The code did not execute successfully with exit code 1. Please have the python analyst fix this error: \n' + result, False)
            try:
                output_start = result.rfind('Calculate Metrics Function Output:')
                if output_start != -1:
                    output_text = result[output_start:].split('\n', 1)[1].strip()
                    output_dict = eval(output_text)
                    formatted_output = f'\nMetric: {output_dict['Metric']}\nDescription: {output_dict['Description']}\nDate Range: {output_dict['start_date']} to {output_dict['end_date']}\nValues: {output_dict['values']}\n'
                    print('\nExtracted metrics:')
                    print(formatted_output)
                    if len(output_dict['values']) < 3:
                        raise Exception(f"There are {len(output_dict['values'])} in the values array. There should be 7 for a week's worth of data")
                    if len(set((str(v) for v in output_dict['values']))) == 1:
                        raise Exception('All the values in the values array are the same. That means it is likely not computed correctly or it is not a useful metric. Pursue another direction')
            except Exception as e:
                return (main_message + f"The storage is not successful. The code had trouble extracting metrics with error: '{e}'. \nPlease have the python analyst fix this error. Here was the output of the code: \n{result}", False)
            query_description_name, query_documentation = run_sitewiz_query_description('functions', {'stream_key': stream_key})
            okr_markdown = okr_to_markdown({'output': output_text})
            prev_okr_markdowns = get_all_okrs_markdown(stream_key)
            documents = {'name': {'type': 'text', 'content': name, 'description': 'OKR Name'}, 'description': {'type': 'text', 'content': description, 'description': 'OKR Description'}, 'okr_markdown': {'type': 'text', 'content': okr_markdown, 'description': 'OKR Markdown'}, 'prev_okr_markdowns': {'type': 'text', 'content': prev_okr_markdowns, 'description': 'All previous OKR Markdowns'}, 'okr_criteria': {'type': 'text', 'content': okr_criteria, 'description': 'OKR Criteria'}, 'code': {'type': 'text', 'content': original_code, 'description': 'Python function for OKR computation'}, 'queries': {'type': 'text', 'content': queries, 'description': 'SQL queries used in the function'}, 'query_execution_output': {'type': 'text', 'content': output_text, 'description': 'Execution output of the SQL queries'}, 'business_context': {'type': 'text', 'content': business_context, 'description': 'Contextual information on business relevance'}, 'query_documentation': {'type': 'text', 'content': query_documentation, 'description': 'Documentation for the queries executed'}}
            validation_results = run_evaluation(documents, okr_questions)
            validation_message, pass_validation = interpret_evaluations(validation_results, main_message + 'OKR validation failed so OKR is not stored')
            if not pass_validation:
                return (validation_message, False)
            dynamodb = get_dynamodb_client()
            try:
                response = dynamodb.get_item(TableName='website-okrs', Key={'streamKey': {'S': stream_key}, 'name': {'S': name}})
                timestamp = str(int(datetime.now().timestamp() * 1000))
                if 'Item' in response:
                    dynamodb.update_item(TableName='website-okrs', Key={'streamKey': {'S': stream_key}, 'name': {'S': name}}, UpdateExpression='SET code = :code, description = :desc, timestamp = :ts, output = :output, reach_code = :reach_code, reach = :reach, verified = :verified', ExpressionAttributeValues={':code': {'S': code_validation_output}, ':desc': {'S': description}, ':ts': {'N': timestamp}, ':output': {'S': output_text}, ':reach_code': {'S': original_reach_code}, ':reach': {'N': str(reach)}, ':verified': {'BOOL': True}})
                else:
                    dynamodb.put_item(TableName='website-okrs', Item={'streamKey': {'S': stream_key}, 'name': {'S': name}, 'code': {'S': code_validation_output}, 'description': {'S': description}, 'timestamp': {'N': timestamp}, 'output': {'S': output_text}, 'reach_code': {'S': original_reach_code}, 'reach': {'N': str(reach)}, 'verified': {'BOOL': True}})
                store_evaluations(stream_key, f'okr#{name}#{timestamp}', validation_results)
                return (f"OKR stored with name '{name}'\n{validation_message}", True)
            except Exception as e:
                return (main_message + f'Error storing OKR code in DynamoDB: {e}', False)
        except Exception as e:
            import traceback
            traceback.print_exc()
            return (main_message + f'Error in store_okr: {e}', False)
    return store_okr

def remove_okr_given_context(stream_key):

    def remove_okr(name: Annotated[str, 'Name of the OKR to remove']) -> Annotated[str, 'Result message']:
        try:
            dynamodb = get_dynamodb_client()
            response = dynamodb.get_item(TableName='website-okrs', Key={'streamKey': {'S': stream_key}, 'name': {'S': name}})
            if 'Item' not in response:
                return f'No OKR found with name: {name}'
            dynamodb.delete_item(TableName='website-okrs', Key={'streamKey': {'S': stream_key}, 'name': {'S': name}})
            return f'Successfully removed OKR: {name}'
        except Exception as e:
            return f'Error removing OKR: {e}'
    return remove_okr

def store_okr(code: Annotated[str, 'The calculate_metrics function code'], reach_code: Annotated[str, 'The function code to calculate reach'], queries: Annotated[str, 'The SQL queries run by the code to fetch data from the database'], name: Annotated[str, 'Name of the OKR'], description: Annotated[str, 'Description of the OKR being tracked']) -> Annotated[tuple[str, bool], 'Result message and success boolean']:
    try:
        main_message = 'Please return to the python analyst to fix all the issues and verify the code works and formatted in the example provided before returning to store the okr. We got this error:\n'
        print('storing okr code')
        print(code)
        output_dict, is_valid_code, original_reach_code = get_reach(reach_code, executor, stream_key)
        if not is_valid_code:
            output = str(output_dict)
            return (main_message + output, False)
        reach = output_dict['reach']
        code_validation_output, is_valid_code = validate_code(code)
        if not is_valid_code:
            return (main_message + "The storage is not successful. Error: The provided code must contain only the 'calculate_metrics' function definition. " + code_validation_output, False)
        original_code = code
        before_code = f"""# you must use these exact imports in your code\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom datetime import timedelta\nfrom functions import run_sitewiz_query \nfrom typing import TypedDict, List, Tuple\n\nclass MetricOutput(TypedDict):\n    Metric: str\n    Description: str\n    start_date: str\n    end_date: str\n    values: List[Tuple[str, float]]\n\nstream_key = '{stream_key}'\n\n# Get yesterday's date as end_date\nend_date = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime("%Y-%m-%d")\n# Get date 6 days before end_date as  start_date\nstart_date = (datetime.datetime.now() - datetime.timedelta(days=7)).strftime("%Y-%m-%d")\n\nstart_time = int(datetime.datetime.strptime(start_date, "%Y-%m-%d").timestamp())\nend_time = int(datetime.datetime.strptime(end_date, "%Y-%m-%d").timestamp())"""
        end_code = 'output = calculate_metrics(start_date, end_date)\nprint("Calculate Metrics Function Output:")\nprint(output)'
        print(code_validation_output)
        code = before_code + '\n' + code_validation_output + '\n' + end_code
        exit_code, result = asyncio.run(evaluate_code(code, executor))
        print(result)
        if exit_code == 1:
            return (main_message + 'The storage is not successful. The code did not execute successfully with exit code 1. Please have the python analyst fix this error: \n' + result, False)
        try:
            output_start = result.rfind('Calculate Metrics Function Output:')
            if output_start != -1:
                output_text = result[output_start:].split('\n', 1)[1].strip()
                output_dict = eval(output_text)
                formatted_output = f'\nMetric: {output_dict['Metric']}\nDescription: {output_dict['Description']}\nDate Range: {output_dict['start_date']} to {output_dict['end_date']}\nValues: {output_dict['values']}\n'
                print('\nExtracted metrics:')
                print(formatted_output)
                if len(output_dict['values']) < 3:
                    raise Exception(f"There are {len(output_dict['values'])} in the values array. There should be 7 for a week's worth of data")
                if len(set((str(v) for v in output_dict['values']))) == 1:
                    raise Exception('All the values in the values array are the same. That means it is likely not computed correctly or it is not a useful metric. Pursue another direction')
        except Exception as e:
            return (main_message + f"The storage is not successful. The code had trouble extracting metrics with error: '{e}'. \nPlease have the python analyst fix this error. Here was the output of the code: \n{result}", False)
        query_description_name, query_documentation = run_sitewiz_query_description('functions', {'stream_key': stream_key})
        okr_markdown = okr_to_markdown({'output': output_text})
        prev_okr_markdowns = get_all_okrs_markdown(stream_key)
        documents = {'name': {'type': 'text', 'content': name, 'description': 'OKR Name'}, 'description': {'type': 'text', 'content': description, 'description': 'OKR Description'}, 'okr_markdown': {'type': 'text', 'content': okr_markdown, 'description': 'OKR Markdown'}, 'prev_okr_markdowns': {'type': 'text', 'content': prev_okr_markdowns, 'description': 'All previous OKR Markdowns'}, 'okr_criteria': {'type': 'text', 'content': okr_criteria, 'description': 'OKR Criteria'}, 'code': {'type': 'text', 'content': original_code, 'description': 'Python function for OKR computation'}, 'queries': {'type': 'text', 'content': queries, 'description': 'SQL queries used in the function'}, 'query_execution_output': {'type': 'text', 'content': output_text, 'description': 'Execution output of the SQL queries'}, 'business_context': {'type': 'text', 'content': business_context, 'description': 'Contextual information on business relevance'}, 'query_documentation': {'type': 'text', 'content': query_documentation, 'description': 'Documentation for the queries executed'}}
        validation_results = run_evaluation(documents, okr_questions)
        validation_message, pass_validation = interpret_evaluations(validation_results, main_message + 'OKR validation failed so OKR is not stored')
        if not pass_validation:
            return (validation_message, False)
        dynamodb = get_dynamodb_client()
        try:
            response = dynamodb.get_item(TableName='website-okrs', Key={'streamKey': {'S': stream_key}, 'name': {'S': name}})
            timestamp = str(int(datetime.now().timestamp() * 1000))
            if 'Item' in response:
                dynamodb.update_item(TableName='website-okrs', Key={'streamKey': {'S': stream_key}, 'name': {'S': name}}, UpdateExpression='SET code = :code, description = :desc, timestamp = :ts, output = :output, reach_code = :reach_code, reach = :reach, verified = :verified', ExpressionAttributeValues={':code': {'S': code_validation_output}, ':desc': {'S': description}, ':ts': {'N': timestamp}, ':output': {'S': output_text}, ':reach_code': {'S': original_reach_code}, ':reach': {'N': str(reach)}, ':verified': {'BOOL': True}})
            else:
                dynamodb.put_item(TableName='website-okrs', Item={'streamKey': {'S': stream_key}, 'name': {'S': name}, 'code': {'S': code_validation_output}, 'description': {'S': description}, 'timestamp': {'N': timestamp}, 'output': {'S': output_text}, 'reach_code': {'S': original_reach_code}, 'reach': {'N': str(reach)}, 'verified': {'BOOL': True}})
            store_evaluations(stream_key, f'okr#{name}#{timestamp}', validation_results)
            return (f"OKR stored with name '{name}'\n{validation_message}", True)
        except Exception as e:
            return (main_message + f'Error storing OKR code in DynamoDB: {e}', False)
    except Exception as e:
        import traceback
        traceback.print_exc()
        return (main_message + f'Error in store_okr: {e}', False)

def remove_okr(name: Annotated[str, 'Name of the OKR to remove']) -> Annotated[str, 'Result message']:
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.get_item(TableName='website-okrs', Key={'streamKey': {'S': stream_key}, 'name': {'S': name}})
        if 'Item' not in response:
            return f'No OKR found with name: {name}'
        dynamodb.delete_item(TableName='website-okrs', Key={'streamKey': {'S': stream_key}, 'name': {'S': name}})
        return f'Successfully removed OKR: {name}'
    except Exception as e:
        return f'Error removing OKR: {e}'

import traceback



# File: backend/agents/data_analyst_group/tools/store_okr_notes.py

from typing_extensions import Annotated

from datetime import datetime

import json

from utils.functions import get_dynamodb_client

from typing import List

def store_okr_notes_given_context(stream_key):

    def store_okr_notes(notes: Annotated[str, 'OKR notes to store']) -> Annotated[str, 'Result message']:
        try:
            timestamp = int(datetime.now().timestamp() * 1000)
            dynamodb = get_dynamodb_client()
            dynamodb.update_item(TableName='WebsiteData', Key={'streamKey': {'S': stream_key}}, UpdateExpression='SET okr_notes = :notes', ExpressionAttributeValues={':notes': {'S': json.dumps(notes)}})
            return f'Successfully stored {len(notes)} OKR notes with timestamp {timestamp}'
        except Exception as e:
            return f'Error storing OKR notes: {e}'
    return store_okr_notes

def store_okr_notes(notes: Annotated[str, 'OKR notes to store']) -> Annotated[str, 'Result message']:
    try:
        timestamp = int(datetime.now().timestamp() * 1000)
        dynamodb = get_dynamodb_client()
        dynamodb.update_item(TableName='WebsiteData', Key={'streamKey': {'S': stream_key}}, UpdateExpression='SET okr_notes = :notes', ExpressionAttributeValues={':notes': {'S': json.dumps(notes)}})
        return f'Successfully stored {len(notes)} OKR notes with timestamp {timestamp}'
    except Exception as e:
        return f'Error storing OKR notes: {e}'



# File: backend/agents/data_analyst_group/tools/store_suggestion.py

import boto3

from typing_extensions import Annotated

from pydantic import BaseModel

import json

import time

from typing import List, Dict, Any, Optional

from botocore.exceptions import ClientError

import requests

from PIL import Image

import os

import base64

from utils.evaluation import run_evaluation, evaluate_impact_confidence, store_evaluations, interpret_evaluations

from utils.functions import get_dynamodb_client, get_dynamodb_table, get_insight, get_video_details, get_heatmap_details, get_analytics_details, download_image, split_image, encode_image, suggestion_to_markdown

from prompts.suggestion_prompts import suggestion_criteria, data_questions, suggestion_questions

from tools.get_stored_suggestions import get_stored_suggestions_given_context

def get_suggestions_summary(stream_key: str) -> str:
    dynamodb_client = get_dynamodb_client()
    response = dynamodb_client.get_item(TableName='WebsiteData', Key={'streamKey': {'S': stream_key}})
    item = response.get('Item', {})
    summary = item.get('summary', {}).get('S', '') if 'summary' in item else ''
    return summary

def process_data(data: Dict[str, Any], messages: List[Dict[str, Any]], stream_key, insight=''):
    evaluation = None
    object_details = None
    description = ''
    if data['type'] == 'Heatmap':
        object_details, description = get_heatmap_details(data)
        if object_details:
            messages.append(object_details)
        else:
            raise Exception(f'The heatmap with ID {data['key']} is not available. Please provide a valid heatmap.')
    elif data['type'] == 'Session Recording':
        object_details, description = get_video_details(data)
        if object_details:
            messages.append(object_details)
        else:
            raise Exception(f'The session recording with ID {data['key']} is not available. Please provide a valid session recording.')
    elif data['type'] == 'Analytics':
        object_details, description = get_analytics_details(data, stream_key)
        if object_details:
            messages.append(object_details)
        else:
            raise Exception(f'The analytics data with ID {data['key']} is not available. Please provide the stored insight timestamp.')
    evidence = None
    if data['type'] in ['Heatmap', 'Session Recording']:
        image_urls = [obj['image_url']['url'] for obj in object_details['content'] if obj['type'] == 'image_url']
        if len(image_urls) > 0:
            evidence = {'type': 'image', 'content': image_urls, 'explanation': description, 'description': 'Data Evidence'}
        else:
            raise Exception(f'No image URLs found')
    else:
        text_content = '\n'.join([obj['text'] for obj in object_details['content'] if obj['type'] == 'text'])
        evidence = {'type': 'text', 'content': text_content, 'explanation': description, 'description': 'Data Evidence'}
    verification_object = {'Insight': {'type': 'text', 'content': insight, 'description': 'Insight generalized from the data'}, 'explanation': {'type': 'text', 'content': evidence['explanation'], 'description': 'Explanation of the data connection to the insight'}, 'Data': evidence}
    validation_results = run_evaluation(verification_object, data_questions, partition=f'{data['type']}#{data['key']}')
    return (validation_results, verification_object)

def save_data_to_dynamodb(json_string, stream_key, timestamp):
    data = json.loads(json_string)
    data['streamKey'] = stream_key
    data['verified'] = True
    data['timestamp'] = timestamp
    try:
        table = get_dynamodb_table('WebsiteReports')
        table.put_item(TableName='WebsiteReports', Item=data)
        return f'Successfully saved the suggestion'
    except ClientError as e:
        return f'Error saving report: {str(e)}'

def evaluate_suggestion(suggestion: Dict[str, Any], context: str, stream_key: str, max_tries=3) -> tuple[str, bool]:
    """Evaluate a suggestion using strict yes/no criteria for traceability, entailment, and uniqueness."""
    messages = []
    all_validation_results = []
    for insight in suggestion['Insights']:
        if 'data' in insight:
            for data in insight['data']:
                validation_results, verification_object = process_data(data, messages, stream_key, insight.get('text', ''))
                all_validation_results += validation_results
    stored_suggestions = get_stored_suggestions_given_context(stream_key)('')
    suggestion_markdown = suggestion_to_markdown(suggestion)
    documents = {'suggestion_markdown': {'type': 'text', 'content': suggestion_markdown, 'description': 'The full suggestion content'}, 'Insights': {'type': 'text', 'content': json.dumps(suggestion.get('Insights', '')), 'description': 'Data insights'}, 'Expanded': {'type': 'text', 'content': json.dumps(suggestion.get('Expanded', '')), 'description': 'Expanded details'}, 'Tags': {'type': 'text', 'content': json.dumps(suggestion.get('Tags', '')), 'description': 'Suggestion tags'}, 'Shortened': {'type': 'text', 'content': json.dumps(suggestion.get('Shortened', '')), 'description': 'Suggestion header'}, 'previous_suggestions': {'type': 'text', 'content': stored_suggestions, 'description': 'Previously stored suggestions'}, 'business_context': {'type': 'text', 'content': context, 'description': 'Business context'}, 'suggestion_summary': {'type': 'text', 'content': get_suggestions_summary(stream_key), 'description': 'Summary of previous suggestions'}}
    validation_results = run_evaluation(documents, suggestion_questions)
    all_validation_results += validation_results
    validation_message, pass_validation = interpret_evaluations(all_validation_results, 'Suggestion validation failed.')
    if not pass_validation:
        return (validation_message, False)
    timestamp = int(time.time())
    save_data_to_dynamodb(json.dumps(suggestion), stream_key, timestamp)
    store_evaluations(stream_key, f'suggestion#{timestamp}', all_validation_results)
    return ('Suggestion passed all validation criteria\n\n' + validation_message, True)

def store_suggestion_given_context(business_context: str, stream_key: str):
    """Create a store_suggestion function with tracking, evidence validation, and uniqueness checking."""

    def store_suggestion(suggestion: Annotated[Suggestion, 'Suggestion data to be stored']) -> Annotated[tuple[str, bool], 'Result message and success boolean']:
        try:
            required_keys = ['Shortened', 'Expanded', 'Insights', 'Tags', 'InsightConnectionTimestamp']
            suggestion_dict = suggestion.model_dump() if isinstance(suggestion, BaseModel) else suggestion
            if not isinstance(suggestion, BaseModel):
                suggestion = Suggestion(**suggestion)
            for key in required_keys:
                if key not in suggestion_dict:
                    return (f"Error: Missing required key '{key}' in suggestion.", False)
            dynamodb_client = get_dynamodb_client()
            insight_item = dynamodb_client.get_item(TableName='website-insights', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion.InsightConnectionTimestamp)}})
            if not insight_item or not insight_item.get('Item'):
                output = f'Insight at timestamp {suggestion.InsightConnectionTimestamp} not found. Use a timestamp from a stored insight or store one first.'
                return (output, False)
            suggestion_dict['timestamp'] = int(time.time())
            message, success = evaluate_suggestion(suggestion_dict, business_context, stream_key)
            return (message, success)
        except Exception as e:
            import traceback
            traceback.print_exc()
            return (f'Error storing suggestion: {str(e)}. Please return to the step with the behavioral analyst.', False)
    return store_suggestion

def store_suggestion(suggestion: Annotated[Suggestion, 'Suggestion data to be stored']) -> Annotated[tuple[str, bool], 'Result message and success boolean']:
    try:
        required_keys = ['Shortened', 'Expanded', 'Insights', 'Tags', 'InsightConnectionTimestamp']
        suggestion_dict = suggestion.model_dump() if isinstance(suggestion, BaseModel) else suggestion
        if not isinstance(suggestion, BaseModel):
            suggestion = Suggestion(**suggestion)
        for key in required_keys:
            if key not in suggestion_dict:
                return (f"Error: Missing required key '{key}' in suggestion.", False)
        dynamodb_client = get_dynamodb_client()
        insight_item = dynamodb_client.get_item(TableName='website-insights', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion.InsightConnectionTimestamp)}})
        if not insight_item or not insight_item.get('Item'):
            output = f'Insight at timestamp {suggestion.InsightConnectionTimestamp} not found. Use a timestamp from a stored insight or store one first.'
            return (output, False)
        suggestion_dict['timestamp'] = int(time.time())
        message, success = evaluate_suggestion(suggestion_dict, business_context, stream_key)
        return (message, success)
    except Exception as e:
        import traceback
        traceback.print_exc()
        return (f'Error storing suggestion: {str(e)}. Please return to the step with the behavioral analyst.', False)

import traceback



# File: backend/agents/data_analyst_group/tools/store_tracking.py

from tools.run_sitewiz_query import run_sitewiz_query, run_sitewiz_query_description

from tools.run_bigquery_query import run_bigquery_query, run_bigquery_query_description

from tools.save_graph import save_graph, save_graph_description

from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor

from pathlib import Path

import tempfile

from typing_extensions import Annotated

from datetime import datetime

import json

from utils.functions import get_dynamodb_client

from pydantic import BaseModel

from typing import List

from autogen_core.code_executor import CodeBlock

from autogen_core import CancellationToken

import asyncio

from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor

from pathlib import Path

import ast

def validate_code(code: str) -> bool:
    try:
        tree = ast.parse(code)
    except SyntaxError:
        return False
    if len(tree.body) != 1:
        return False
    node = tree.body[0]
    return isinstance(node, ast.FunctionDef) and node.name == 'calculate_metrics'

def store_tracking_given_context(stream_key, executor):

    def store_tracking(code: Annotated[str, 'The calculate_metrics function code'], suggestion_id: Annotated[str, 'ID of the suggestion being tracked'], description: Annotated[str, 'Description of what is being tracked']) -> Annotated[tuple[str, bool], 'Result message and success boolean']:
        try:
            print('storing tracking code')
            print(code)
            if not validate_code(code):
                return ("The storage is not successful. Error: The provided code must contain only the 'calculate_metrics' function definition.", False)
            original_code = code
            before_code = f"""# you must use these exact imports in your code\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom functions import run_sitewiz_query\n\n# Get yesterday's date as end_date\nend_date = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime("%Y-%m-%d")\n# Get date 6 days before end_date as start_date\nstart_date = (datetime.datetime.now() - datetime.timedelta(days=7)).strftime("%Y-%m-%d")\n\nstart_time = int(datetime.datetime.strptime(start_date, "%Y-%m-%d").timestamp())\nend_time = int(datetime.datetime.strptime(end_date, "%Y-%m-%d").timestamp())"""
            end_code = 'output = calculate_metrics(start_date, end_date)\nprint("Calculate Metrics Function Output:")\nprint(output)'
            code = before_code + '\n' + code + '\n' + end_code
            exit_code, result = asyncio.run(evaluate_code(code, executor))
            print(result)
            if exit_code == 1:
                return ('The storage is not successful. The code did not execute successfully with exit code 1. Please have the python analyst fix this error: \n' + result, False)
            try:
                output_start = result.rfind('Calculate Metrics Function Output:')
                if output_start != -1:
                    output_text = result[output_start:].split('\n', 1)[1].strip()
                    output_dict = eval(output_text)
                    formatted_output = f'\nMetric: {output_dict['Metric']}\nDescription: {output_dict['Description']}\nDate Range: {output_dict['start_date']} to {output_dict['end_date']}\nValues: {output_dict['values']}\n'
                    print('\nExtracted metrics:')
                    print(formatted_output)
                    values = output_dict['values']
                    if len(values) < 3:
                        raise Exception(f"There are {len(values)} in the values array. There should be 7 for a week's worth of data")
                    if len(set([v[1] for v in values])) == 1:
                        raise Exception(f'All the values in the values array are the same. That means it is likely not computed correctly or it is not a useful metric. Pursue another direction')
            except Exception as e:
                return (f"The storage is not successful. The code had trouble extracting metrics with error: '{e}'.\nPlease have the python analyst fix this error. Here was the output of the code: \n{result}", False)
            dynamodb = get_dynamodb_client()
            try:
                response = dynamodb.get_item(TableName='website-tracking', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}})
                if 'Item' in response:
                    dynamodb.update_item(TableName='website-tracking', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}}, UpdateExpression='SET code = :code, description = :desc, timestamp = :ts', ExpressionAttributeValues={':code': {'S': original_code}, ':desc': {'S': description}, ':ts': {'N': str(int(datetime.now().timestamp() * 1000))}})
                    return (f"Successfully updated existing tracking code for suggestion '{suggestion_id}'", True)
                else:
                    dynamodb.put_item(TableName='website-tracking', Item={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}, 'code': {'S': original_code}, 'description': {'S': description}, 'timestamp': {'N': str(int(datetime.now().timestamp() * 1000))}})
                    return (f"Successfully created new tracking code for suggestion '{suggestion_id}'", True)
            except Exception as e:
                return (f'Error storing tracking code: {e}', False)
        except Exception as e:
            return (f'Error storing tracking code: {e}', False)
    return store_tracking

def remove_tracking_given_context(stream_key):

    def remove_tracking(suggestion_id: Annotated[str, 'ID of the suggestion whose tracking code should be removed']) -> Annotated[str, 'Result message']:
        try:
            dynamodb = get_dynamodb_client()
            response = dynamodb.get_item(TableName='website-tracking', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}})
            if 'Item' not in response:
                return f'No tracking code found for suggestion: {suggestion_id}'
            dynamodb.delete_item(TableName='website-tracking', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}})
            return f'Successfully removed tracking code for suggestion: {suggestion_id}'
        except Exception as e:
            return f'Error removing tracking code: {e}'
    return remove_tracking

def store_tracking(code: Annotated[str, 'The calculate_metrics function code'], suggestion_id: Annotated[str, 'ID of the suggestion being tracked'], description: Annotated[str, 'Description of what is being tracked']) -> Annotated[tuple[str, bool], 'Result message and success boolean']:
    try:
        print('storing tracking code')
        print(code)
        if not validate_code(code):
            return ("The storage is not successful. Error: The provided code must contain only the 'calculate_metrics' function definition.", False)
        original_code = code
        before_code = f"""# you must use these exact imports in your code\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom functions import run_sitewiz_query\n\n# Get yesterday's date as end_date\nend_date = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime("%Y-%m-%d")\n# Get date 6 days before end_date as start_date\nstart_date = (datetime.datetime.now() - datetime.timedelta(days=7)).strftime("%Y-%m-%d")\n\nstart_time = int(datetime.datetime.strptime(start_date, "%Y-%m-%d").timestamp())\nend_time = int(datetime.datetime.strptime(end_date, "%Y-%m-%d").timestamp())"""
        end_code = 'output = calculate_metrics(start_date, end_date)\nprint("Calculate Metrics Function Output:")\nprint(output)'
        code = before_code + '\n' + code + '\n' + end_code
        exit_code, result = asyncio.run(evaluate_code(code, executor))
        print(result)
        if exit_code == 1:
            return ('The storage is not successful. The code did not execute successfully with exit code 1. Please have the python analyst fix this error: \n' + result, False)
        try:
            output_start = result.rfind('Calculate Metrics Function Output:')
            if output_start != -1:
                output_text = result[output_start:].split('\n', 1)[1].strip()
                output_dict = eval(output_text)
                formatted_output = f'\nMetric: {output_dict['Metric']}\nDescription: {output_dict['Description']}\nDate Range: {output_dict['start_date']} to {output_dict['end_date']}\nValues: {output_dict['values']}\n'
                print('\nExtracted metrics:')
                print(formatted_output)
                values = output_dict['values']
                if len(values) < 3:
                    raise Exception(f"There are {len(values)} in the values array. There should be 7 for a week's worth of data")
                if len(set([v[1] for v in values])) == 1:
                    raise Exception(f'All the values in the values array are the same. That means it is likely not computed correctly or it is not a useful metric. Pursue another direction')
        except Exception as e:
            return (f"The storage is not successful. The code had trouble extracting metrics with error: '{e}'.\nPlease have the python analyst fix this error. Here was the output of the code: \n{result}", False)
        dynamodb = get_dynamodb_client()
        try:
            response = dynamodb.get_item(TableName='website-tracking', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}})
            if 'Item' in response:
                dynamodb.update_item(TableName='website-tracking', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}}, UpdateExpression='SET code = :code, description = :desc, timestamp = :ts', ExpressionAttributeValues={':code': {'S': original_code}, ':desc': {'S': description}, ':ts': {'N': str(int(datetime.now().timestamp() * 1000))}})
                return (f"Successfully updated existing tracking code for suggestion '{suggestion_id}'", True)
            else:
                dynamodb.put_item(TableName='website-tracking', Item={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}, 'code': {'S': original_code}, 'description': {'S': description}, 'timestamp': {'N': str(int(datetime.now().timestamp() * 1000))}})
                return (f"Successfully created new tracking code for suggestion '{suggestion_id}'", True)
        except Exception as e:
            return (f'Error storing tracking code: {e}', False)
    except Exception as e:
        return (f'Error storing tracking code: {e}', False)

def remove_tracking(suggestion_id: Annotated[str, 'ID of the suggestion whose tracking code should be removed']) -> Annotated[str, 'Result message']:
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.get_item(TableName='website-tracking', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}})
        if 'Item' not in response:
            return f'No tracking code found for suggestion: {suggestion_id}'
        dynamodb.delete_item(TableName='website-tracking', Key={'streamKey': {'S': stream_key}, 'suggestionId': {'S': suggestion_id}})
        return f'Successfully removed tracking code for suggestion: {suggestion_id}'
    except Exception as e:
        return f'Error removing tracking code: {e}'



# File: backend/agents/data_analyst_group/tools/store_website.py

import requests

from PIL import Image

from io import BytesIO

from typing import Annotated, Literal, Tuple

import os

import boto3

from pathlib import Path

import time

import json

from utils.functions import get_dynamodb_client, save_results, run_completion_with_fallback

from pydantic import BaseModel, Field

from tools.website_screenshot import get_screenshot_given_context

import difflib

import asyncio

from prompts.code_prompts import code_questions

from utils.evaluation import run_evaluation, store_evaluations, interpret_evaluations

from tools.get_website import get_website_given_context

def get_content_type(ext: str) -> str:
    mime_types = {'.html': 'text/html; charset=utf-8', '.css': 'text/css; charset=utf-8', '.js': 'application/javascript; charset=utf-8', '.png': 'image/png', '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg', '.gif': 'image/gif', '.svg': 'image/svg+xml', '.woff': 'font/woff', '.woff2': 'font/woff2', '.ttf': 'font/ttf', '.json': 'application/json; charset=utf-8'}
    return mime_types.get(ext, 'application/octet-stream')

def upload_s3_file(bucket: str, folder: str, file_name: str, upload_name: str, workspace_base: str):
    """Upload modified file back to S3"""
    s3 = boto3.client('s3')
    key = os.path.join(folder, upload_name)
    file_path = os.path.join(workspace_base, file_name)
    if os.path.exists(file_path):
        content_type = get_content_type(os.path.splitext(file_name)[1])
        extra_args = {'ContentType': content_type} if content_type else {}
        s3.upload_file(file_path, bucket, key, ExtraArgs=extra_args)
        print(f'Uploaded {file_path} to s3://{bucket}/{key}')
    else:
        print(f'File {file_path} does not exist and cannot be uploaded.')
    url = f'https://{bucket}.s3.us-east-1.amazonaws.com/{key}'
    return url

def get_s3_file(bucket: str, key: str, save_path: str) -> str:
    """Retrieve a file from S3 and save it locally."""
    s3 = boto3.client('s3')
    try:
        obj = s3.get_object(Bucket=bucket, Key=key)
        content = obj['Body'].read().decode('utf-8')
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return content
    except Exception as e:
        print(f'Error retrieving {key} from S3: {e}')
        return ''

def compute_diff(original: str, updated: str) -> str:
    """Compute and return the differences between the original and updated HTML files."""
    diff = difflib.unified_diff(original.splitlines(), updated.splitlines(), lineterm='', n=5)
    return '\n'.join(diff)

def verify_change_with_ai(diff_text: str, old_screenshots: list, new_screenshots: list, changes_markdown: str, x: int, y: int, short_hypothesis: str) -> Tuple[str, bool, list]:
    """Use AI completion to determine if the change is significant and properly implemented."""
    documents = {'code': {'type': 'text', 'content': diff_text, 'description': 'Code changes made'}, 'changes_markdown': {'type': 'text', 'content': changes_markdown, 'description': 'Description of changes'}, 'coordinates': {'type': 'text', 'content': f'x: {x}, y: {y}', 'description': 'Change coordinates'}, 'old_screenshots': {'type': 'image', 'content': old_screenshots, 'description': 'Screenshots before changes'}, 'new_screenshots': {'type': 'image', 'content': new_screenshots, 'description': 'Screenshots after changes'}, 'short_hypothesis': {'type': 'text', 'content': short_hypothesis, 'description': 'Brief change description'}}
    validation_results = run_evaluation(documents, code_questions)
    validation_message, pass_validation = interpret_evaluations(validation_results, 'Code storage failed.')
    if not pass_validation:
        return (validation_message, False, validation_results)
    return ('All validation checks passed successfully.\n\n' + validation_message, True, validation_results)

def store_website_given_context(workspace_base: str, stream_key: str):

    def upload_results(bucket: str, folder: str, workspace_base: str, work_folder: str, changes_markdown: str, suggestion_timestamp: str, x: int, y: int, short_hypothesis: str):
        try:
            old_html_path = os.path.join(workspace_base, 'old_html', 'index.html')
            os.makedirs(os.path.dirname(old_html_path), exist_ok=True)
            new_html_path = os.path.join(workspace_base, 'index.html')
            original_html = get_s3_file(bucket, f'{folder}/index.html', old_html_path)
            if not os.path.exists(new_html_path):
                return ('Error: Updated HTML file not found.', False)
            with open(new_html_path, 'r', encoding='utf-8') as f:
                updated_html = f.read()
            diff_text = compute_diff(original_html, updated_html)
            if not diff_text.strip():
                return ('No changes detected in the HTML file. You must use the edit tool to make the required changes to the code, then take screenshots to verify the changes.', False)
            get_screenshot = get_screenshot_given_context(Path(workspace_base) / 'old_html', stream_key)
            print('Capturing 5 screenshots of the old HTML (before changes)...')
            old_screenshots_str = asyncio.run(get_screenshot(device_type='desktop', max_screenshots=5))
            old_screenshots = []
            if old_screenshots_str:
                old_screenshots = old_screenshots_str.split('\n')
            print('Old HTML Screenshots:', old_screenshots)
            get_screenshot = get_screenshot_given_context(Path(workspace_base), stream_key)
            print('Capturing 5 screenshots of the new HTML (after changes)...')
            new_screenshots_str = asyncio.run(get_screenshot(device_type='desktop', max_screenshots=5))
            new_screenshots = []
            if new_screenshots_str:
                new_screenshots = new_screenshots_str.split('\n')
            print('New HTML Screenshots:', new_screenshots)
            reasoning, changes_made, validation_results = verify_change_with_ai(diff_text, old_screenshots, new_screenshots, changes_markdown, x, y, short_hypothesis)
            if not changes_made:
                return (reasoning, False)
            updated_url = upload_s3_file(bucket, folder, 'index.html', f'{work_folder}_sitewiz_codeagent_fixed.html', workspace_base)
            print(f'Uploaded results to {updated_url}')
            dynamodb = get_dynamodb_client()
            table_name = os.getenv('DYNAMODB_TABLE_NAME', 'WebsiteReports')
            Code = json.dumps({'original_url': f'https://{bucket}.s3.amazonaws.com/{folder}/index.html', 'updated_url': updated_url, 'changes': changes_markdown, 'x': x, 'y': y, 'short_hypothesis': short_hypothesis, 'old_screenshots': old_screenshots, 'new_screenshots': new_screenshots, 'reasoning': reasoning})
            response = dynamodb.update_item(TableName=table_name, Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': suggestion_timestamp}}, UpdateExpression='SET Code = :Code', ExpressionAttributeValues={':Code': {'S': Code}})
            store_evaluations(stream_key, f'website#{suggestion_timestamp}', validation_results)
            return (f'Website stored successfully at {updated_url}.\n\n' + reasoning, True)
        except Exception as e:
            return (f'Error storing results: {e}', False)

    def store_website(changes_markdown: Annotated[str, 'All the changes made including exactly what was changed and why to implement these changes in a markdown format'], suggestion_timestamp: Annotated[int, 'Timestamp of the suggestion we are changing'], x: Annotated[int, 'Approximate x location of the change'], y: Annotated[int, 'Approximate y location of the change'], short_hypothesis: Annotated[str, 'Suggestion in short of the change we are making']) -> Annotated[tuple[str, bool], 'URL of the uploaded website and success boolean']:
        """Save the website as a snapshot."""
        try:
            response_data = None
            try:
                with open(f'{workspace_base}/sitewiz_change_info.json', 'r') as f:
                    response_data = json.load(f)
            except FileNotFoundError:
                return ('Error: sitewiz_change_info.json not found.', False)
            bucket = response_data['bucket']
            folder = response_data['folder']
            timestamp = response_data['timestamp']
            work_folder = f'{stream_key}_{timestamp}'
            message, success = upload_results(bucket, folder, workspace_base, work_folder, changes_markdown, str(suggestion_timestamp), x, y, short_hypothesis)
            return (message, success)
        except Exception as e:
            return (f'Error saving website: {str(e)}', False)
    return store_website

def upload_results(bucket: str, folder: str, workspace_base: str, work_folder: str, changes_markdown: str, suggestion_timestamp: str, x: int, y: int, short_hypothesis: str):
    try:
        old_html_path = os.path.join(workspace_base, 'old_html', 'index.html')
        os.makedirs(os.path.dirname(old_html_path), exist_ok=True)
        new_html_path = os.path.join(workspace_base, 'index.html')
        original_html = get_s3_file(bucket, f'{folder}/index.html', old_html_path)
        if not os.path.exists(new_html_path):
            return ('Error: Updated HTML file not found.', False)
        with open(new_html_path, 'r', encoding='utf-8') as f:
            updated_html = f.read()
        diff_text = compute_diff(original_html, updated_html)
        if not diff_text.strip():
            return ('No changes detected in the HTML file. You must use the edit tool to make the required changes to the code, then take screenshots to verify the changes.', False)
        get_screenshot = get_screenshot_given_context(Path(workspace_base) / 'old_html', stream_key)
        print('Capturing 5 screenshots of the old HTML (before changes)...')
        old_screenshots_str = asyncio.run(get_screenshot(device_type='desktop', max_screenshots=5))
        old_screenshots = []
        if old_screenshots_str:
            old_screenshots = old_screenshots_str.split('\n')
        print('Old HTML Screenshots:', old_screenshots)
        get_screenshot = get_screenshot_given_context(Path(workspace_base), stream_key)
        print('Capturing 5 screenshots of the new HTML (after changes)...')
        new_screenshots_str = asyncio.run(get_screenshot(device_type='desktop', max_screenshots=5))
        new_screenshots = []
        if new_screenshots_str:
            new_screenshots = new_screenshots_str.split('\n')
        print('New HTML Screenshots:', new_screenshots)
        reasoning, changes_made, validation_results = verify_change_with_ai(diff_text, old_screenshots, new_screenshots, changes_markdown, x, y, short_hypothesis)
        if not changes_made:
            return (reasoning, False)
        updated_url = upload_s3_file(bucket, folder, 'index.html', f'{work_folder}_sitewiz_codeagent_fixed.html', workspace_base)
        print(f'Uploaded results to {updated_url}')
        dynamodb = get_dynamodb_client()
        table_name = os.getenv('DYNAMODB_TABLE_NAME', 'WebsiteReports')
        Code = json.dumps({'original_url': f'https://{bucket}.s3.amazonaws.com/{folder}/index.html', 'updated_url': updated_url, 'changes': changes_markdown, 'x': x, 'y': y, 'short_hypothesis': short_hypothesis, 'old_screenshots': old_screenshots, 'new_screenshots': new_screenshots, 'reasoning': reasoning})
        response = dynamodb.update_item(TableName=table_name, Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': suggestion_timestamp}}, UpdateExpression='SET Code = :Code', ExpressionAttributeValues={':Code': {'S': Code}})
        store_evaluations(stream_key, f'website#{suggestion_timestamp}', validation_results)
        return (f'Website stored successfully at {updated_url}.\n\n' + reasoning, True)
    except Exception as e:
        return (f'Error storing results: {e}', False)

def store_website(changes_markdown: Annotated[str, 'All the changes made including exactly what was changed and why to implement these changes in a markdown format'], suggestion_timestamp: Annotated[int, 'Timestamp of the suggestion we are changing'], x: Annotated[int, 'Approximate x location of the change'], y: Annotated[int, 'Approximate y location of the change'], short_hypothesis: Annotated[str, 'Suggestion in short of the change we are making']) -> Annotated[tuple[str, bool], 'URL of the uploaded website and success boolean']:
    """Save the website as a snapshot."""
    try:
        response_data = None
        try:
            with open(f'{workspace_base}/sitewiz_change_info.json', 'r') as f:
                response_data = json.load(f)
        except FileNotFoundError:
            return ('Error: sitewiz_change_info.json not found.', False)
        bucket = response_data['bucket']
        folder = response_data['folder']
        timestamp = response_data['timestamp']
        work_folder = f'{stream_key}_{timestamp}'
        message, success = upload_results(bucket, folder, workspace_base, work_folder, changes_markdown, str(suggestion_timestamp), x, y, short_hypothesis)
        return (message, success)
    except Exception as e:
        return (f'Error saving website: {str(e)}', False)



# File: backend/agents/data_analyst_group/tools/str_replace_editor.py

import re

import json

import os

import shutil

from typing import Annotated, Literal, List, Tuple

def str_replace_editor_given_context(website_workspace: str, stream_key: str):
    """
    Replace a string in the editor based on the provided command and parameters.

    Args:
        website_workspace (str): The base directory of the website workspace.
        stream_key (str): The stream key associated with the session.

    Returns:
        function: The str_replace_editor function.
    """
    THRESHOLD = 10000

    def reduceSize(content: str) -> Tuple[str, List[str]]:
        messages = []
        if len(content) > THRESHOLD:
            new_content = re.sub('<svg[^>]*>.*?</svg>', '[SVG removed]', content, flags=re.DOTALL)
            if new_content != content:
                messages.append('Removed <svg> blocks')
            content = new_content
            new_content = re.sub('\\s*(data-[\\w-]+)="[^"]+"', '', content)
            if new_content != content:
                messages.append('Removed data attributes')
            content = new_content
            if len(content) > THRESHOLD:
                content = content[:THRESHOLD]
                messages.append(f'Content trimmed to first {THRESHOLD} characters')
        return (content, messages)

    def view_file(file_path: str, view_range: ViewRange=None) -> str:
        """
        View the content of the file or a specific range of lines.

        Args:
            file_path (str): The path to the file.
            view_range (ViewRange, optional): The range of lines to view.

        Returns:
            str: The content to be viewed.
        """
        if not os.path.isfile(file_path):
            raise FileNotFoundError(f'File not found: {file_path}')
        with open(file_path, 'r') as file:
            content = file.read()
        if view_range:
            lines = content.splitlines()
            start, end = view_range
            start = max(start, 1)
            end = end if end != -1 else len(lines)
            content = '\n'.join(lines[start - 1:end])
        content, messages = reduceSize(content)
        if messages:
            header = f'Modifications made to reduce to {THRESHOLD} characters: ' + ', '.join(messages) + '\n\n'
            content = header + content
        return content

    def create_file(file_path: str, file_text: FileText) -> str:
        """
        Create a new file with the provided content.

        Args:
            file_path (str): The path to the new file.
            file_text (str): The content to write to the file.

        Returns:
            str: Confirmation message.
        """
        if os.path.exists(file_path):
            raise FileExistsError(f'File already exists: {file_path}')
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w') as file:
            file.write(file_text)
        return f'File created at {file_path}'

    def str_replace(file_path: str, old_str: OldStr, new_str: NewStr) -> str:
        """
        Replace all occurrences of old_str with new_str in the file.

        Args:
            file_path (str): The path to the file.
            old_str (str): The string to be replaced.
            new_str (str): The string to replace with.

        Returns:
            str: Confirmation message.
        """
        if not os.path.isfile(file_path):
            raise FileNotFoundError(f'File not found: {file_path}')
        backup_path = file_path + '.bak'
        if not os.path.isfile(backup_path):
            shutil.copyfile(file_path, backup_path)
        with open(file_path, 'r') as file:
            content = file.read()
        if old_str not in content:
            raise ValueError('The old string does not exist in the file.')
        updated_content = content.replace(old_str, new_str)
        with open(file_path, 'w') as file:
            file.write(updated_content)
        return f"Replaced '{old_str}' with '{new_str}' in {file_path}"

    def insert_line_func(file_path: str, insertline: InsertLine, new_str: NewStr) -> str:
        """
        Insert a new string after the specified line number.

        Args:
            file_path (str): The path to the file.
            insertline (int): The line number after which to insert.
            new_str (str): The string to insert.

        Returns:
            str: Confirmation message.
        """
        if not os.path.isfile(file_path):
            raise FileNotFoundError(f'File not found: {file_path}')
        backup_path = file_path + '.bak'
        if not os.path.isfile(backup_path):
            shutil.copyfile(file_path, backup_path)
        with open(file_path, 'r') as file:
            lines = file.readlines()
        if insertline < 1 or insertline > len(lines):
            raise ValueError('Invalid line number for insertion.')
        lines.insert(insertline, new_str + '\n')
        with open(file_path, 'w') as file:
            file.writelines(lines)
        return f'Inserted line after {insertline} in {file_path}'

    def undo_edit(file_path: str) -> str:
        """
        Undo the last edit made to the file.

        Args:
            file_path (str): The path to the file.

        Returns:
            str: Confirmation message.
        """
        backup_path = file_path + '.bak'
        if not os.path.isfile(backup_path):
            raise FileNotFoundError(f'No backup found to undo for {file_path}')
        shutil.copyfile(backup_path, file_path)
        return f'Reverted the last edit in {file_path}'

    def find_occurrences(file_path: str, search_str: str, max_occurrences: int=20) -> str:
        if not os.path.isfile(file_path):
            raise FileNotFoundError(f'File not found: {file_path}')
        with open(file_path, 'r') as file:
            lines = file.readlines()
        search_str_lower = search_str.lower()
        matches = []
        for idx, line in enumerate(lines, start=1):
            if search_str_lower in line.lower():
                small_line, messages = reduceSize(line.strip())
                matches.append(f'Line {idx}: {small_line}{('\nMade modifications for viewing: ' + ', '.join(messages) if messages else '')}')
        num_matches = min(max_occurrences, len(matches))
        if num_matches > 0:
            return f"{len(matches)} occurrences of '{search_str}' found in {file_path}. Here are the first {num_matches}:\n    {'\n'.join(matches[:num_matches])}"
        return f"No occurrences of '{search_str}' found in {file_path}"

    def str_replace_editor(command: Command, insert_line: InsertLine=None, new_str: NewStr=None, old_str: OldStr=None, view_range: ViewRange=None, search_str: SearchStr=None) -> Annotated[str, 'Result string of tool call']:
        """
        Perform string replacement operations based on the provided command and parameters.

        Args:
            command (Command): The command to execute.
            insert_line (InsertLine): The line number to insert after.
            new_str (NewStr): The new string to insert or replace with.
            old_str (OldStr): The old string to replace.
            view_range (ViewRange): The range of lines to view.
            search_str (SearchStr): The string to search for.


        Returns:
            str: The updated content after performing the specified operation.
        """
        path = 'index.html'
        if not command or not path:
            raise ValueError("Both 'command' and 'path' parameters are required.")
        file_path = os.path.join(website_workspace, path)
        if command == 'view':
            return view_file(file_path, view_range)
        elif command == 'str_replace':
            return str_replace(file_path, old_str, new_str)
        elif command == 'insert':
            return insert_line_func(file_path, insert_line, new_str)
        elif command == 'undo_edit':
            return undo_edit(file_path)
        elif command == 'find':
            return find_occurrences(file_path, search_str)
        else:
            raise ValueError(f'Unsupported command: {command}')
    return str_replace_editor

def reduceSize(content: str) -> Tuple[str, List[str]]:
    messages = []
    if len(content) > THRESHOLD:
        new_content = re.sub('<svg[^>]*>.*?</svg>', '[SVG removed]', content, flags=re.DOTALL)
        if new_content != content:
            messages.append('Removed <svg> blocks')
        content = new_content
        new_content = re.sub('\\s*(data-[\\w-]+)="[^"]+"', '', content)
        if new_content != content:
            messages.append('Removed data attributes')
        content = new_content
        if len(content) > THRESHOLD:
            content = content[:THRESHOLD]
            messages.append(f'Content trimmed to first {THRESHOLD} characters')
    return (content, messages)

def view_file(file_path: str, view_range: ViewRange=None) -> str:
    """
        View the content of the file or a specific range of lines.

        Args:
            file_path (str): The path to the file.
            view_range (ViewRange, optional): The range of lines to view.

        Returns:
            str: The content to be viewed.
        """
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f'File not found: {file_path}')
    with open(file_path, 'r') as file:
        content = file.read()
    if view_range:
        lines = content.splitlines()
        start, end = view_range
        start = max(start, 1)
        end = end if end != -1 else len(lines)
        content = '\n'.join(lines[start - 1:end])
    content, messages = reduceSize(content)
    if messages:
        header = f'Modifications made to reduce to {THRESHOLD} characters: ' + ', '.join(messages) + '\n\n'
        content = header + content
    return content

def create_file(file_path: str, file_text: FileText) -> str:
    """
        Create a new file with the provided content.

        Args:
            file_path (str): The path to the new file.
            file_text (str): The content to write to the file.

        Returns:
            str: Confirmation message.
        """
    if os.path.exists(file_path):
        raise FileExistsError(f'File already exists: {file_path}')
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w') as file:
        file.write(file_text)
    return f'File created at {file_path}'

def str_replace(file_path: str, old_str: OldStr, new_str: NewStr) -> str:
    """
        Replace all occurrences of old_str with new_str in the file.

        Args:
            file_path (str): The path to the file.
            old_str (str): The string to be replaced.
            new_str (str): The string to replace with.

        Returns:
            str: Confirmation message.
        """
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f'File not found: {file_path}')
    backup_path = file_path + '.bak'
    if not os.path.isfile(backup_path):
        shutil.copyfile(file_path, backup_path)
    with open(file_path, 'r') as file:
        content = file.read()
    if old_str not in content:
        raise ValueError('The old string does not exist in the file.')
    updated_content = content.replace(old_str, new_str)
    with open(file_path, 'w') as file:
        file.write(updated_content)
    return f"Replaced '{old_str}' with '{new_str}' in {file_path}"

def insert_line_func(file_path: str, insertline: InsertLine, new_str: NewStr) -> str:
    """
        Insert a new string after the specified line number.

        Args:
            file_path (str): The path to the file.
            insertline (int): The line number after which to insert.
            new_str (str): The string to insert.

        Returns:
            str: Confirmation message.
        """
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f'File not found: {file_path}')
    backup_path = file_path + '.bak'
    if not os.path.isfile(backup_path):
        shutil.copyfile(file_path, backup_path)
    with open(file_path, 'r') as file:
        lines = file.readlines()
    if insertline < 1 or insertline > len(lines):
        raise ValueError('Invalid line number for insertion.')
    lines.insert(insertline, new_str + '\n')
    with open(file_path, 'w') as file:
        file.writelines(lines)
    return f'Inserted line after {insertline} in {file_path}'

def undo_edit(file_path: str) -> str:
    """
        Undo the last edit made to the file.

        Args:
            file_path (str): The path to the file.

        Returns:
            str: Confirmation message.
        """
    backup_path = file_path + '.bak'
    if not os.path.isfile(backup_path):
        raise FileNotFoundError(f'No backup found to undo for {file_path}')
    shutil.copyfile(backup_path, file_path)
    return f'Reverted the last edit in {file_path}'

def find_occurrences(file_path: str, search_str: str, max_occurrences: int=20) -> str:
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f'File not found: {file_path}')
    with open(file_path, 'r') as file:
        lines = file.readlines()
    search_str_lower = search_str.lower()
    matches = []
    for idx, line in enumerate(lines, start=1):
        if search_str_lower in line.lower():
            small_line, messages = reduceSize(line.strip())
            matches.append(f'Line {idx}: {small_line}{('\nMade modifications for viewing: ' + ', '.join(messages) if messages else '')}')
    num_matches = min(max_occurrences, len(matches))
    if num_matches > 0:
        return f"{len(matches)} occurrences of '{search_str}' found in {file_path}. Here are the first {num_matches}:\n    {'\n'.join(matches[:num_matches])}"
    return f"No occurrences of '{search_str}' found in {file_path}"

def str_replace_editor(command: Command, insert_line: InsertLine=None, new_str: NewStr=None, old_str: OldStr=None, view_range: ViewRange=None, search_str: SearchStr=None) -> Annotated[str, 'Result string of tool call']:
    """
        Perform string replacement operations based on the provided command and parameters.

        Args:
            command (Command): The command to execute.
            insert_line (InsertLine): The line number to insert after.
            new_str (NewStr): The new string to insert or replace with.
            old_str (OldStr): The old string to replace.
            view_range (ViewRange): The range of lines to view.
            search_str (SearchStr): The string to search for.


        Returns:
            str: The updated content after performing the specified operation.
        """
    path = 'index.html'
    if not command or not path:
        raise ValueError("Both 'command' and 'path' parameters are required.")
    file_path = os.path.join(website_workspace, path)
    if command == 'view':
        return view_file(file_path, view_range)
    elif command == 'str_replace':
        return str_replace(file_path, old_str, new_str)
    elif command == 'insert':
        return insert_line_func(file_path, insert_line, new_str)
    elif command == 'undo_edit':
        return undo_edit(file_path)
    elif command == 'find':
        return find_occurrences(file_path, search_str)
    else:
        raise ValueError(f'Unsupported command: {command}')



# File: backend/agents/data_analyst_group/tools/suggestion_editor.py

from typing import Dict, Any, List

from typing_extensions import Annotated

from utils.functions import get_dynamodb_client

import json

from datetime import datetime

def edit_suggestion(suggestion_id: Annotated[str, 'Timestamp of the suggestion to edit'], stream_key: Annotated[str, 'Stream key for the website'], updates: Annotated[Dict[str, Any], 'Fields to update in the suggestion']) -> Annotated[Dict[str, Any], 'Success status or error message']:
    """Edit a suggestion based on guardrails analysis"""
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.get_item(TableName='WebsiteReports', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion_id)}})
        if 'Item' not in response:
            return {'error': 'Suggestion not found'}
        suggestion = response['Item']
        update_expressions = []
        expression_values = {}
        expression_names = {}
        for key, value in updates.items():
            update_expressions.append(f'#{key} = :{key}')
            expression_names[f'#{key}'] = key
            if isinstance(value, bool):
                expression_values[f':{key}'] = {'BOOL': value}
            elif isinstance(value, (int, float)):
                expression_values[f':{key}'] = {'N': str(value)}
            elif isinstance(value, list):
                expression_values[f':{key}'] = {'L': [{'S': str(item)} for item in value]}
            elif isinstance(value, dict):
                expression_values[f':{key}'] = {'M': value}
            else:
                expression_values[f':{key}'] = {'S': str(value)}
        update_expressions.append('#edited_at = :edited_at')
        expression_names['#edited_at'] = 'guardrails_edited_at'
        expression_values[':edited_at'] = {'N': str(int(datetime.now().timestamp()))}
        update_expression = 'SET ' + ', '.join(update_expressions)
        dynamodb.update_item(TableName='WebsiteReports', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion_id)}}, UpdateExpression=update_expression, ExpressionAttributeNames=expression_names, ExpressionAttributeValues=expression_values)
        return {'success': True, 'message': 'Suggestion updated successfully'}
    except Exception as e:
        return {'error': str(e)}

def get_suggestion(suggestion_id: Annotated[str, 'Timestamp of the suggestion to retrieve'], stream_key: Annotated[str, 'Stream key for the website']) -> Annotated[Dict[str, Any], 'Suggestion data or error message']:
    """Get a suggestion by ID"""
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.get_item(TableName='WebsiteReports', Key={'streamKey': {'S': stream_key}, 'timestamp': {'N': str(suggestion_id)}})
        if 'Item' not in response:
            return {'error': 'Suggestion not found'}
        return response['Item']
    except Exception as e:
        return {'error': str(e)}



# File: backend/agents/data_analyst_group/tools/tavily_search.py

from tavily import TavilyClient

from typing_extensions import Annotated

from utils.functions import get_api_key

import json

def tavily_search(query: Annotated[str, 'The search query string']) -> Annotated[str, 'The search result as a string']:
    api_keys = get_api_key('AI_KEYS')
    tavily_api_key = api_keys.get('TAVILY_API_KEY')
    tavily_client = TavilyClient(api_key=tavily_api_key) if tavily_api_key else None
    if not tavily_client:
        return 'Tavily API key not found. Skipping search.'
    try:
        search_result = tavily_client.search(query=query, search_depth='advanced', include_domains=['nngroup.com', 'usability.gov', 'lawsofux.com', 'uxdesign.cc', 'uxtweak.com', 'guides', 'usabilitygeek.com', 'alistapart.com', 'customerthink.com', 'smashingmagazine.com', 'uxmag.com', 'uxmatters.com', 'uxmovement.com', 'uxmyths.com', 'uxplanet.org', 'baymard.com'])
        return json.dumps(search_result, indent=2)
    except Exception as e:
        return f'Error performing Tavily search: {str(e)}'



# File: backend/agents/data_analyst_group/tools/website_screenshot.py

import asyncio

import time

import os

import boto3

from playwright.async_api import async_playwright

from pathlib import Path

from typing import List, Optional, Literal, Annotated

from bs4 import BeautifulSoup

def get_screenshot_given_context(workspace_base: Annotated[str, 'The base directory of the website workspace'], stream_key: Annotated[str, 'The stream key associated with the session']):

    async def get_screenshot(device_type: Annotated[Literal['desktop', 'tablet', 'mobile'], 'The device type to simulate']='desktop', start_line: Annotated[Optional[int], 'The starting line number (1-based) in the html where screenshot should start from']=None, end_line: Annotated[Optional[int], 'The ending line number (1-based) in the html where the screenshots should end']=None, max_screenshots: Annotated[int, 'Maximum number of screenshots to capture']=5) -> Annotated[str, 'URLs of the images']:
        """
        Loads the full index.html, extracts the lines from start_line to end_line as a chunk,
        parses that chunk with BeautifulSoup for top-level elements, attempts to find them in the loaded page,
        and then computes the combined bounding box (lowest y, highest y) of those matched elements.

        If a top-level element has no results, this code iterates its child elements (up to 3 levels deep)
        and uses their outerHTML to find bounding boxes. If still no elements were matched, or if any issue
        arises, it falls back to a single full-page screenshot.
        """
        index_path = Path(workspace_base) / 'index.html'
        if not index_path.exists():
            print('Error: index.html does not exist in the workspace.')
            return 'index.html does not exist in the workspace.'
        with open(index_path, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()
        total_lines = len(all_lines)
        if start_line is None and end_line is None:
            start_line = 1
            end_line = total_lines
        else:
            if start_line is None:
                start_line = 1
            if end_line is None:
                end_line = total_lines
            if start_line < 1:
                start_line = 1
            if end_line > total_lines:
                end_line = total_lines
            if start_line > end_line:
                print('Invalid line range. Falling back to full-page screenshot.')
                start_line = 1
                end_line = total_lines
        chunk = ''.join(all_lines[start_line - 1:end_line])
        soup_chunk = BeautifulSoup(chunk, 'html.parser')
        top_level_elements = soup_chunk.find_all(recursive=False)
        print(f'Extracted chunk from lines {start_line} to {end_line}. Found {len(top_level_elements)} top-level elements.')

        def gather_sub_elements(parent, max_depth=3, current_depth=0):
            """
            Gathers this element plus all children up to 'max_depth' levels deep.
            Returns a list of HTML strings.
            """
            results = [str(parent).strip()]
            if current_depth >= max_depth:
                return results
            for child in parent.find_all(recursive=False):
                results.extend(gather_sub_elements(child, max_depth, current_depth + 1))
            return results
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            if device_type == 'desktop':
                viewport = {'width': 1280, 'height': 720}
            elif device_type == 'tablet':
                viewport = {'width': 768, 'height': 1024}
            elif device_type == 'mobile':
                viewport = {'width': 375, 'height': 812}
            else:
                viewport = {'width': 1280, 'height': 720}
            context = await browser.new_context(viewport=viewport, device_scale_factor=1, is_mobile=device_type == 'mobile')
            page = await context.new_page()
            await page.goto(f'file://{index_path.resolve()}')
            print('Full index.html loaded in browser.')
            await asyncio.sleep(1)
            min_y = float('inf')
            max_y = 0
            for elem in top_level_elements:
                sub_html_list = gather_sub_elements(elem, max_depth=3, current_depth=0)
                found_any = False
                for partial_html in sub_html_list:
                    if not partial_html:
                        continue
                    boxes = await page.evaluate(f'\n                        (() => {{\n                            const results = [];\n                            const allElems = document.querySelectorAll("*");\n                            for (const e of allElems) {{\n                                const outer = e.outerHTML;\n                                if (outer && outer.includes({repr(partial_html)})) {{\n                                    const rect = e.getBoundingClientRect();\n                                    if (rect.width > 0 && rect.height > 0) {{\n                                        results.push({{\n                                            top: rect.top + window.scrollY,\n                                            bottom: rect.bottom + window.scrollY\n                                        }});\n                                    }}\n                                }}\n                            }}\n                            return results;\n                        }})()\n                    ')
                    if boxes:
                        found_any = True
                        for box in boxes:
                            if box['top'] < min_y:
                                min_y = box['top']
                            if box['bottom'] > max_y:
                                max_y = box['bottom']
                if found_any:
                    print('Matched bounding boxes for at least one partial_html at top-level or child.')
                else:
                    print('No bounding boxes found for this top-level element or its children up to depth 3.')
            screenshot_urls: List[str] = []
            if min_y == float('inf') or max_y <= 0:
                print('No matching elements found. Taking a full-page screenshot.')
                fallback_path = 'fallback_screenshot.png'
                await page.screenshot(path=fallback_path, full_page=True)
                s3 = boto3.client('s3')
                bucket = 'sitewiz-websites'
                tstamp = int(time.time())
                s3_key = f'temp_screenshots/{stream_key}/fallback_{tstamp}.png'
                s3.upload_file(fallback_path, bucket, s3_key)
                fallback_url = f'https://{bucket}.s3.amazonaws.com/{s3_key}'
                screenshot_urls.append(fallback_url)
                os.remove(fallback_path)
                await browser.close()
                return '\n'.join(screenshot_urls)
            print(f'Found bounding box range from y={min_y} to y={max_y}.')
            current_scroll = min_y
            screenshots_taken = 0
            while current_scroll < max_y and screenshots_taken < max_screenshots:
                clip_height = min(viewport['height'], max_y - current_scroll)
                if clip_height <= 0:
                    break
                await page.evaluate(f'window.scrollTo(0, {current_scroll});')
                await asyncio.sleep(1)
                shot_name = f'screenshot_{screenshots_taken + 1}.png'
                try:
                    await page.screenshot(path=shot_name, clip={'x': 0, 'y': current_scroll, 'width': viewport['width'], 'height': clip_height})
                except Exception as e:
                    print(f'Warning: Screenshot error at y={current_scroll}: {e}')
                    break
                print(f'Captured {shot_name}.')
                s3 = boto3.client('s3')
                bucket = 'sitewiz-websites'
                tstamp = int(time.time())
                s3_key = f'temp_screenshots/{stream_key}/screenshot_{tstamp}_{screenshots_taken + 1}.png'
                s3.upload_file(shot_name, bucket, s3_key)
                shot_url = f'https://{bucket}.s3.amazonaws.com/{s3_key}'
                screenshot_urls.append(shot_url)
                os.remove(shot_name)
                current_scroll += viewport['height']
                screenshots_taken += 1
            if screenshots_taken == 0:
                print('No bounding-box screenshots taken, capturing fallback full-page.')
                shot_name = 'fullpage.png'
                await page.screenshot(path=shot_name, full_page=True)
                s3 = boto3.client('s3')
                bucket = 'sitewiz-websites'
                tstamp = int(time.time())
                s3_key = f'temp_screenshots/{stream_key}/fullpage_{tstamp}.png'
                s3.upload_file(shot_name, bucket, s3_key)
                shot_url = f'https://{bucket}.s3.amazonaws.com/{s3_key}'
                screenshot_urls.append(shot_url)
                os.remove(shot_name)
            if screenshots_taken == max_screenshots:
                print(f'Reached maximum {max_screenshots} screenshots and stopped taking screenshots.')
            await browser.close()
            print('Browser closed. Final screenshots:', screenshot_urls)
            return '\n'.join(screenshot_urls)
    return get_screenshot

def gather_sub_elements(parent, max_depth=3, current_depth=0):
    """
            Gathers this element plus all children up to 'max_depth' levels deep.
            Returns a list of HTML strings.
            """
    results = [str(parent).strip()]
    if current_depth >= max_depth:
        return results
    for child in parent.find_all(recursive=False):
        results.extend(gather_sub_elements(child, max_depth, current_depth + 1))
    return results



# File: backend/agents/data_analyst_group/utils/__init__.py



# File: backend/agents/data_analyst_group/utils/evaluation.py

from typing import List, Dict, Any, Callable, Tuple

import json

from pydantic import BaseModel, Field, field_validator

from utils.functions import run_completion_with_fallback, get_dynamodb_client, suggestion_to_markdown, insight_to_markdown

import ast

from autogen_core.code_executor import CodeBlock

from autogen_core import CancellationToken

import asyncio

import traceback

def validate_reach_code(code: str) -> Tuple[str, bool]:
    try:
        tree = ast.parse(code)
    except SyntaxError as e:
        return (str(e), False)
    node = None
    for tree_node in tree.body:
        if isinstance(tree_node, ast.FunctionDef):
            node = tree_node
            break
    if isinstance(node, ast.FunctionDef):
        node.name = 'calculate_reach'
        function_code = ast.unparse(node)
        print('Extracted Function:\n', function_code)
        return (function_code, True)
    else:
        return ('calculate_reach is not the top-level function', False)

def get_reach(code: str, executor, stream_key):
    code_validation_output, is_valid_code = validate_reach_code(code)
    if not is_valid_code:
        return (code_validation_output, False, '')
    before_code = f"""# you must use these exact imports in your code\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom datetime import timedelta\nfrom functions import run_sitewiz_query \nfrom typing import TypedDict, List, Tuple\n\nclass ReachOutput(TypedDict):\n    Description: str\n    start_date: str\n    end_date: str\n    values: List[Tuple[str, float]]\n\nstream_key = '{stream_key}'\n\n# Get yesterday's date as end_date\nend_date = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime("%Y-%m-%d")\n# Get date 6 days before end_date as start_date\nstart_date = (datetime.datetime.now() - datetime.timedelta(days=7)).strftime("%Y-%m-%d")\n\nstart_time = int(datetime.datetime.strptime(start_date, "%Y-%m-%d").timestamp())\nend_time = int(datetime.datetime.strptime(end_date, "%Y-%m-%d").timestamp())"""
    end_code = 'output = calculate_reach(start_date, end_date)\nprint("Calculate Reach Output:")\nprint(output)'
    code = before_code + '\n' + code_validation_output + '\n' + end_code
    print(code)
    output_dict = None
    exit_code, result = asyncio.run(evaluate_code(code, executor))
    print(result)
    if exit_code == 1:
        return ('The reach function code is not correct. Please have the python analyst fix this error: \n' + result, False, code_validation_output)
    try:
        output_start = result.rfind('Calculate Reach Output:')
        if output_start != -1:
            output_text = result[output_start:].split('\n', 1)[1].strip()
            output_dict = eval(output_text)
            formatted_output = f'Values: {output_dict['values']}\nDescription: {output_dict['Description']}\nDate Range: {output_dict['start_date']} to {output_dict['end_date']}\n'
            print('\nExtracted metrics:')
            print(formatted_output)
    except Exception as e:
        return (f"The code had trouble extracting the reach with error: '{e}'. Please have the python analyst fix this error. Here was the output of the code: \n{result}", False, code_validation_output)
    print(output_dict)
    total_sessions = sum([v for _, v in output_dict['values']])
    average_reach = total_sessions / len(output_dict['values'])
    print(f'Average Reach per Day: {average_reach}')
    output_dict['reach'] = average_reach
    return (output_dict, True, code_validation_output)

def run_evaluation(documents: Dict[str, Dict[str, Any]], questions: List[Dict[str, Any]], n_times: int=2, partition: str='') -> List[EvaluationResult]:
    """
    Evaluates each question using chain-of-thought reasoning and enforces individual confidence thresholds.
    Runs multiple times (`n_times`) and checks if all evaluations exceed the threshold.

    Args:
        documents (Dict[str, Dict[str, Any]]): 
            Dictionary where each key maps to:
                - {"type": "text", "content": "...", "description": "..."}  
                - {"type": "image", "content": ["image_path1", "image_path2"], "description": "..."}
        questions (List[Dict[str, Any]]): 
            List of dicts containing:
                - "question": The question text.
                - "output": List of document fields to evaluate.
                - "reference": List of document fields that serve as ground truth.
                - "confidence_threshold" (optional): Minimum confidence required (defaults to 0.99).
        n_times (int): 
            Number of times to run the evaluation for each question.

    Returns:
        List[EvaluationResult]: A list of evaluation results.
    """
    results = []
    for q in questions:
        try:
            confidence_threshold = q.get('confidence_threshold', 0.99)
            all_evaluations = []
            for _ in range(n_times):
                messages = []
                messages.append({'role': 'system', 'content': 'You are an unbiased, strict evaluator with advanced reasoning skills.'})
                messages.append({'role': 'user', 'content': f'Question: {q.get('question')}'})

                def add_messages_from_data(data_fields: List[str], role: str):
                    for field in data_fields:
                        if field not in documents:
                            continue
                        entry = documents[field]
                        if entry['type'] == 'text':
                            messages.append({'role': role, 'content': f'Here is the {entry['description']}:\n\n{entry['content']}'})
                        elif entry['type'] == 'image':
                            image_messages = [{'type': 'text', 'text': f'Here is the {entry['description']}'}]
                            for image_url in entry['content']:
                                image_messages.append({'type': 'image_url', 'image_url': {'url': image_url}})
                            messages.append({'role': role, 'content': image_messages})
                add_messages_from_data(q.get('output', []), 'user')
                add_messages_from_data(q.get('reference', []), 'user')
                messages.append({'role': 'user', 'content': f"""\nYou trust the reference data, but you don't trust the output data. \nEven if the output data makes a claim, you don't trust the claim unless there is irrefutable evidence.\n\nInstructions:\n1. Analyze how well the output data meets the requirements posed by the question in comparison to the reference data.\n2. Provide a detailed chain-of-thought reasoning of your analysis.\n3. Conclude with a final evaluation using the exact JSON format provided:\n   {{\n      "question": "<the original question>",\n      "answer": "<Yes or No>",\n      "confidence": <a float between 0 and 1>,\n      "explanation": "<a concise summary of your reasoning>"\n   }}\n4. If your final confidence score is below {confidence_threshold} and you answer "Yes", override your final answer to "No" and state that the confidence is insufficient.\n5. Output only the JSON object.\n"""})
                evaluation_data = run_completion_with_fallback(messages=messages, models=['video'], response_format=BaseEvaluationResult)
                evaluation_data['partition'] = partition
                evaluation_data['output'] = q.get('output', [])
                evaluation_data['reference'] = q.get('reference', [])
                all_evaluations.append(evaluation_data)
                if evaluation_data.get('confidence', 0) < confidence_threshold:
                    evaluation_data['answer'] = 'No'
                    evaluation_data['explanation'] += f' (Final confidence of {evaluation_data.get('confidence', 0)} below threshold {confidence_threshold}.)\n'
                    evaluation_data['explanation'] += q.get('feedback', '')
                evaluation_result = EvaluationResult(**evaluation_data)
                if evaluation_result.answer == 'No':
                    print('Validation failed on one or more runs:', all_evaluations)
                    return all_evaluations
            results.append(evaluation_result)
        except Exception as e:
            evaluation_result = EvaluationResult(question=q.get('question'), answer='No', confidence=0.0, explanation=f'Error: {e}', partition=partition, output=[], reference=[])
            results.append(evaluation_result)
            traceback.print_exc()
            return results
    return results

def interpret_evaluations(validation_results: List[EvaluationResult], main_message):
    msg = ''
    print(validation_results)
    for result in validation_results:
        if isinstance(result, dict):
            result = EvaluationResult(**result)
        msg += f'Question: {result.question}\n'
        msg += f'Answer: {result.answer}\n'
        msg += f'Explanation: {result.explanation}\n'
        msg += f'Confidence: {result.confidence}\n\n'
        if result.answer == 'No':
            output_message = main_message
            output_message = 'The failure was in this question:'
            output_message += f'Question: {result.question}\n'
            output_message += f'Explanation: {result.explanation}\n'
            output_message += f'Confidence: {result.confidence}\n\n'
            output_message += 'Here is what went well: \n' + msg
            return (output_message, False)
    output_message = 'The scoring was a success! Here is what went well: \n' + msg
    return (output_message, True)

def evaluate_impact_confidence(suggestion: Dict[str, Any], n_times: int=1) -> Tuple[float, float]:
    suggestion_markdown = suggestion_to_markdown(suggestion)
    prompt = f'Estimate the confidence and impact of this suggestion for A/B testing prioritization. You must use the available data, quality of data, and related test results to estimate your result.\n\nCite your sources in your explanation. The explanation should be in markdown format.\n    \n{suggestion_markdown}\n\nProvide your evaluation in the following format:\n{{\n    "impact": <float between 0 and 1>,\n    "confidence": <float between 0 and 1>,\n    "explanation": "<detailed explanation of your reasoning>"\n}}'
    total_impact = 0.0
    total_confidence = 0.0
    explanation = ''
    for _ in range(n_times):
        result = run_completion_with_fallback(prompt=prompt, models=['reasoning-mini'], response_format=ImpactConfidenceResult)
        total_impact += result.impact
        total_confidence += result.confidence
        explanation = result.explanation
    avg_impact = total_impact / n_times
    avg_confidence = total_confidence / n_times
    return (avg_impact, avg_confidence, explanation)

def evaluate_insight_sfrc(insight: Dict[str, Any], n_times: int=1) -> Tuple[float, float, float, float, str]:
    insight_markdown = insight_to_markdown(insight)
    prompt = f'Evaluate this insight using the SFC framework:\n\nSeverity (S): How severe is the issue? (Rate 1 - 5)\n- Impact on user experience\n- Business impact\n- Technical debt implications\n\nConfidence (C): How confident are we in this evaluation?\n- Data quality\n- Sample size\n- Correlation strength\n- Historical precedent\n\nUse available data, metrics, and related insights to inform your evaluation.\nCite your sources in your explanation. The explanation should be in markdown format.\n    \n{insight_markdown}\n\nProvide your evaluation in the following format:\n{{\n    "severity": <int between 1 and 5>,\n    "confidence": <float between 0 and 1>,\n    "explanation": "<detailed explanation of your reasoning>"\n}}'
    total_severity = 0.0
    total_confidence = 0.0
    explanation = ''
    for _ in range(n_times):
        result = run_completion_with_fallback(prompt=prompt, models=['reasoning-mini'], response_format=SFRCResult)
        total_severity += result.severity
        total_confidence += result.confidence
        explanation = result.explanation
    avg_severity = total_severity / n_times
    avg_confidence = total_confidence / n_times
    return (avg_severity, avg_confidence, explanation)

def store_evaluations(streamKey: str, partition: str, evaluations: List[EvaluationResult]):
    """
    Stores evaluation results into the DynamoDB TraceabilityTable.

    Args:
        streamKey (str): The primary key for the DynamoDB table.
        partition (str): The partition key used as the first part of compositeKey.
        evaluations (List[EvaluationResult]): List of evaluation results to store.
    """
    if not evaluations:
        print('No evaluations to store.')
        raise ValueError('No evaluations to store.')
    try:
        table_name = 'TraceabilityTable'
        dynamodb = get_dynamodb_client()
        for idx, evaluation in enumerate(evaluations):
            composite_key = f'{partition}#{idx:04d}'
            if isinstance(evaluation, dict):
                evaluation = EvaluationResult(**evaluation)
            item = {'streamKey': {'S': streamKey}, 'compositeKey': {'S': composite_key}, 'question': {'S': evaluation.question}, 'answer': {'S': evaluation.answer}, 'confidence': {'N': str(evaluation.confidence)}, 'explanation': {'S': evaluation.explanation}, 'output': {'S': json.dumps(evaluation.output)}, 'reference': {'S': json.dumps(evaluation.reference)}, 'partition': {'S': evaluation.partition}}
            dynamodb.put_item(TableName=table_name, Item=item)
    except Exception as e:
        print(f'Error storing evaluations: {e}')
        raise e

@field_validator('confidence')
@classmethod
def check_confidence(cls, v):
    if not 0 <= v <= 1:
        raise ValueError('Confidence must be between 0 and 1')
    return v

@field_validator('confidence')
@classmethod
def check_confidence(cls, v):
    if not 0 <= v <= 1:
        raise ValueError('Confidence must be between 0 and 1')
    return v

@field_validator('impact', 'confidence')
@classmethod
def check_values(cls, v):
    if not 0 <= v <= 1:
        raise ValueError('Values must be between 0 and 1')
    return v

def add_messages_from_data(data_fields: List[str], role: str):
    for field in data_fields:
        if field not in documents:
            continue
        entry = documents[field]
        if entry['type'] == 'text':
            messages.append({'role': role, 'content': f'Here is the {entry['description']}:\n\n{entry['content']}'})
        elif entry['type'] == 'image':
            image_messages = [{'type': 'text', 'text': f'Here is the {entry['description']}'}]
            for image_url in entry['content']:
                image_messages.append({'type': 'image_url', 'image_url': {'url': image_url}})
            messages.append({'role': role, 'content': image_messages})



# File: backend/agents/data_analyst_group/utils/extra_functions.py

from tools.get_element import get_element_description

from tools.get_session_recording import get_session_recording_description

from tools.run_bigquery_query import run_bigquery_query_description

from tools.run_sitewiz_query import run_sitewiz_query_description

from tools.get_heatmap import get_heatmap_description

def readable_functions(functions):
    function_details = ''
    for func in functions:
        name, description = func
        function_details += f'\n    - **Function**: {name}\n    - **Description**: {description}\n    '
    return function_details

def get_function_descriptions():
    functions_module = 'functions'
    functions = [get_element_description, get_session_recording_description, run_bigquery_query_description, run_sitewiz_query_description, get_heatmap_description]
    function_descriptions = [function(functions_module, {}) for function in functions]
    return readable_functions(function_descriptions)



# File: backend/agents/data_analyst_group/utils/functions.py

import boto3

import json

import psycopg2

import os

import tempfile

import decimal

from decimal import Decimal

import litellm

from litellm.utils import trim_messages

from litellm import completion

from datetime import datetime, timedelta, timezone

from boto3.dynamodb.conditions import Key, Attr

from typing import List, Dict, Any, Optional, Annotated

import time

from zep_cloud.client import Zep

from autogen_ext.models.openai import OpenAIChatCompletionClient, AzureOpenAIChatCompletionClient

from dotenv import load_dotenv

from autogen_agentchat import messages

from pathlib import Path

from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor

import re

import asyncio

import math

import ast

from botocore.exceptions import ClientError

import traceback

from pydantic import BaseModel, Field, field_validator

import weave

import base64

from PIL import Image

import requests

def save_to_memory(data, stream_key):
    api_keys = get_api_key('AI_KEYS')
    ZEP_API_KEY = api_keys['ZEP_API_KEY']
    zep = Zep(api_key=ZEP_API_KEY)
    try:
        zep.graph.add(group_id=stream_key, data=data, type='json')
    except Exception as e:
        print(f'Error saving to memory: {e}')

def save_results(key, value):
    script_dir = os.path.dirname(os.path.abspath(__file__))
    results_dir = os.path.join(script_dir, 'results')
    results_file = os.path.join(results_dir, 'results.json')
    os.makedirs(results_dir, exist_ok=True)
    try:
        with open(results_file, 'r') as f:
            results = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        results = {}
    if key not in results:
        results[key] = []
    results[key].append(value)
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

def fetch_results(key=None):
    script_dir = os.path.dirname(os.path.abspath(__file__))
    results_file = os.path.join(script_dir, 'results', 'results.json')
    try:
        with open(results_file, 'r') as f:
            results = json.load(f)
            if key is not None:
                return results.get(key, [])
            return results
    except (FileNotFoundError, json.JSONDecodeError):
        if key is not None:
            return []
        return {}

def convert_message_to_dict(message: messages) -> Dict[str, Any]:
    if message.type == 'TextMessage':
        return {'message': message.content}
    if message.type == 'ToolCallRequestEvent':
        function_calls = []
        for call in message.content:
            function_calls.append({'name': call.name, 'arguments': call.arguments})
        return {'function_calls': function_calls}
    return None

def get_settings(stream_key: str) -> Dict[str, Any]:
    """Get settings from DynamoDB"""
    try:
        dynamodb = boto3.client('dynamodb', region_name='us-east-1')
        response = dynamodb.get_item(TableName='WebsiteData', Key={'streamKey': {'S': stream_key}})
        settings = json.loads(response.get('Item', {}).get('settings', '{}'))
        return settings
    except Exception as e:
        print(f'Error getting settings: {e}')
        return {}

def is_termination_msg(x):
    try:
        output = x.get('content') and 'TERMINATE' in x.get('content') and ('TERMINATE' in x.get('content').strip()[-14:])
        return output
    except:
        return False

def run_completion_with_fallback(messages=None, prompt=None, models=model_fallback_list, response_format=None, temperature=None):
    """
    Run completion with fallback to evaluate.
    """
    initialize_vertex_ai()
    if messages is None:
        if prompt is None:
            raise ValueError('Either messages or prompt should be provided.')
        else:
            messages = [{'role': 'user', 'content': prompt}]
    trimmed_messages = messages
    try:
        trimmed_messages = trim_messages(messages, model)
    except Exception as e:
        pass
    for model in models:
        try:
            if response_format is None:
                response = completion(model='litellm_proxy/' + model, messages=trimmed_messages, temperature=temperature)
                content = response.choices[0].message.content
                return content
            else:
                response = completion(model='litellm_proxy/' + model, messages=trimmed_messages, response_format=response_format, temperature=temperature)
                content = json.loads(response.choices[0].message.content)
                if isinstance(response_format, BaseModel):
                    response_format.model_validate(content)
                return content
        except Exception as e:
            print(f'Failed to run completion with model {model}. Error: {str(e)}')
    return None

def substitute_single_braces(text: str, variables: List[Dict[str, Any]], derivations: List[Dict[str, Any]], references: List[Dict[str, Any]]) -> str:
    variable_map = {v['variable_name']: v['readable'] for v in variables}
    variable_map.update({d['variable_name']: str(d['value']) for d in derivations})
    variable_map.update({r['key']: r['readable'] for r in references})

    def replace_braces(match):
        var_name = match.group(1)
        return variable_map.get(var_name, f'{{{var_name}}}')
    return re.sub('\\{([^{}]+)\\}', replace_braces, text)

def evaluate_calc(expr: str) -> float:
    try:
        return eval(expr)
    except:
        return float('nan')

def process_data_statement(statement: str, variables: List[Dict[str, Any]], derivations: List[Dict[str, Any]], references: List[Dict[str, Any]]=[]) -> str:
    replaced = substitute_single_braces(statement, variables, derivations, references)
    parts = re.split('(\\{calc\\([\\s\\S]*?\\)\\})', replaced)
    result_parts = []
    for part in parts:
        calc_match = re.match('\\{calc\\(([\\s\\S]*?)\\)\\}', part)
        if calc_match:
            expr = calc_match.group(1)
            expr = substitute_single_braces(expr, variables, derivations, references)
            result = evaluate_calc(expr)
            result_parts.append(f'{result:.1f}' if not isinstance(result, float) or not math.isnan(result) else 'N/A')
        else:
            result_parts.append(part)
    return ''.join(result_parts)

def get_dynamodb_client():
    aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')
    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
    aws_region = os.environ.get('AWS_REGION') or 'us-east-1'
    dynamodb_client = boto3.client('dynamodb', region_name=aws_region, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)
    return dynamodb_client

def get_data(stream_key: str) -> Dict[str, Any]:
    """
    Get OKRs, insights and suggestions with markdown representations and relationship counts.
    Each OKR includes the number of insights connected.
    Each insight includes the number of suggestions connected.
    The 'code' list is a subset of suggestions that include a Code field.
    """
    try:
        okr_table = get_dynamodb_table('website-okrs')
        insight_table = get_dynamodb_table('website-insights')
        suggestion_table = get_dynamodb_table('WebsiteReports')
        one_week_ago_ms = int((datetime.now() - timedelta(days=7)).timestamp() * 1000)
        one_week_ago_s = int((datetime.now() - timedelta(days=7)).timestamp())
        okr_response = okr_table.query(KeyConditionExpression=Key('streamKey').eq(stream_key), FilterExpression=Attr('verified').eq(True))
        okrs = okr_response.get('Items', [])
        insight_response = insight_table.query(KeyConditionExpression=Key('streamKey').eq(stream_key) & Key('timestamp').gte(one_week_ago_ms), FilterExpression=Attr('verified').eq(True))
        insights = [item for item in insight_response.get('Items', []) if 'okr_name' in item]
        suggestion_response = suggestion_table.query(KeyConditionExpression=Key('streamKey').eq(stream_key) & Key('timestamp').gte(one_week_ago_s), FilterExpression=Attr('verified').eq(True))
        suggestions = [item for item in suggestion_response.get('Items', []) if 'InsightConnectionTimestamp' in item]
        processed_data = {'okrs': [], 'insights': [], 'suggestions': [], 'code': []}
        okr_map = {}
        for okr in okrs:
            okr_name = okr.get('name', 'N/A')
            okr_record = {'markdown': okr_to_markdown(okr), 'name': okr_name, 'insight_count': 0}
            processed_data['okrs'].append(okr_record)
            okr_map[okr_name] = okr_record
        insight_map = {}
        for insight in insights:
            okr_name = insight.get('okr_name', 'N/A')
            insight_id = str(insight.get('timestamp', '0'))
            insight_record = {'markdown': insight_to_markdown(insight), 'okr_name': okr_name, 'timestamp': insight_id, 'suggestion_count': 0}
            processed_data['insights'].append(insight_record)
            insight_map[insight_id] = insight_record
            if okr_name in okr_map:
                okr_map[okr_name]['insight_count'] += 1
        for suggestion in suggestions:
            insight_id = str(suggestion.get('InsightConnectionTimestamp', '0'))
            has_code = suggestion.get('Code') is not None
            suggestion_record = {'markdown': suggestion_to_markdown(suggestion, timestamp=True), 'timestamp': suggestion['timestamp'], 'InsightConnectionTimestamp': insight_id, 'has_code': has_code}
            processed_data['suggestions'].append(suggestion_record)
            if insight_id in insight_map:
                insight_map[insight_id]['suggestion_count'] += 1
            if has_code:
                processed_data['code'].append(suggestion_record)
        return processed_data
    except Exception as e:
        print(f'Error processing data: {e}')
        traceback.print_exc()
        return None

def split_image(filepath: str, max_height: int=1024, max_return: int=3) -> List[str]:
    img = Image.open(filepath)
    width, height = img.size
    num_splits = height // max_height
    if height % max_height != 0:
        num_splits += 1
    directory, filename = os.path.split(filepath)
    name, ext = os.path.splitext(filename)
    split_filepaths = []
    for i in range(num_splits):
        if i >= max_return:
            break
        start = i * max_height
        end = min(start + max_height, height)
        split = img.crop((0, start, width, end))
        split_filename = f'{name}_part_{i}{ext}'
        split_filepath = os.path.join(directory, split_filename)
        split.save(split_filepath)
        split_filepaths.append(split_filepath)
    return split_filepaths

def encode_image(image_path: str) -> str:
    with open(image_path, 'rb') as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def get_analytics_details(data, stream_key: str) -> Dict[str, Any]:
    try:
        timestamp = data['key']
        explanation = data['explanation']
        name = data['name']
        description = f'An insight from querying the database with name {name}'
        insight, markdown = get_insight(stream_key, timestamp)
        if insight is None or markdown is None:
            return (None, 'Error retrieving insight from analytics. Ensure the key timestamp of insight is correct.')
        return ({'role': 'user', 'content': [{'type': 'text', 'text': markdown}]}, description)
    except Exception as e:
        print(f'Error retrieving analytics details: {e}')
        return (None, f'Error retrieving analytics details: {e}')

def get_heatmap_details(data, split=True):
    try:
        heatmap_id = data['key']
        explanation = data['explanation']
        name = data['name']
        description = f'Here is the heatmap with ID {heatmap_id} and name {name}'
        heatmap_bucket_name = 'sitewiz-websites'
        heatmap_url = f'https://{heatmap_bucket_name}.s3.amazonaws.com/{heatmap_id}'
        content = [{'type': 'text', 'text': f'Here is the heatmap with ID {heatmap_id} and explanation {explanation}'}]
        response = requests.get(heatmap_url)
        if response.status_code != 200:
            msg = f'The heatmap with ID {heatmap_id} is not available. Please provide a valid heatmap.'
            print(msg)
            return (None, description)
        if split:
            images_dir = f'/tmp/{heatmap_id}'
            os.makedirs(images_dir, exist_ok=True)
            url = heatmap_url
            image_filename = url.split('/')[-1]
            image_filepath = os.path.join(images_dir, image_filename)
            download_image(url, image_filepath)
            split_images = split_image(image_filepath)
            for image_path in split_images:
                base64_image = encode_image(image_path)
                content.append({'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{base64_image}'}})
        else:
            content.append({'type': 'image_url', 'image_url': {'url': heatmap_url}})
        return ({'role': 'user', 'content': content}, description)
    except Exception as e:
        return (None, f'Error retrieving heatmap details: {e}')

def download_image(url: str, filepath: str):
    response = requests.get(url)
    response.raise_for_status()
    with open(filepath, 'wb') as f:
        f.write(response.content)

def get_video_details(data, include_video=True, explanation=''):
    video_id = data['key']
    explanation = data['explanation']
    name = data['name']
    video_bucket_name = 'sitewiz-videos'
    session_key = video_id
    session_url = ''
    description = f'Here is the video with ID {session_key} and name {name}'
    content = [{'type': 'text', 'text': f'Here is the video with ID {session_key} and explanation {explanation}'}]
    try:
        connection = get_db_connection()
        cursor = connection.cursor()
        cursor.execute('\n            SELECT\n                s.device_form,\n                s.region,\n                sr.duration,\n                sr.filepath,\n                ss.summary\n            FROM sessions s\n            JOIN session_recordings sr ON s.session_id = sr.session_id\n            LEFT JOIN session_summaries ss ON s.session_id = ss.session_id\n            WHERE s.session_id = %s\n        ', (session_key,))
        result = cursor.fetchone()
        if result:
            device_form, region, duration, filepath, summary = result
            device_type = device_types.get(int(device_form), 'unknown')
            description = f'Here is the info for video with ID {session_key} and name {name}:\n            - Device: {device_type}\n            - Region: {region}\n            - Duration: {duration} seconds\n            - Summary: {summary}\n            '
            inputFilePath = filepath
            videoFileName = ''.join((e if e.isalnum() else '_' for e in inputFilePath)).lower()
            s3Key = f'videos/{videoFileName}.mp4'
            session_url = f'https://{video_bucket_name}.s3.amazonaws.com/{s3Key}'
        else:
            print('No data found for the given session key.')
            return (None, description)
        content.append({'type': 'text', 'text': description})
        if include_video:
            content.append({'type': 'image_url', 'image_url': {'url': session_url}})
        return ({'role': 'user', 'content': content}, description)
    except Exception as e:
        return (None, f'Error retrieving session recording details: {e}')
    finally:
        if connection:
            connection.close()

def get_okr_name(stream_key: str, okr_name: str) -> Annotated[tuple[str, bool], 'Result message with OKR data and success status']:
    dynamodb = get_dynamodb_client()
    try:
        response = dynamodb.query(TableName='website-okrs', KeyConditionExpression='streamKey = :sk', FilterExpression='name = :n', ExpressionAttributeValues={':sk': {'S': stream_key}, ':n': {'S': okr_name}})
        if not response['Items']:
            return ('No OKR found for this stream key and name', False)
        item = response['Items'][0]
        okr = {'name': item['name']['S'], 'code': item['code']['S'], 'description': item['description']['S']}
        return (json.dumps(okr, indent=2), True)
    except Exception as e:
        return (f'Error retrieving OKR: {e}', False)

def get_dynamodb_table(table):
    aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')
    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
    aws_region = os.environ.get('AWS_REGION') or 'us-east-1'
    dynamodb_resource = boto3.resource('dynamodb', region_name=aws_region, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)
    return dynamodb_resource.Table(table)

def process_messages_success(messages, tool_name, eval_group, group, max_turns=500):
    try:
        function_calls = {}
        print('start processing messages')
        for message in messages:
            if not eval_group['stored']:
                eval_group['num_turns'] += 1
            if message.type == 'ToolCallRequestEvent':
                for call in message.content:
                    if call.name == tool_name:
                        function_calls[call.id] = {'name': call.name, 'arguments': call.arguments}
            elif message.type == 'ToolCallExecutionEvent':
                for result in message.content:
                    if result.call_id in function_calls:
                        eval_group['attempts'] += 1
                        try:
                            parts = result.content.strip('()').split(',', 1)
                            result_tuple = ast.literal_eval(result.content)
                            msg_part = result_tuple[0]
                            success = result_tuple[1]
                            if success:
                                eval_group['successes'] += 1
                                eval_group['stored'] = True
                                eval_group['success_outputs'].append(msg_part)
                            else:
                                eval_group['failures'] += 1
                                eval_group['failure_reasons'].append(f'{tool_name} with arguments failed: {msg_part}\n\nCall arguments:  {json.dumps(function_calls[result.call_id]['arguments'], indent=4, cls=DecimalEncoder)}\n\n')
                        except Exception as e:
                            print('Result Content')
                            print(result.content)
                            try:
                                if 'successfully' in result.content.lower():
                                    eval_group['successes'] += 1
                                    eval_group['stored'] = True
                                else:
                                    eval_group['failures'] += 1
                                    eval_group['failure_reasons'].append(f'{tool_name} failed ({str(e)}): {result.content}. Call arguments:  {json.dumps(function_calls[result.call_id]['arguments'])}')
                            except Exception as e:
                                eval_group['failures'] += 1
                                eval_group['failure_reasons'].append(f'{tool_name} error: {str(e)}')
        stats = eval_group
        total_attempts = stats['attempts']
        total_successes = stats['successes']
        num_turns = stats['num_turns']
        summary_lines = [f"Chat Evaluation Summary for group '{group}':", f'Total Attempts: {total_attempts}', f'Total Successes: {total_successes}']
        if not stats['stored']:
            stats['num_turns'] = max_turns
        else:
            summary_lines.append(f'Number of Turns until Success: {num_turns}')
        if stats['success_outputs']:
            summary_lines.append('Success Outputs:')
            for reason in stats['success_outputs']:
                summary_lines.append(f'  - {reason}')
        if stats['failure_reasons']:
            summary_lines.append('Failure Reasons:')
            for reason in stats['failure_reasons']:
                summary_lines.append(f'  - {reason}')
        summary = '\n'.join(summary_lines)
        return (stats, summary)
    except Exception as e:
        print(e)
        traceback.print_exc()
        return (None, None)

def suggestion_to_markdown_parts(item: Dict[str, Any], timestamp=False) -> Dict[str, str]:
    """
    Convert a suggestion to markdown parts.
    """
    parts = {}
    if timestamp:
        timestamp_int = int(item.get('timestamp', 0))
        parts['timestamp'] = f'## Timestamp\n- {datetime.fromtimestamp(timestamp_int, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}\n'
    parts['header'] = ''
    if 'Shortened' in item:
        for shortened in item.get('Shortened', []):
            if shortened.get('type') == 'header':
                parts['header'] += f'## {shortened.get('text', '')}\n'
    parts['tags'] = ''
    if 'Tags' in item:
        parts['tags'] = '## Tags\n'
        for tag in item.get('Tags', []):
            parts['tags'] += f'- **{tag.get('type', '')}:** {tag.get('Value', '')} ({tag.get('Tooltip', '')})\n'
    parts['expanded'] = ''
    if 'Expanded' in item:
        for expanded in item.get('Expanded', []):
            if expanded.get('type') == 'text':
                parts['expanded'] += f'### {expanded.get('header', '')}\n\n{expanded.get('text', '')}\n'
    parts['insights'] = ''
    if 'Insights' in item:
        parts['insights'] = '## Insights\n'
        for insight in item.get('Insights', []):
            if 'data' in insight:
                for data_point in insight.get('data', []):
                    if data_point.get('type') == 'Heatmap':
                        parts['insights'] += f'- **Heatmap (id: {data_point.get('key', '')}, {data_point.get('name', '')}):** [{data_point.get('explanation', '')}]\n'
                    elif data_point.get('type') == 'Session Recording':
                        parts['insights'] += f'- **Session Recording (id: {data_point.get('key', '')}, {data_point.get('name', '')}):** [{data_point.get('explanation', '')}]\n'
                    else:
                        parts['insights'] += f'- **{data_point.get('type')} (id: {data_point.get('key', '')}, {data_point.get('name', '')}):** [{data_point.get('explanation', '')}]\n'
            parts['insights'] += f'{insight.get('text', '')}\n'
    return parts

def suggestion_to_markdown(item: Dict[str, Any], timestamp=False) -> str:
    """
    Convert a suggestion to markdown using suggestion_to_markdown_parts.
    """
    parts = suggestion_to_markdown_parts(item, timestamp)
    markdown = []
    if timestamp and parts.get('timestamp'):
        markdown.append(parts['timestamp'])
    if parts.get('header'):
        markdown.append(parts['header'])
    if parts.get('tags'):
        markdown.append(parts['tags'])
    if parts.get('expanded'):
        markdown.append(parts['expanded'])
    if parts.get('insights'):
        markdown.append(parts['insights'])
    return '\n'.join(markdown)

def evaluate_calc_expression(expr: str, substitutions: dict) -> str:
    """Evaluate a calc expression by substituting values and computing the result."""
    for var, value in substitutions.items():
        expr = expr.replace(f'{{{var}}}', str(value))
    try:
        result = eval(expr)
        return f'{float(result):.2f}'
    except:
        return expr

def insight_to_markdown(insight: dict) -> str:
    """Convert an insight to markdown format with substituted values."""
    try:
        variables = insight.get('variables', '[]')
        if isinstance(variables, str):
            variables = json.loads(variables)
        derivation = insight.get('derivation', '[]')
        if isinstance(derivation, str):
            derivation = json.loads(derivation)
        substitutions = {}
        for var in variables:
            substitutions[var['variable_name']] = var['readable']
        for deriv in derivation:
            if 'value' in deriv:
                substitutions[deriv['variable_name']] = deriv['value']
        markdown = '# Insight Analysis\n\n'
        data_statement = insight.get('data_statement', '')
        calc_pattern = '\\{calc\\((.*?)\\)\\}'
        while True:
            match = re.search(calc_pattern, data_statement)
            if not match:
                break
            expr = match.group(1)
            result = evaluate_calc_expression(expr, substitutions)
            data_statement = data_statement.replace(match.group(0), result)
        for var_name, value in substitutions.items():
            data_statement = data_statement.replace(f'{{{var_name}}}', str(value))
        markdown += f'## Data Statement\n{data_statement}\n\n'
        markdown += f'## Problem Statement\n{insight.get('problem_statement', '')}\n\n'
        markdown += f'## Business Objective\n{insight.get('business_objective', '')}\n\n'
        markdown += f'## Hypothesis\n{insight.get('hypothesis', '')}\n\n'
        markdown += '## Metrics\n'
        markdown += f'- Frequency: {insight.get('frequency', 'N/A')}\n'
        markdown += f'- Severity: {insight.get('severity', 'N/A')}\n'
        markdown += f'- Severity reasoning: {insight.get('severity_reasoning', 'N/A')}\n'
        markdown += f'- Confidence: {insight.get('confidence', 'N/A')}\n'
        markdown += f'- Confidence reasoning: {insight.get('confidence_reasoning', 'N/A')}\n'
        return markdown
    except Exception as e:
        print(f'Error converting insight to markdown: {e}')
        traceback.print_exc()
        return 'Error'

def get_insight(stream_key, timestamp):
    try:
        dynamodb_table = get_dynamodb_table('website-insights')
        insight_item = dynamodb_table.get_item(Key={'streamKey': stream_key, 'timestamp': int(timestamp)})
        insight = insight_item['Item']
        return (insight, insight_to_markdown(insight))
    except Exception as e:
        print(f'Error retrieving insight: {e}')
        return (None, None)

def get_previous_insights(stream_key: str) -> tuple[list[Dict[str, Any]], list[str]]:
    """
    Get all previous verified insights and their markdown representations.
    
    Args:
        stream_key (str): The stream key to get insights for
        
    Returns:
        Tuple containing list of raw insights and list of markdown strings
    """
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.query(TableName='website-insights', KeyConditionExpression='streamKey = :sk', FilterExpression='verified = :v', ExpressionAttributeValues={':sk': {'S': stream_key}, ':v': {'BOOL': True}})
        insights = []
        markdowns = []
        for item in response.get('Items', []):
            insights.append(item)
            markdowns.append(insight_to_markdown(item))
        return (insights, markdowns)
    except Exception as e:
        print(f'Error retrieving previous insights: {e}')
        return ([], [])

def get_all_okrs_markdown(stream_key: str) -> tuple[list[Dict[str, Any]], list[str]]:
    """
    Get all OKRs and their markdown representations.
    
    Args:
        stream_key (str): The stream key to get OKRs for
        
    Returns:
        Tuple containing list of raw OKRs and list of markdown strings
    """
    try:
        dynamodb = get_dynamodb_client()
        response = dynamodb.query(TableName='website-okrs', KeyConditionExpression='streamKey = :sk', ExpressionAttributeValues={':sk': {'S': stream_key}})
        okrs = []
        markdowns = []
        for item in response.get('Items', []):
            okrs.append(item)
            markdowns.append(okr_to_markdown(item))
        return (okrs, markdowns)
    except Exception as e:
        print(f'Error retrieving OKRs: {e}')
        return ([], [])

def okr_to_markdown(okr: dict) -> str:
    """Convert an OKR to markdown format."""
    markdown = '# OKR Analysis\n\n'
    markdown += f'## Name\n{okr.get('name', '')}'
    markdown += f'## Description\n{okr.get('description', '')}'
    if 'timestamp' in okr:
        timestamp_int = int(okr.get('timestamp', 0))
        markdown += f'## Last Updated\n{datetime.fromtimestamp(timestamp_int / 1000, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}\n\n'
    if 'output' in okr:
        try:
            output_dict = eval(okr['output'])
            markdown += '## Metrics\n'
            markdown += f'- Metric Name: {output_dict.get('Metric', 'N/A')}\n'
            markdown += f'- Description: {output_dict.get('Description', 'N/A')}\n'
            markdown += f'- Date Range: {output_dict.get('start_date', 'N/A')} to {output_dict.get('end_date', 'N/A')}\n'
            if 'values' in output_dict:
                markdown += '- Values:\n'
                for date, value in output_dict['values']:
                    markdown += f'  - {date}: {value}\n'
        except:
            markdown += f'## Raw Output\n{okr.get('output', 'N/A')}\n'
    if 'reach_value' in okr:
        markdown += f'\n## Reach\n{okr.get('reach_value', 'N/A')}\n'
    return markdown

def is_running_locally():
    """
    Determines if the Lambda function is running locally based on Serverless environment or local testing indicators.
    """
    return os.getenv('IS_LOCAL', 'false').lower() == 'true' or os.getenv('AWS_SAM_LOCAL', 'false').lower() == 'true' or 'AWS_LAMBDA_RUNTIME_API' not in os.environ

def summarize_chat(task_result, prompt, context, question, response_format=None):
    try:
        print('Summarizing chat')
        chat_messages: messages = task_result.messages
        chat_messages = [convert_message_to_dict(message) for message in chat_messages]
        chat_messages = list(filter(None, chat_messages))
        history = [{'role': 'user', 'content': [{'type': 'text', 'text': f'This chat tried to answer the question: {question} with the context: {context}\n\n    This is the chat history:\n    ```json\n    {json.dumps(chat_messages, indent=4)}\n    ```\n\n    {prompt}\n    '}]}]
        if response_format is None:
            output = (run_completion_with_fallback(history), history)
        else:
            output = (run_completion_with_fallback(history, response_format=response_format), history)
        print('Summary: ', output[0])
        return output
    except Exception as e:
        print(f'Error summarizing chat: {str(e)}')
        return ('Error', [])

def create_executor(functions):
    temp_dir = tempfile.mkdtemp()
    work_dir = Path(temp_dir)
    work_dir.mkdir(exist_ok=True)
    executor = LocalCommandLineCodeExecutor(work_dir=work_dir.name, functions=functions, timeout=600)
    functions_module = executor.functions_module
    return (executor, functions_module, temp_dir)

def get_api_key(secret_name):
    region_name = 'us-east-1'
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region_name)
    get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    return json.loads(get_secret_value_response['SecretString'])

def initialize_vertex_ai():
    """Initialize Vertex AI with service account credentials"""
    global vertex_ai_initialized
    if not vertex_ai_initialized:
        AI_KEYS = get_api_key('AI_KEYS')
        litellm.api_key = AI_KEYS['LLM_API_KEY']
        litellm.api_base = 'https://llms.sitewiz.ai'
        litellm.enable_json_schema_validation = True
        vertex_ai_initialized = True

def initialize_env():
    initialize_vertex_ai()
    api_keys = get_api_key('AI_KEYS')
    os.environ['ZEP_API_KEY'] = api_keys['ZEP_API_KEY']
    os.environ['LLM_API_KEY'] = api_keys['LLM_API_KEY']
    os.environ['WANDB_API_KEY'] = api_keys['WANDB_API_KEY']
    LLM_API_KEY = api_keys['LLM_API_KEY']
    weave.init('Agents')

    def get_llm_config(temp: float=1, model='main'):
        if model == 'gpt-4o':
            return OpenAIChatCompletionClient(model='main', api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': True, 'vision': True, 'function_calling': True})
        if model == 'o1':
            return OpenAIChatCompletionClient(model='reasoning', api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': False, 'vision': True, 'function_calling': False})
        others = ['main', 'main-mini', 'code', 'video']
        if model in others:
            return OpenAIChatCompletionClient(model=model, api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': True, 'vision': True, 'function_calling': True})
        if model == 'deepseek':
            return OpenAIChatCompletionClient(model='reasoning', api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': True, 'vision': False, 'function_calling': True})
        if model == 'reasoning':
            return OpenAIChatCompletionClient(model=model, api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': True, 'vision': False, 'function_calling': False})
        if model == 'reasoning-mini':
            return OpenAIChatCompletionClient(model=model, api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': True, 'vision': True, 'function_calling': False})
        raise ValueError(f'Invalid model name: {model}')
    return get_llm_config

def filter_history(history):
    history = [msg for msg in history if 'tool' not in msg.get('role', '').lower() and 'tool_calls' not in msg]
    return history

def get_secret():
    if is_running_locally():
        secret_name = 'heatmap/credentials'
    else:
        secret_name = 'heatmap/credentials-fetch'
    region_name = 'us-east-1'
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region_name)
    get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    return json.loads(get_secret_value_response['SecretString'])

def get_db_connection():
    try:
        secret = get_secret()
        connection = psycopg2.connect(dbname=secret['dbname'], user=secret['username'], password=secret['password'], host=secret['host'], port=secret['port'])
        return connection
    except Exception as e:
        print(f'Error connecting to database: {e}')
        return None

def bge_en_base_embedding(summary: str):
    """
    Generate embeddings for the given summary using the SageMaker endpoint.

    Args:
        summary (str): The textual summary to generate embeddings for.

    Returns:
        List[float]: The embedding vector.
    """
    sagemaker_client = boto3.client('sagemaker-runtime', region_name='us-east-1')
    endpoint_name = 'bge-base-en'
    payload = {'inputs': [summary]}
    try:
        response = sagemaker_client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=json.dumps(payload))
        result = response['Body'].read().decode('utf-8')
        embedding = json.loads(result)['embeddings'][0]
        return embedding
    except Exception as e:
        print(f'Error fetching embedding: {e}')
        return []

def default(self, obj):
    if isinstance(obj, Decimal):
        return str(obj)
    return super(DecimalEncoder, self).default(obj)

def replace_braces(match):
    var_name = match.group(1)
    return variable_map.get(var_name, f'{{{var_name}}}')

def get_llm_config(temp: float=1, model='main'):
    if model == 'gpt-4o':
        return OpenAIChatCompletionClient(model='main', api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': True, 'vision': True, 'function_calling': True})
    if model == 'o1':
        return OpenAIChatCompletionClient(model='reasoning', api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': False, 'vision': True, 'function_calling': False})
    others = ['main', 'main-mini', 'code', 'video']
    if model in others:
        return OpenAIChatCompletionClient(model=model, api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': True, 'vision': True, 'function_calling': True})
    if model == 'deepseek':
        return OpenAIChatCompletionClient(model='reasoning', api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': True, 'vision': False, 'function_calling': True})
    if model == 'reasoning':
        return OpenAIChatCompletionClient(model=model, api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': True, 'vision': False, 'function_calling': False})
    if model == 'reasoning-mini':
        return OpenAIChatCompletionClient(model=model, api_key=LLM_API_KEY, base_url='https://llms.sitewiz.ai', model_capabilities={'json_output': True, 'vision': True, 'function_calling': False})
    raise ValueError(f'Invalid model name: {model}')



# File: backend/agents/data_analyst_group/utils/secrets.py

import boto3

import json

from typing import Dict, Any

def fetch_secret(secret_name: str) -> Dict[str, Any]:
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name='us-east-1')
    get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    return json.loads(get_secret_value_response['SecretString'])



# File: backend/agents/data_analyst_group/utils/verifications.py

from utils.functions import run_completion_with_fallback, get_dynamodb_table

from typing import Tuple, Dict, List

import json

from botocore.exceptions import ClientError

import time

from decimal import Decimal

from pydantic import BaseModel, Field

def extract_entries_from_messages(messages_str: str) -> List[Dict[str, str]]:
    """Extract timestamps and stream keys using LLM."""
    prompt = {'role': 'user', 'content': [{'type': 'text', 'text': f'Extract all timestamp and streamKey pairs from the following messages. Return in JSON format:\n{{\n    "entries": [\n        {{\n            "timestamp": "numeric_timestamp",\n            "stream_key": "stream_key_value"\n        }}\n    ]\n}}\n\nMessages:\n{messages_str}'}]}
    try:
        result = run_completion_with_fallback([prompt], response_format=TimestampResponse)
        return [entry.dict() for entry in result.entries]
    except Exception as e:
        return []

def batch_verify_dynamodb_entries(table_name: str, entries: List[Dict[str, str]]) -> Dict[str, bool]:
    """Batch verify multiple entries in DynamoDB using table operations with retries."""
    try:
        table = get_dynamodb_table(table_name)
        results = {}
        for i in range(0, len(entries), BATCH_SIZE):
            batch = entries[i:i + BATCH_SIZE]
            retry_count = 0
            while retry_count < MAX_RETRIES:
                try:
                    keys = [{'streamKey': entry['stream_key'], 'timestamp': Decimal(str(entry['timestamp']))} for entry in batch]
                    response = table.batch_get(Keys=keys, ConsistentRead=True)
                    for entry in batch:
                        key = f'{entry['stream_key']}:{entry['timestamp']}'
                        results[key] = any((item['streamKey'] == entry['stream_key'] and str(item['timestamp']) == str(entry['timestamp']) for item in response.get('Responses', [])))
                    break
                except ClientError as e:
                    error_code = e.response['Error']['Code']
                    if error_code in ['ProvisionedThroughputExceededException', 'ThrottlingException']:
                        if retry_count < MAX_RETRIES - 1:
                            delay = RETRY_BASE_DELAY * 2 ** retry_count
                            time.sleep(delay)
                            retry_count += 1
                            continue
                    for entry in batch:
                        key = f'{entry['stream_key']}:{entry['timestamp']}'
                        results[key] = False
                    break
        return results
    except Exception as e:
        return {f'{entry['stream_key']}:{entry['timestamp']}': False for entry in entries}

def verify_entries(messages_str: str, table_name: str) -> Tuple[bool, str]:
    """Common verification logic for both insights and suggestions."""
    entries = extract_entries_from_messages(messages_str)
    if not entries:
        return (False, 'No timestamps or stream keys found in messages')
    verification_results = batch_verify_dynamodb_entries(table_name, entries)
    verified = []
    failed = []
    for entry in entries:
        key = f'{entry['stream_key']}:{entry['timestamp']}'
        if verification_results.get(key, False):
            verified.append(entry)
        else:
            failed.append(entry)
    if failed:
        failed_details = [f'timestamp: {e['timestamp']}, stream_key: {e['stream_key']}' for e in failed]
        return (False, f'Failed to verify in {table_name}: {', '.join(failed_details)}')
    if verified:
        verified_details = [f'timestamp: {e['timestamp']}, stream_key: {e['stream_key']}' for e in verified]
        return (True, f'Successfully verified in {table_name}: {', '.join(verified_details)}')
    return (False, f'No valid entries found to verify in {table_name}')

def verify_insights(messages_str: str) -> Tuple[bool, str]:
    return verify_entries(messages_str, 'website-insights')

def verify_suggestions(messages_str: str) -> Tuple[bool, str]:
    return verify_entries(messages_str, 'WebsiteReports')

def verify_guardrails(messages_str: str) -> Tuple[bool, str]:
    history = [{'role': 'user', 'content': [{'type': 'text', 'text': f'Given these messages:\n{messages_str}\n\nVerify if there is a clear guardrails verification for each suggestion. Return in format:\n{{\n    "has_verification": true/false,\n    "reason": "explanation why verification failed or succeeded"\n}}'}]}]
    result = run_completion_with_fallback(history, response_format=VerificationResponse)
    return (result.has_verification, result.reason)

def verify_website_code(messages_str: str) -> Tuple[bool, str]:
    history = [{'role': 'user', 'content': [{'type': 'text', 'text': f'Given these messages:\n{messages_str}\n\nVerify if there is website code and confirmation of store_website execution. Return in format:\n{{\n    "has_save_confirmation": true/false,\n    "reason": "explanation why verification failed or succeeded"\n}}'}]}]
    result = run_completion_with_fallback(history, response_format=WebsiteCodeResponse)
    return (result.has_save_confirmation, result.reason)



# File: backend/agents/data_analyst_group/utils/website_data_utils.py

import boto3

import logging

from decimal import Decimal

from utils.functions import get_dynamodb_table

def convert_decimal_to_number(obj):
    """Recursively convert all Decimal types in a dict or list to int or float."""
    if isinstance(obj, list):
        return [convert_decimal_to_number(i) for i in obj]
    elif isinstance(obj, dict):
        return {k: convert_decimal_to_number(v) for k, v in obj.items()}
    elif isinstance(obj, Decimal):
        return int(obj) if obj % 1 == 0 else float(obj)
    else:
        return obj

def update_website_data(stream_key: str, data: dict):
    """Update website data in DynamoDB."""
    try:
        response = website_data_table.get_item(Key={'streamKey': stream_key})
        existing_item = response.get('Item', {'streamKey': stream_key})
        if 'streamKey' not in existing_item:
            existing_item['streamKey'] = stream_key
        existing_item.update(data)
        website_data_table.put_item(Item=existing_item)
        logger.info(f'Website data updated successfully for stream_key: {stream_key}')
    except Exception as e:
        logger.error(f'Error updating website data: {str(e)}')
        raise e

def get_website_data(stream_key: str):
    """Get website data from DynamoDB."""
    try:
        response = website_data_table.get_item(Key={'streamKey': stream_key})
        item = response.get('Item', {})
        return convert_decimal_to_number(item)
    except Exception as e:
        logger.error(f'Error getting website data: {str(e)}')
        raise e

